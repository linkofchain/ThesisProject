% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{agrawalLearningWhenTrust2023}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=bfdf7cfed2931b695b0ec2d70a3331a4}{%
           family={Agrawal},
           familyi={A\bibinitperiod},
           given={Aakriti},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4558f7f2d9343618f380fdda2279e321}{%
           family={Rao},
           familyi={R\bibinitperiod},
           given={Milind},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=230b471a7c8c406b360a8044fe687eba}{%
           family={Sahu},
           familyi={S\bibinitperiod},
           given={Anit\bibnamedelima Kumar},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1a7c4a4ef0b592c66e33bea9fc2106d8}{%
           family={Chennupati},
           familyi={C\bibinitperiod},
           given={Gopinath},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8b60cd3ee633c8d1d585f08b92fefbf4}{%
           family={Stolcke},
           familyi={S\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{0ca54fbd7d16b1891b65f8a017e4bef7}
      \strng{fullhash}{61a2049bce9601e1a5b4be983d986970}
      \strng{bibnamehash}{61a2049bce9601e1a5b4be983d986970}
      \strng{authorbibnamehash}{61a2049bce9601e1a5b4be983d986970}
      \strng{authornamehash}{0ca54fbd7d16b1891b65f8a017e4bef7}
      \strng{authorfullhash}{61a2049bce9601e1a5b4be983d986970}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25\% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER.}
      \field{booktitle}{{{INTERSPEECH}} 2023}
      \field{day}{20}
      \field{eventtitle}{{{INTERSPEECH}} 2023}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{Learning {{When}} to {{Trust Which Teacher}} for {{Weakly Supervised ASR}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{381\bibrangedash 385}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2023-2205
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/LCGXDWE2/Agrawal et al. - 2023 - Learning When to Trust Which Teacher for Weakly Supervised ASR.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2023/agrawal23_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2023/agrawal23_interspeech.html
      \endverb
    \endentry
    \entry{ananthakrishnanUsingEnsembleClassifiers2011}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=42c4c3c92ef15d474e69e7280dd804b4}{%
           family={Ananthakrishnan},
           familyi={A\bibinitperiod},
           given={Gopal},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=db0f52ff482106f02af3b75f29cc2b11}{%
           family={Wik},
           familyi={W\bibinitperiod},
           given={Preben},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a970c082d42c183351e85273406ee3f2}{%
           family={Engwall},
           familyi={E\bibinitperiod},
           given={Olov},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=52132341c6498d6ac44832a67bd65163}{%
           family={Abdou},
           familyi={A\bibinitperiod},
           given={Sherif},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{9fdf7a20cf667655366e69b5c912eae2}
      \strng{fullhash}{202a94a525b20b8fc8740711472a482c}
      \strng{bibnamehash}{202a94a525b20b8fc8740711472a482c}
      \strng{authorbibnamehash}{202a94a525b20b8fc8740711472a482c}
      \strng{authornamehash}{9fdf7a20cf667655366e69b5c912eae2}
      \strng{authorfullhash}{202a94a525b20b8fc8740711472a482c}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Speech and {{Language Technology}} in {{Education}} ({{SLaTE}} 2011)}
      \field{day}{24}
      \field{eventtitle}{Speech and {{Language Technology}} in {{Education}} ({{SLaTE}} 2011)}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{Using an Ensemble of Classifiers for Mispronunciation Feedback}
      \field{urlday}{17}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2011}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{49\bibrangedash 52}
      \range{pages}{4}
      \verb{doi}
      \verb 10.21437/SLaTE.2011-13
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/2NR39XB8/ananthakrishnan11_slate.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/slate_2011/ananthakrishnan11_slate.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/slate_2011/ananthakrishnan11_slate.html
      \endverb
    \endentry
    \entry{ardilaCommonVoiceMassivelyMultilingual2020}{online}{}
      \name{author}{10}{}{%
        {{un=0,uniquepart=base,hash=7321bb4e1da541b5a6e891e020264c63}{%
           family={Ardila},
           familyi={A\bibinitperiod},
           given={Rosana},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=846f981fc6e97b5a84fa1ebdc8085a9d}{%
           family={Branson},
           familyi={B\bibinitperiod},
           given={Megan},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d92773f085a9e80944dd7f898767b3e9}{%
           family={Davis},
           familyi={D\bibinitperiod},
           given={Kelly},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8986d50512e32eaf81d70367977bf7e8}{%
           family={Henretty},
           familyi={H\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bf19183a0bad2eff52167e01391227d5}{%
           family={Kohler},
           familyi={K\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c61fec18fc73e757e4b7cbf3347e0616}{%
           family={Meyer},
           familyi={M\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=affe60f86582a0b0d4d7b0dc5f69ddcd}{%
           family={Morais},
           familyi={M\bibinitperiod},
           given={Reuben},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8485ef8e8a382a4dcb8edc45951e8948}{%
           family={Saunders},
           familyi={S\bibinitperiod},
           given={Lindsay},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e8a05c165e02340ed918110411776e8e}{%
           family={Tyers},
           familyi={T\bibinitperiod},
           given={Francis\bibnamedelima M.},
           giveni={F\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc7da550cf5c28937bb0f55221f957a4}{%
           family={Weber},
           familyi={W\bibinitperiod},
           given={Gregor},
           giveni={G\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e94d33e7232b0ded5172807b20fa0970}
      \strng{fullhash}{54958f998917b4f43b06cc2a742794f2}
      \strng{bibnamehash}{54958f998917b4f43b06cc2a742794f2}
      \strng{authorbibnamehash}{54958f998917b4f43b06cc2a742794f2}
      \strng{authornamehash}{e94d33e7232b0ded5172807b20fa0970}
      \strng{authorfullhash}{54958f998917b4f43b06cc2a742794f2}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla’s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 ± 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.}
      \field{day}{5}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Common {{Voice}}}
      \field{title}{Common {{Voice}}: {{A Massively-Multilingual Speech Corpus}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1912.06670
      \endverb
      \verb{eprint}
      \verb 1912.06670
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/DVASHKJ2/Ardila et al. - 2020 - Common Voice A Massively-Multilingual Speech Corpus.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1912.06670
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1912.06670
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{arunkumarInvestigationEnsembleFeatures2022}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=89362ac59f641c2a216ab3a5ef950ab3}{%
           family={Arunkumar},
           familyi={A\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3d8bc4cc4256a87df9d283999830f43b}{%
           family={Sukhadia},
           familyi={S\bibinitperiod},
           given={Vrunda\bibnamedelima N.},
           giveni={V\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7c484fdab01bf26d67f62489ad31885d}{%
           family={Umesh},
           familyi={U\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f4e565dcf7247c4b3b943ab8440c4ffc}
      \strng{fullhash}{3a47b6427b847a1f36c5c1f4b92a42aa}
      \strng{bibnamehash}{3a47b6427b847a1f36c5c1f4b92a42aa}
      \strng{authorbibnamehash}{3a47b6427b847a1f36c5c1f4b92a42aa}
      \strng{authornamehash}{f4e565dcf7247c4b3b943ab8440c4ffc}
      \strng{authorfullhash}{3a47b6427b847a1f36c5c1f4b92a42aa}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Self-supervised learning (SSL) based models have been shown to generate powerful representations that can be used to improve the performance of downstream speech tasks. Several state-of-the-art SSL models are available, and each of these models optimizes a different loss which gives rise to the possibility of their features being complementary. This paper proposes using an ensemble of such SSL representations and models, which exploits the complementary nature of the features extracted by the various pretrained models. We hypothesize that this results in a richer feature representation and shows results for the ASR downstream task. To this end, we use three SSL models that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the ensemble of features using the embeddings obtained from the pre-trained models for a downstream ASR task. We get improved performance over individual models and pre-trained features using Librispeech(100h) and WSJ dataset for the downstream tasks.}
      \field{booktitle}{Interspeech 2022}
      \field{day}{18}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{title}{Investigation of {{Ensemble}} Features of {{Self-Supervised Pretrained Models}} for {{Automatic Speech Recognition}}}
      \field{urlday}{29}
      \field{urlmonth}{8}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{5145\bibrangedash 5149}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2022-11376
      \endverb
      \verb{eprint}
      \verb 2206.05518
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/5WIILZ6Q/Arunkumar et al. - 2022 - Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recogni.pdf;/home/pccady/Zotero/storage/6H4JHM6G/2206.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2206.05518
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2206.05518
      \endverb
      \keyw{Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{moseleyAtlasWorldsLanguages2010}{misc}{}
      \name{namea}{2}{}{%
        {{hash=1be4a49be999a9fdbf490c5b34f06e04}{%
           family={Moseley},
           familyi={M\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=d5261aa7acd169f78b0cf19a10a6dcfb}{%
           family={Nicolas},
           familyi={N\bibinitperiod},
           given={Alexandre},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Paris}%
      }
      \list{organization}{1}{%
        {UNESCO Publishing}%
      }
      \strng{nameabibnamehash}{ff98648deea816cbe36858c04d841f1c}
      \strng{nameanamehash}{ff98648deea816cbe36858c04d841f1c}
      \strng{nameafullhash}{ff98648deea816cbe36858c04d841f1c}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labeltitlesource}{title}
      \field{edition}{3. ed., entirely rev., enlarged and updated}
      \field{isbn}{978-92-3-104096-2 978-92-3-104095-5}
      \field{langid}{english}
      \field{nameatype}{collaborator}
      \field{pagetotal}{62}
      \field{title}{Atlas of the World's Languages in Danger}
      \field{year}{2010}
      \field{dateera}{ce}
    \endentry
    \entry{babuXLSRSelfsupervisedCrosslingual2021}{online}{}
      \name{author}{13}{}{%
        {{un=0,uniquepart=base,hash=c0ad178236e5ea7bb5572a576ee5ff82}{%
           family={Babu},
           familyi={B\bibinitperiod},
           given={Arun},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2242e23b82996e29a58069cd958eaca7}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Changhan},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=66dd2c90abd0bf676a14e5c1df8927d9}{%
           family={Tjandra},
           familyi={T\bibinitperiod},
           given={Andros},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=90da1abde84b45ee8f620c681a9f2088}{%
           family={Lakhotia},
           familyi={L\bibinitperiod},
           given={Kushal},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f1f086561872e5a943563c890f3c3bef}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Qiantong},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=78a420900f50a8470ad085fb05231bf6}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Naman},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=17d6908b34ba75b16ed548797582f783}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Kritika},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,useprefix=false,hash=0897406aed42d0962c35ddc9d5c76431}{%
           family={Platen},
           familyi={P\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod},
           givenun=0,
           prefix={von},
           prefixi={v\bibinitperiod},
           prefixun=0}}%
        {{un=0,uniquepart=base,hash=479d8c24aad2a0c8723e90d44862c69c}{%
           family={Saraf},
           familyi={S\bibinitperiod},
           given={Yatharth},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=70fbbebd4875461935ebed5a7c7d4d1d}{%
           family={Pino},
           familyi={P\bibinitperiod},
           given={Juan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e478046d4abc39e9814f6e364a736511}{%
           family={Baevski},
           familyi={B\bibinitperiod},
           given={Alexei},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7cf7ac6a9456559cc67ee138c7f21cec}{%
           family={Conneau},
           familyi={C\bibinitperiod},
           given={Alexis},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ff0eff0a1d1258618c88f2ec57095f}{%
           family={Auli},
           familyi={A\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{41c9ddbcbb73bf222d5a61e0b0e9468a}
      \strng{fullhash}{8c2c90b0f0a0a7d7f28ac7ef45ccaf85}
      \strng{bibnamehash}{8c2c90b0f0a0a7d7f28ac7ef45ccaf85}
      \strng{authorbibnamehash}{8c2c90b0f0a0a7d7f28ac7ef45ccaf85}
      \strng{authornamehash}{41c9ddbcbb73bf222d5a61e0b0e9468a}
      \strng{authorfullhash}{8c2c90b0f0a0a7d7f28ac7ef45ccaf85}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34\% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.}
      \field{day}{16}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{XLS-R}}}
      \field{title}{{{XLS-R}}: {{Self-supervised Cross-lingual Speech Representation Learning}} at {{Scale}}}
      \field{urlday}{7}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2111.09296
      \endverb
      \verb{eprint}
      \verb 2111.09296
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/YEX5SNF2/Babu et al. - 2021 - XLS-R Self-supervised Cross-lingual Speech Representation Learning at Scale.pdf;/home/pccady/Zotero/storage/T8QG289D/2111.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2111.09296
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2111.09296
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{baevskiEffectivenessSelfsupervisedPretraining2020}{online}{}
      \name{author}{3}{ul=2}{%
        {{un=0,uniquepart=base,hash=e478046d4abc39e9814f6e364a736511}{%
           family={Baevski},
           familyi={B\bibinitperiod},
           given={Alexei},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ff0eff0a1d1258618c88f2ec57095f}{%
           family={Auli},
           familyi={A\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=838ae213145f7410d963d9727504b2df}{%
           family={Mohamed},
           familyi={M\bibinitperiod},
           given={Abdelrahman},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b81d7ae6e69bcb58f969bb3a50905801}
      \strng{fullhash}{57131208bf99f1e9bf17505834567dcf}
      \strng{bibnamehash}{57131208bf99f1e9bf17505834567dcf}
      \strng{authorbibnamehash}{57131208bf99f1e9bf17505834567dcf}
      \strng{authornamehash}{b81d7ae6e69bcb58f969bb3a50905801}
      \strng{authorfullhash}{57131208bf99f1e9bf17505834567dcf}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25\% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.}
      \field{day}{18}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{title}{Effectiveness of Self-Supervised Pre-Training for Speech Recognition}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1911.03912
      \endverb
      \verb{eprint}
      \verb 1911.03912
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/M9V7A8QV/Baevski et al. - 2020 - Effectiveness of self-supervised pre-training for speech recognition.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1911.03912
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1911.03912
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{baevskiVqwav2vecSelfSupervisedLearning2020}{online}{}
      \name{author}{3}{ul=2}{%
        {{un=0,uniquepart=base,hash=e478046d4abc39e9814f6e364a736511}{%
           family={Baevski},
           familyi={B\bibinitperiod},
           given={Alexei},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a13c8464f464250a409c78edf90ea235}{%
           family={Schneider},
           familyi={S\bibinitperiod},
           given={Steffen},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ff0eff0a1d1258618c88f2ec57095f}{%
           family={Auli},
           familyi={A\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{3de575d02086b46b3037b42a314f9421}
      \strng{fullhash}{f27b45b9fadd86a4e99bde34a1d2b41f}
      \strng{bibnamehash}{f27b45b9fadd86a4e99bde34a1d2b41f}
      \strng{authorbibnamehash}{f27b45b9fadd86a4e99bde34a1d2b41f}
      \strng{authornamehash}{3de575d02086b46b3037b42a314f9421}
      \strng{authorfullhash}{f27b45b9fadd86a4e99bde34a1d2b41f}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.}
      \field{day}{16}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{2}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Vq-Wav2vec}
      \field{title}{Vq-Wav2vec: {{Self-Supervised Learning}} of {{Discrete Speech Representations}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1910.05453
      \endverb
      \verb{eprint}
      \verb 1910.05453
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/VXRQVZIT/Baevski et al. - 2020 - vq-wav2vec Self-Supervised Learning of Discrete Speech Representations.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1910.05453
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1910.05453
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{baevskiWav2vec20Framework2020a}{inproceedings}{}
      \name{author}{4}{ul=2}{%
        {{un=0,uniquepart=base,hash=e478046d4abc39e9814f6e364a736511}{%
           family={Baevski},
           familyi={B\bibinitperiod},
           given={Alexei},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86718cd959eccc231b89841a6abbde4c}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Yuhao},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=838ae213145f7410d963d9727504b2df}{%
           family={Mohamed},
           familyi={M\bibinitperiod},
           given={Abdelrahman},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ff0eff0a1d1258618c88f2ec57095f}{%
           family={Auli},
           familyi={A\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{d314a5100c1b68f38a138b5bd5bb60d1}
      \strng{fullhash}{f96a13c5747d71a5f892a49a4c2ea941}
      \strng{bibnamehash}{f96a13c5747d71a5f892a49a4c2ea941}
      \strng{authorbibnamehash}{f96a13c5747d71a5f892a49a4c2ea941}
      \strng{authornamehash}{d314a5100c1b68f38a138b5bd5bb60d1}
      \strng{authorfullhash}{f96a13c5747d71a5f892a49a4c2ea941}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{shorttitle}{Wav2vec 2.0}
      \field{title}{Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}}
      \field{urlday}{9}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{volume}{33}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{12449\bibrangedash 12460}
      \range{pages}{12}
      \verb{file}
      \verb /home/pccady/Zotero/storage/IYTHPXY4/Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learning of Speech Representations.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html
      \endverb
    \endentry
    \entry{bajorek2017l2}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=8631c19273e5a80e28776391ab19751a}{%
           family={Bajorek},
           familyi={B\bibinitperiod},
           given={Joan\bibnamedelima Palmiter},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8631c19273e5a80e28776391ab19751a}
      \strng{fullhash}{8631c19273e5a80e28776391ab19751a}
      \strng{bibnamehash}{8631c19273e5a80e28776391ab19751a}
      \strng{authorbibnamehash}{8631c19273e5a80e28776391ab19751a}
      \strng{authornamehash}{8631c19273e5a80e28776391ab19751a}
      \strng{authorfullhash}{8631c19273e5a80e28776391ab19751a}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Issues and Trends in Educational Technology}
      \field{number}{1}
      \field{title}{L2 Pronunciation in {{CALL}}: {{The}} Unrealized Potential of {{Rosetta}} Stone, {{Duolingo}}, {{Babbel}}, and Mango Languages}
      \field{volume}{5}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{24\bibrangedash 51}
      \range{pages}{28}
      \verb{file}
      \verb /home/pccady/Zotero/storage/IQR5R4FR/Bajorek - 2017 - L2 pronunciation in CALL The unrealized potential of Rosetta stone, Duolingo, Babbel, and mango lan.pdf
      \endverb
    \endentry
    \entry{barteldsMakingMoreLittle2023}{online}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=07c4453efe601e93fc331d72ed4c5be2}{%
           family={Bartelds},
           familyi={B\bibinitperiod},
           given={Martijn},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=39cbeb7e56afcc2dece256d14248c50a}{%
           family={San},
           familyi={S\bibinitperiod},
           given={Nay},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5cb9a4e25cf34341a4d955c890436cdb}{%
           family={McDonnell},
           familyi={M\bibinitperiod},
           given={Bradley},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3147296c99a3f829087becd1a4eaec08}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=210957e931f1a0a12afd9b9d7f3e8947}{%
           family={Wieling},
           familyi={W\bibinitperiod},
           given={Martijn},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{13d6acb28fdc132d6453fe76fd2c5f5b}
      \strng{fullhash}{937dd00ea439fbf13238e3a3e948f097}
      \strng{bibnamehash}{937dd00ea439fbf13238e3a3e948f097}
      \strng{authorbibnamehash}{937dd00ea439fbf13238e3a3e948f097}
      \strng{authornamehash}{13d6acb28fdc132d6453fe76fd2c5f5b}
      \strng{authorfullhash}{937dd00ea439fbf13238e3a3e948f097}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training data from text-only sources. We find that using a self-training approach consistently yields improved performance (a relative WER reduction up to 20.5\% compared to using an ASR system trained on 24 minutes of manually transcribed speech). The performance gain from TTS augmentation for Gronings was even stronger (up to 25.5\% relative reduction in WER compared to a system based on 24 minutes of manually transcribed speech). In sum, our results show the benefit of using self-training or (if possible) TTS-generated data as an efficient solution to overcome the limitations of data availability for resource-scarce languages in order to improve ASR performance.}
      \field{day}{19}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Making {{More}} of {{Little Data}}}
      \field{title}{Making {{More}} of {{Little Data}}: {{Improving Low-Resource Automatic Speech Recognition Using Data Augmentation}}}
      \field{urlday}{15}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2305.10951
      \endverb
      \verb{eprint}
      \verb 2305.10951
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/JSBDMVYY/Bartelds et al. - 2023 - Making More of Little Data Improving Low-Resource Automatic Speech Recognition Using Data Augmentat.pdf;/home/pccady/Zotero/storage/UECVQWGQ/2305.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2305.10951
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2305.10951
      \endverb
      \keyw{Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{benderAchievingEvaluatingLanguageIndependence2011}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=e579c650beb31c286389278140fadd63}{%
           family={Bender},
           familyi={B\bibinitperiod},
           given={Emily\bibnamedelima M.},
           giveni={E\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e579c650beb31c286389278140fadd63}
      \strng{fullhash}{e579c650beb31c286389278140fadd63}
      \strng{bibnamehash}{e579c650beb31c286389278140fadd63}
      \strng{authorbibnamehash}{e579c650beb31c286389278140fadd63}
      \strng{authornamehash}{e579c650beb31c286389278140fadd63}
      \strng{authorfullhash}{e579c650beb31c286389278140fadd63}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability. In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.}
      \field{day}{1}
      \field{issn}{1945-3604}
      \field{journaltitle}{Linguistic Issues in Language Technology}
      \field{langid}{english}
      \field{month}{10}
      \field{shortjournal}{LiLT}
      \field{title}{On {{Achieving}} and {{Evaluating Language-Independence}} in {{NLP}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{6}
      \field{year}{2011}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.33011/lilt.v6i.1239
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/US6RJ7YH/Bender - 2011 - On Achieving and Evaluating Language-Independence in NLP.pdf
      \endverb
      \verb{urlraw}
      \verb https://journals.colorado.edu/index.php/lilt/article/view/1239
      \endverb
      \verb{url}
      \verb https://journals.colorado.edu/index.php/lilt/article/view/1239
      \endverb
    \endentry
    \entry{besacierAutomaticSpeechRecognition2014}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=393dbf8e1aaa82b3c52ce5fde54751dc}{%
           family={Besacier},
           familyi={B\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8d146e62af0ef9c9436022e82a30bb4f}{%
           family={Barnard},
           familyi={B\bibinitperiod},
           given={Etienne},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6404be0e94e9d1e52252cbe343e118e5}{%
           family={Karpov},
           familyi={K\bibinitperiod},
           given={Alexey},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1f89e6d7240f123a6ea1333a02e28990}{%
           family={Schultz},
           familyi={S\bibinitperiod},
           given={Tanja},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Elsevier BV}%
      }
      \strng{namehash}{8b41073f345ca2222582b8b131fdefe4}
      \strng{fullhash}{99f93f6ef361745678aa4494e9d281c9}
      \strng{bibnamehash}{99f93f6ef361745678aa4494e9d281c9}
      \strng{authorbibnamehash}{99f93f6ef361745678aa4494e9d281c9}
      \strng{authornamehash}{8b41073f345ca2222582b8b131fdefe4}
      \strng{authorfullhash}{99f93f6ef361745678aa4494e9d281c9}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Speech processing for under-resourced languages is an active field of research, which has experienced significant progress during the past decade. We propose, in this paper, a survey that focuses on automatic speech recognition (ASR) for these languages. The definition of under-resourced languages and the challenges associated to them are first defined. The main part of the paper is a literature review of the recent (last 8 years) contributions made in ASR for under-resourced languages. Examples of past projects and future trends when dealing with under-resourced languages are also presented. We believe that this paper will be a good starting point for anyone interested to initiate research in (or operational development of) ASR for one or several under-resourced languages. It should be clear, however, that many of the issues and approaches presented here, apply to speech technology in general (text-to-speech synthesis for instance).}
      \field{issn}{0167-6393}
      \field{journaltitle}{Speech Communication}
      \field{langid}{english}
      \field{month}{1}
      \field{shorttitle}{Automatic Speech Recognition for Under-Resourced Languages}
      \field{title}{Automatic Speech Recognition for Under-Resourced Languages: {{A}} Survey}
      \field{urlday}{15}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{volume}{56}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{85\bibrangedash 100}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1016/j.specom.2013.07.008
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/AKFY7UJQ/Besacier et al. - 2014 - Automatic speech recognition for under-resourced languages A survey.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988
      \endverb
    \endentry
    \entry{broinNewUrbanIrish2014}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=d27b372d6738704d986062cd20f59d1f}{%
           family={Broin},
           familyi={B\bibinitperiod},
           given={Brian\bibnamedelima Ó},
           giveni={B\bibinitperiod\bibinitdelim Ó\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d27b372d6738704d986062cd20f59d1f}
      \strng{fullhash}{d27b372d6738704d986062cd20f59d1f}
      \strng{bibnamehash}{d27b372d6738704d986062cd20f59d1f}
      \strng{authorbibnamehash}{d27b372d6738704d986062cd20f59d1f}
      \strng{authornamehash}{d27b372d6738704d986062cd20f59d1f}
      \strng{authorfullhash}{d27b372d6738704d986062cd20f59d1f}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This article compares the phonetics and morphology of Irish spoken in the Gaeltacht with that spoken in Irish cities. Informants were identified by randomly selecting newsreaders and chat show hosts on Gaeltacht and urban Irishlanguage radio stations. Recordings of the speakers were transcribed and then analysed for morphological and phonetic accuracy. City speakers demonstrated a move towards simplified morphology and phonology, making fewer than 50\% of expected changes, while Gaeltacht speakers retained the language’s traditional forms, making more than 90\% of expected changes. It was discovered that the city speakers, while apparently speaking stable idiolects, each returned very different rates, suggesting that the cities do not yet have stable Irish dialects. The Gaeltacht speakers all returned very similar rates.}
      \field{langid}{english}
      \field{title}{New {{Urban Irish}}: {{Pidgin}}, {{Creole}}, or {{Bona Fide Dialect}}? {{The Phonetics}} and {{Morphology}} of {{City}} and {{Gaeltacht Speakers Systematically Compared}}}
      \field{year}{2014}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/ULMAZYR4/Broin - New Urban Irish Pidgin, Creole, or Bona Fide Dialect The Phonetics and Morphology of City and Gael.pdf
      \endverb
    \endentry
    \entry{calikEnsemblebasedFrameworkMispronunciation2023a}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=6fa895921e72ba4bd0471510d5314d7d}{%
           family={Calık},
           familyi={C\bibinitperiod},
           given={Sükrü\bibnamedelima Selim},
           giveni={S\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a847af30fc6d8a393a27cda47588d3c2}{%
           family={Kucukmanisa},
           familyi={K\bibinitperiod},
           given={Ayhan},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=20c52ef30f3fc3881af8cdb39d9d665e}{%
           family={Kilimci},
           familyi={K\bibinitperiod},
           given={Zeynep\bibnamedelima Hilal},
           giveni={Z\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{983dc1cc4875b0a83160627d1f4f3562}
      \strng{fullhash}{7a5d41c1b88775ac4a15b557a625a564}
      \strng{bibnamehash}{7a5d41c1b88775ac4a15b557a625a564}
      \strng{authorbibnamehash}{7a5d41c1b88775ac4a15b557a625a564}
      \strng{authornamehash}{983dc1cc4875b0a83160627d1f4f3562}
      \strng{authorfullhash}{7a5d41c1b88775ac4a15b557a625a564}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0003682X}
      \field{journaltitle}{Applied Acoustics}
      \field{langid}{english}
      \field{month}{9}
      \field{shortjournal}{Applied Acoustics}
      \field{title}{An Ensemble-Based Framework for Mispronunciation Detection of {{Arabic}} Phonemes}
      \field{urlday}{17}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{volume}{212}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{109593}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1016/j.apacoust.2023.109593
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/MH9668SX/main.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0003682X23003912
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0003682X23003912
      \endverb
    \endentry
    \entry{chasaideABAIRInitiativeBringing2017}{book}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=dfdef9c4f0b798139ec7baeb9c10b726}{%
           family={Chasaide},
           familyi={C\bibinitperiod},
           given={Ailbhe},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ff6612f89e173ef3f99da3d8e3198ec0}{%
           family={Ní\bibnamedelima Chiaráin},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Neasa},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a09def41ef54cd6582b67d86ae3785c1}{%
           family={Wendler},
           familyi={W\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4ca87a8b7ddfc49d2811ec5f34f86af}{%
           family={Berthelsen},
           familyi={B\bibinitperiod},
           given={Harald},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c18def86d87b42fc64a7f14ad616b8d1}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f08b34b12657a55637cced651ec89cc3}{%
           family={Gobl},
           familyi={G\bibinitperiod},
           given={Christer},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{956fdc8171f999d575de92d489af31ca}
      \strng{fullhash}{a366136d263e53aedd99fa07980855b7}
      \strng{bibnamehash}{a366136d263e53aedd99fa07980855b7}
      \strng{authorbibnamehash}{a366136d263e53aedd99fa07980855b7}
      \strng{authornamehash}{956fdc8171f999d575de92d489af31ca}
      \strng{authorfullhash}{a366136d263e53aedd99fa07980855b7}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{day}{20}
      \field{month}{8}
      \field{shorttitle}{The {{ABAIR}} Initiative}
      \field{title}{The {{ABAIR}} Initiative: {{Bringing Spoken Irish}} into the {{Digital Space}}}
      \field{year}{2017}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.21437/Interspeech.2017-1407
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/2JKBUKKG/Chasaide et al. - 2017 - The ABAIR initiative Bringing Spoken Irish into the Digital Space.pdf
      \endverb
    \endentry
    \entry{conneauUnsupervisedCrosslingualRepresentation2020}{online}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=7cf7ac6a9456559cc67ee138c7f21cec}{%
           family={Conneau},
           familyi={C\bibinitperiod},
           given={Alexis},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e478046d4abc39e9814f6e364a736511}{%
           family={Baevski},
           familyi={B\bibinitperiod},
           given={Alexei},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=133261178beda1dc3991e0e1dfd5a791}{%
           family={Collobert},
           familyi={C\bibinitperiod},
           given={Ronan},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=838ae213145f7410d963d9727504b2df}{%
           family={Mohamed},
           familyi={M\bibinitperiod},
           given={Abdelrahman},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ff0eff0a1d1258618c88f2ec57095f}{%
           family={Auli},
           familyi={A\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2916bb4d39f814823624e9e16627e23a}
      \strng{fullhash}{6e68297063fd6e1f247f0478a67fd8c9}
      \strng{bibnamehash}{6e68297063fd6e1f247f0478a67fd8c9}
      \strng{authorbibnamehash}{6e68297063fd6e1f247f0478a67fd8c9}
      \strng{authornamehash}{2916bb4d39f814823624e9e16627e23a}
      \strng{authorfullhash}{6e68297063fd6e1f247f0478a67fd8c9}
      \field{extraname}{1}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.}
      \field{day}{15}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Unsupervised {{Cross-lingual Representation Learning}} for {{Speech Recognition}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2006.13979
      \endverb
      \verb{eprint}
      \verb 2006.13979
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/KQLZCMY9/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning for Speech Recognition.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.13979
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.13979
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{conneauFLEURSFewshotLearning2022}{online}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=7cf7ac6a9456559cc67ee138c7f21cec}{%
           family={Conneau},
           familyi={C\bibinitperiod},
           given={Alexis},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=32f2d0efd7ab7988edc0ae637e441a81}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2d5d4d6610084e1ec6ee51ef32df252e}{%
           family={Khanuja},
           familyi={K\bibinitperiod},
           given={Simran},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9a4f4a1ff661cd600eb26523a5ba8bb4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=21dc628b949090d920cec3c106ae1019}{%
           family={Axelrod},
           familyi={A\bibinitperiod},
           given={Vera},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e102dda6b0e5bf95c65afe90b9454aa2}{%
           family={Dalmia},
           familyi={D\bibinitperiod},
           given={Siddharth},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b137e608b42835dfb224061b97b8aba4}{%
           family={Riesa},
           familyi={R\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=eb8ed9e3bdc50c922825a6193a311b66}{%
           family={Rivera},
           familyi={R\bibinitperiod},
           given={Clara},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=75d3660c3d4af9c1db2372eee7e68746}{%
           family={Bapna},
           familyi={B\bibinitperiod},
           given={Ankur},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2916bb4d39f814823624e9e16627e23a}
      \strng{fullhash}{b0faa309f6f6531a62185551e776d769}
      \strng{bibnamehash}{b0faa309f6f6531a62185551e776d769}
      \strng{authorbibnamehash}{b0faa309f6f6531a62185551e776d769}
      \strng{authornamehash}{2916bb4d39f814823624e9e16627e23a}
      \strng{authorfullhash}{b0faa309f6f6531a62185551e776d769}
      \field{extraname}{2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.}
      \field{day}{25}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{FLEURS}}}
      \field{title}{{{FLEURS}}: {{Few-shot Learning Evaluation}} of {{Universal Representations}} of {{Speech}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2205.12446
      \endverb
      \verb{eprint}
      \verb 2205.12446
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/ETIS2DLG/Conneau et al. - 2022 - FLEURS Few-shot Learning Evaluation of Universal Representations of Speech.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2205.12446
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2205.12446
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{deichlerMMConvMultimodalConversational2024}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=cf5e90fa1ca4f9f3962088f5b009c40e}{%
           family={Deichler},
           familyi={D\bibinitperiod},
           given={Anna},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7fdaca3c38da7961e6ce30f62d5fce54}{%
           family={O'Regan},
           familyi={O\bibinitperiod},
           given={Jim},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=360375b5fce1d04c3e8e15b273de2833}{%
           family={Beskow},
           familyi={B\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{73fa768409b15075f4120d59e36f4986}
      \strng{fullhash}{0e3f3768bfcb8f4bdef967b87b1a0dfe}
      \strng{bibnamehash}{0e3f3768bfcb8f4bdef967b87b1a0dfe}
      \strng{authorbibnamehash}{0e3f3768bfcb8f4bdef967b87b1a0dfe}
      \strng{authornamehash}{73fa768409b15075f4120d59e36f4986}
      \strng{authorfullhash}{0e3f3768bfcb8f4bdef967b87b1a0dfe}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this paper, we present a novel dataset captured using a VR headset to record conversations between participants within a physics simulator (AI2-THOR). Our primary objective is to extend the field of co-speech gesture generation by incorporating rich contextual information within referential settings. Participants engaged in various conversational scenarios, all based on referential communication tasks. The dataset provides a rich set of multimodal recordings such as motion capture, speech, gaze, and scene graphs. This comprehensive dataset aims to enhance the understanding and development of gesture generation models in 3D scenes by providing diverse and contextually rich data.}
      \field{day}{30}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{9}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{MM-Conv}}}
      \field{title}{{{MM-Conv}}: {{A Multi-modal Conversational Dataset}} for {{Virtual Humans}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2410.00253
      \endverb
      \verb{eprint}
      \verb 2410.00253
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/M5EG8TE2/Deichler et al. - 2024 - MM-Conv A Multi-modal Conversational Dataset for Virtual Humans.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2410.00253
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2410.00253
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction}
    \endentry
    \entry{dengEnsembleDeepLearning2014}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=1,uniquepart=given,hash=2f5fbdc5c3cf91f62a64663cd72397b3}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=b18cccf0f11300769fc4b01f5fe5f13a}{%
           family={Platt},
           familyi={P\bibinitperiod},
           given={John\bibnamedelima C.},
           giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c65f94daad3fe52ea565c465903fca66}
      \strng{fullhash}{c65f94daad3fe52ea565c465903fca66}
      \strng{bibnamehash}{c65f94daad3fe52ea565c465903fca66}
      \strng{authorbibnamehash}{c65f94daad3fe52ea565c465903fca66}
      \strng{authornamehash}{c65f94daad3fe52ea565c465903fca66}
      \strng{authorfullhash}{c65f94daad3fe52ea565c465903fca66}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Ensemble Deep Learning for Speech Recognition Li Deng and John C. Platt Microsoft Research, One Microsoft Way, Redmond, WA, USA deng@microsoft.com; jplatt@microsoft.com Abstract Deep learning systems have dramatically improved the accuracy of speech recognition, and various deep architecture...}
      \field{booktitle}{Proc. Interspeech}
      \field{langid}{english}
      \field{title}{Ensemble {{Deep Learning}} for {{Speech Recognition}}}
      \field{year}{2014}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/44E9M6S7/Deng and Platt - 2014 - Ensemble Deep Learning for Speech Recognition.pdf
      \endverb
    \endentry
    \entry{dengScalableStackingLearning2012}{article}{}
      \name{author}{3}{}{%
        {{un=1,uniquepart=given,hash=60f1cadf17dec9d3385a6e3d49ee96b3}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={li},
           giveni={l\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=93509a9a7ccede33feb1e702b5a8bb9f}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Dong},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=74ebe0fc06c0292871d5f756cfe5414d}{%
           family={Platt},
           familyi={P\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ea319b6cad7c0c25191588f66673a549}
      \strng{fullhash}{4bf7f7629df01edacc202b37e4193d27}
      \strng{bibnamehash}{4bf7f7629df01edacc202b37e4193d27}
      \strng{authorbibnamehash}{4bf7f7629df01edacc202b37e4193d27}
      \strng{authornamehash}{ea319b6cad7c0c25191588f66673a549}
      \strng{authorfullhash}{4bf7f7629df01edacc202b37e4193d27}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep Neural Networks (DNNs) have shown remarkable success in pattern recognition tasks. However, parallelizing DNN training across computers has been dif?cult. We present the Deep Stack- ing Network (DSN), which overcomes the problem of paralleliz- ing learning algorithms for deep architectures. The DSN provides a method of stacking simple processing modules in buiding deep architectures, with a convex learning problem in each module. Ad- ditional ?ne tuning further improves the DSN, while introducing mi- nor non-convexity. Full learning in the DSN is batch-mode, making it amenable to parallel training over many machines and thus be scal- able over the potentially huge size of the training data. Experimental results on both the MNIST (image) and TIMIT (speech) classi?ca- tion tasks demonstrate that the DSN learning algorithm developed in this work is not only parallelizable in implementation but it also attains higher classi?cation accuracy than the DNN.}
      \field{day}{1}
      \field{journaltitle}{Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on}
      \field{month}{3}
      \field{shortjournal}{Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on}
      \field{title}{Scalable Stacking and Learning for Building Deep Architectures}
      \field{year}{2012}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1109/ICASSP.2012.6288333
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/WN97TG6Y/Deng et al. - 2012 - Scalable stacking and learning for building deep architectures.pdf
      \endverb
    \endentry
    \entry{devlinBERTPretrainingDeep2019}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=13202969e372bc82318f9629cbdd199b}{%
           family={Devlin},
           familyi={D\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a45784fe7163b45f11d166564f5d24b6}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Ming-Wei},
           giveni={M\bibinithyphendelim W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8dde73b4194f5bc4230c4808f3fc1534}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kenton},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b92aa283415413bb8d2a1548716d0c7d}{%
           family={Toutanova},
           familyi={T\bibinitperiod},
           given={Kristina},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{3}{}{%
        {{hash=f974925c5c6cca1d67fd604cc9fc9c79}{%
           family={Burstein},
           familyi={B\bibinitperiod},
           given={Jill},
           giveni={J\bibinitperiod}}}%
        {{hash=3aec97177cb69c892f2c8a2d9bc31a10}{%
           family={Doran},
           familyi={D\bibinitperiod},
           given={Christy},
           giveni={C\bibinitperiod}}}%
        {{hash=b47889ad94571547c92970150485798b}{%
           family={Solorio},
           familyi={S\bibinitperiod},
           given={Thamar},
           giveni={T\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Minneapolis, Minnesota}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{fullhash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \strng{bibnamehash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \strng{authorbibnamehash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \strng{authornamehash}{ad8e2e2a80d28ade21e86852b804fc9b}
      \strng{authorfullhash}{e8bccd48302a14eeba57d9dce2f49ef4}
      \strng{editorbibnamehash}{3ba8abc3fd60f22784bd617fd230d1c6}
      \strng{editornamehash}{0f9989a68b9a4dd59d368e46c7ff0bda}
      \strng{editorfullhash}{3ba8abc3fd60f22784bd617fd230d1c6}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
      \field{booktitle}{Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})}
      \field{eventtitle}{{{NAACL-HLT}} 2019}
      \field{month}{6}
      \field{shorttitle}{{{BERT}}}
      \field{title}{{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}}
      \field{urlday}{28}
      \field{urlmonth}{8}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4171\bibrangedash 4186}
      \range{pages}{16}
      \verb{doi}
      \verb 10.18653/v1/N19-1423
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/QRBYYV99/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf
      \endverb
      \verb{urlraw}
      \verb https://aclanthology.org/N19-1423/
      \endverb
      \verb{url}
      \verb https://aclanthology.org/N19-1423/
      \endverb
    \endentry
    \entry{dietterichEnsembleMethodsMachine2000}{inproceedings}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=f9a517f99d8450b78124ea4348265e82}{%
           family={Dietterich},
           familyi={D\bibinitperiod},
           given={Thomas\bibnamedelima G},
           giveni={T\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{f9a517f99d8450b78124ea4348265e82}
      \strng{fullhash}{f9a517f99d8450b78124ea4348265e82}
      \strng{bibnamehash}{f9a517f99d8450b78124ea4348265e82}
      \strng{authorbibnamehash}{f9a517f99d8450b78124ea4348265e82}
      \strng{authornamehash}{f9a517f99d8450b78124ea4348265e82}
      \strng{authorfullhash}{f9a517f99d8450b78124ea4348265e82}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eventtitle}{International Workshop on Multiple Classifier Systems}
      \field{title}{Ensemble Methods in Machine Learning}
      \field{urlday}{6}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2000}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 15}
      \range{pages}{15}
      \verb{file}
      \verb /home/pccady/Zotero/storage/TGEEURGX/T5-3.pdf
      \endverb
      \verb{urlraw}
      \verb https://www2.cs.uh.edu/~ceick/7362/T5-3.pdf
      \endverb
      \verb{url}
      \verb https://www2.cs.uh.edu/~ceick/7362/T5-3.pdf
      \endverb
    \endentry
    \entry{matthewdryerWorldAtlasLanguage2024}{dataset}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=b7e65067ded37e5f6af4a7ca6b2ea9bf}{%
           family={Dryer},
           familyi={D\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6554cfb014a9427af44c63f16722a3dd}{%
           family={Haspelmath},
           familyi={H\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b7e65067ded37e5f6af4a7ca6b2ea9bf}{%
           family={Dryer},
           familyi={D\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6554cfb014a9427af44c63f16722a3dd}{%
           family={Haspelmath},
           familyi={H\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \name{namea}{1}{}{%
        {{hash=951238340aa8b7462747dcf6168e4cd3}{%
           family={Forkel},
           familyi={F\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Zenodo}%
      }
      \strng{namehash}{ab2abf2c1f5d3085340249e71171cdeb}
      \strng{fullhash}{bf6e8fe4075d3f99eb0eea37b449b4dd}
      \strng{bibnamehash}{bf6e8fe4075d3f99eb0eea37b449b4dd}
      \strng{authorbibnamehash}{bf6e8fe4075d3f99eb0eea37b449b4dd}
      \strng{authornamehash}{ab2abf2c1f5d3085340249e71171cdeb}
      \strng{authorfullhash}{bf6e8fe4075d3f99eb0eea37b449b4dd}
      \strng{nameabibnamehash}{951238340aa8b7462747dcf6168e4cd3}
      \strng{nameanamehash}{951238340aa8b7462747dcf6168e4cd3}
      \strng{nameafullhash}{951238340aa8b7462747dcf6168e4cd3}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Cite the source of the dataset as: Dryer, Matthew S. \& Haspelmath, Martin (eds.) 2013. The World Atlas of Language Structures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. (Available online at https://wals.info)}
      \field{day}{18}
      \field{month}{10}
      \field{nameatype}{collaborator}
      \field{title}{The {{World Atlas}} of {{Language Structures Online}}}
      \field{urlday}{8}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{version}{v2020.4}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.5281/ZENODO.13950591
      \endverb
      \verb{urlraw}
      \verb https://zenodo.org/doi/10.5281/zenodo.13950591
      \endverb
      \verb{url}
      \verb https://zenodo.org/doi/10.5281/zenodo.13950591
      \endverb
      \keyw{cldf:StructureDataset,linguistics}
    \endentry
    \entry{engstrandFonetikensGrunder2004}{book}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=98db60a4e9dce92a3608e34daf6c50f7}{%
           family={Engstrand},
           familyi={E\bibinitperiod},
           given={Olle},
           giveni={O\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Lund}%
      }
      \list{publisher}{1}{%
        {Studentlitteratur}%
      }
      \strng{namehash}{98db60a4e9dce92a3608e34daf6c50f7}
      \strng{fullhash}{98db60a4e9dce92a3608e34daf6c50f7}
      \strng{bibnamehash}{98db60a4e9dce92a3608e34daf6c50f7}
      \strng{authorbibnamehash}{98db60a4e9dce92a3608e34daf6c50f7}
      \strng{authornamehash}{98db60a4e9dce92a3608e34daf6c50f7}
      \strng{authorfullhash}{98db60a4e9dce92a3608e34daf6c50f7}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-91-44-04238-1}
      \field{pagetotal}{355}
      \field{title}{Fonetikens Grunder}
      \field{year}{2004}
      \field{dateera}{ce}
    \endentry
    \entry{fazelSynthASRUnlockingSynthetic2021}{online}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=fdc6a8de432312621bf6c7fd925aee81}{%
           family={Fazel},
           familyi={F\bibinitperiod},
           given={Amin},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b32f61b294f8e4aa5306c6ea6859da26}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0c0829000d9b5ec452920d6b816d3465}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Yulan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=105d4064dedc8711a1a2267e2a5919ec}{%
           family={Barra-Chicote},
           familyi={B\bibinithyphendelim C\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1191eac7329b2afd5c2b865036add5f2}{%
           family={Meng},
           familyi={M\bibinitperiod},
           given={Yixiong},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5cd0c3f80c337a643e6e22b4314885e1}{%
           family={Maas},
           familyi={M\bibinitperiod},
           given={Roland},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c137643a1d9eebff847bff4a61a5326d}{%
           family={Droppo},
           familyi={D\bibinitperiod},
           given={Jasha},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{517d0d51f663a5819e82d8ebf551e094}
      \strng{fullhash}{3748719dadc7b8708b6fb907a5d20a33}
      \strng{bibnamehash}{3748719dadc7b8708b6fb907a5d20a33}
      \strng{authorbibnamehash}{3748719dadc7b8708b6fb907a5d20a33}
      \strng{authornamehash}{517d0d51f663a5819e82d8ebf551e094}
      \strng{authorfullhash}{3748719dadc7b8708b6fb907a5d20a33}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{End-to-end (E2E) automatic speech recognition (ASR) models have recently demonstrated superior performance over the traditional hybrid ASR models. Training an E2E ASR model requires a large amount of data which is not only expensive but may also raise dependency on production data. At the same time, synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines has advanced to near-human naturalness. In this work, we propose to utilize synthetic speech for ASR training (SynthASR) in applications where data is sparse or hard to get for ASR model training. In addition, we apply continual learning with a novel multi-stage training strategy to address catastrophic forgetting, achieved by a mix of weighted multi-style training, data augmentation, encoder freezing, and parameter regularization. In our experiments conducted on in-house datasets for a new application of recognizing medication names, training ASR RNN-T models with synthetic audio via the proposed multi-stage training improved the recognition performance on new application by more than 65\% relative, without degradation on existing general applications. Our observations show that SynthASR holds great promise in training the state-of-the-art large-scale E2E ASR models for new applications while reducing the costs and dependency on production data.}
      \field{day}{14}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{SynthASR}}}
      \field{title}{{{SynthASR}}: {{Unlocking Synthetic Data}} for {{Speech Recognition}}}
      \field{urlday}{13}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2106.07803
      \endverb
      \verb{eprint}
      \verb 2106.07803
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/5SBEK7VU/Fazel et al. - 2021 - SynthASR Unlocking Synthetic Data for Speech Recognition.pdf;/home/pccady/Zotero/storage/P8XNTKVX/2106.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2106.07803
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2106.07803
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{fiscus1997post}{inproceedings}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=5eb94c7444909f6544dafe84917447f3}{%
           family={Fiscus},
           familyi={F\bibinitperiod},
           given={Jonathan\bibnamedelima G},
           giveni={J\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{5eb94c7444909f6544dafe84917447f3}
      \strng{fullhash}{5eb94c7444909f6544dafe84917447f3}
      \strng{bibnamehash}{5eb94c7444909f6544dafe84917447f3}
      \strng{authorbibnamehash}{5eb94c7444909f6544dafe84917447f3}
      \strng{authornamehash}{5eb94c7444909f6544dafe84917447f3}
      \strng{authorfullhash}{5eb94c7444909f6544dafe84917447f3}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{1997 {{IEEE}} Workshop on Automatic Speech Recognition and Understanding Proceedings}
      \field{title}{A Post-Processing System to Yield Reduced Word Error Rates: {{Recognizer}} Output Voting Error Reduction ({{ROVER}})}
      \field{year}{1997}
      \field{dateera}{ce}
      \field{pages}{347\bibrangedash 354}
      \range{pages}{8}
      \verb{file}
      \verb /home/pccady/Zotero/storage/X9QBVI6H/Fiscus - 1997 - A post-processing system to yield reduced word error rates Recognizer output voting error reduction.pdf
      \endverb
    \endentry
    \entry{gabrieleEnglishInfluenceL2}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=122d45f92fde0703f6ca31c3f18fe13d}{%
           family={Gabriele},
           familyi={G\bibinitperiod},
           given={Jennifer\bibnamedelima C},
           giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{122d45f92fde0703f6ca31c3f18fe13d}
      \strng{fullhash}{122d45f92fde0703f6ca31c3f18fe13d}
      \strng{bibnamehash}{122d45f92fde0703f6ca31c3f18fe13d}
      \strng{authorbibnamehash}{122d45f92fde0703f6ca31c3f18fe13d}
      \strng{authornamehash}{122d45f92fde0703f6ca31c3f18fe13d}
      \strng{authorfullhash}{122d45f92fde0703f6ca31c3f18fe13d}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{langid}{english}
      \field{title}{English {{Influence}} on {{L2 Speakers}}’ {{Production}} of {{Palatalization}} and {{Velarization}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/Y3E3RFSQ/Gabriele - English Influence on L2 Speakers’ Production of Palatalization and Velarization.pdf
      \endverb
    \endentry
    \entry{gitmanConfidencebasedEnsemblesEndtoEnd2023}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=3ebf1483613fa32fd2310a1840e1479f}{%
           family={Gitman},
           familyi={G\bibinitperiod},
           given={Igor},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ea50ad27e3aeecaaeee4cf5837a6b13a}{%
           family={Lavrukhin},
           familyi={L\bibinitperiod},
           given={Vitaly},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=be6aebc721bfde5ca511ee8447424b5c}{%
           family={Laptev},
           familyi={L\bibinitperiod},
           given={Aleksandr},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=57b6ee8703cf89267a2d51429a4e61a7}{%
           family={Ginsburg},
           familyi={G\bibinitperiod},
           given={Boris},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c59710d7b0e517579b6b531306dfd0fc}
      \strng{fullhash}{d22d5cd4941d715b4378ca92343ac846}
      \strng{bibnamehash}{d22d5cd4941d715b4378ca92343ac846}
      \strng{authorbibnamehash}{d22d5cd4941d715b4378ca92343ac846}
      \strng{authornamehash}{c59710d7b0e517579b6b531306dfd0fc}
      \strng{authorfullhash}{d22d5cd4941d715b4378ca92343ac846}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models’ target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.}
      \field{booktitle}{{{INTERSPEECH}} 2023}
      \field{day}{20}
      \field{eprintclass}{eess}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{Confidence-Based {{Ensembles}} of {{End-to-End Speech Recognition Models}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1414\bibrangedash 1418}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2023-1281
      \endverb
      \verb{eprint}
      \verb 2306.15824
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/HV8PGS5P/Gitman et al. - 2023 - Confidence-based Ensembles of End-to-End Speech Recognition Models.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2306.15824
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2306.15824
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{gongTransformerBasedMultiAspectMultiGranularity2022}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=001add0b14cdbd927a79bb51b1e48332}{%
           family={Gong},
           familyi={G\bibinitperiod},
           given={Yuan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a9aaa25f3317100dc7167c8c44c5f220}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Ziyi},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=10df0031d0c63dbc8bc3fc10eba5151e}{%
           family={Chu},
           familyi={C\bibinitperiod},
           given={Iek-Heng},
           giveni={I\bibinithyphendelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2a1ab9792be24e1401bdb865de053497}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Peng},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=079738ec97fefceec5036d7c3657c667}{%
           family={Glass},
           familyi={G\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d23ffb125862abcdbb865bea4c794f32}
      \strng{fullhash}{24b8a8dd01f47628bcbe2f1b7172e33a}
      \strng{bibnamehash}{24b8a8dd01f47628bcbe2f1b7172e33a}
      \strng{authorbibnamehash}{24b8a8dd01f47628bcbe2f1b7172e33a}
      \strng{authornamehash}{d23ffb125862abcdbb865bea4c794f32}
      \strng{authorfullhash}{24b8a8dd01f47628bcbe2f1b7172e33a}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Automatic pronunciation assessment is an important technology to help self-directed language learners. While pronunciation quality has multiple aspects including accuracy, fluency, completeness, and prosody, previous efforts typically only model one aspect (e.g., accuracy) at one granularity (e.g., at the phoneme-level). In this work, we explore modeling multi-aspect pronunciation assessment at multiple granularities. Specifically, we train a Goodness Of Pronunciation feature-based Transformer (GOPT) with multi-task learning. Experiments show that GOPT achieves the best results on speechocean762 with a public automatic speech recognition (ASR) acoustic model trained on Librispeech. Code at https://github.com/YuanGongND/gopt.}
      \field{booktitle}{{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
      \field{day}{23}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{Transformer-{{Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{7262\bibrangedash 7266}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICASSP43922.2022.9746743
      \endverb
      \verb{eprint}
      \verb 2205.03432
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/BS9CBM4Q/Gong et al. - 2022 - Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2205.03432
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2205.03432
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{gravesConnectionistTemporalClassification}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=299d87a73d184d0f2be5c0710ab298e8}{%
           family={Fernandez},
           familyi={F\bibinitperiod},
           given={Santiago},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6ac505680efa9d3591dd05930a13ccd5}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Faustino},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c3a004dc2b8b6fb4dd79c5b8c1469da7}{%
           family={Schmidhuber},
           familyi={S\bibinitperiod},
           given={Jurgen},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8b91c8aa3714ffeda8908af3996aa567}
      \strng{fullhash}{f5e5158a9b0b82b5f8db6483b94f8d53}
      \strng{bibnamehash}{f5e5158a9b0b82b5f8db6483b94f8d53}
      \strng{authorbibnamehash}{f5e5158a9b0b82b5f8db6483b94f8d53}
      \strng{authornamehash}{8b91c8aa3714ffeda8908af3996aa567}
      \strng{authorfullhash}{f5e5158a9b0b82b5f8db6483b94f8d53}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.}
      \field{langid}{english}
      \field{title}{Connectionist {{Temporal Classiﬁcation}}: {{Labelling Unsegmented Sequence Data}} with {{Recurrent Neural Networks}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/KY7HCUCD/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf
      \endverb
    \endentry
    \entry{guoCalibrationModernNeural2017}{online}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=e053cf028522c39af65dd862da2d8ed1}{%
           family={Guo},
           familyi={G\bibinitperiod},
           given={Chuan},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=373d04718ec0efd3786a0e2b6c0adf00}{%
           family={Pleiss},
           familyi={P\bibinitperiod},
           given={Geoff},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=17b6a43f5598c7efd897f9d254090fdd}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=68a0238356fbd88b34be8886f25938a7}{%
           family={Weinberger},
           familyi={W\bibinitperiod},
           given={Kilian\bibnamedelima Q.},
           giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{203b115999f73097be6ab32411fb1f42}
      \strng{fullhash}{f3833eed8ef776fc6c89f407c7d66c34}
      \strng{bibnamehash}{f3833eed8ef776fc6c89f407c7d66c34}
      \strng{authorbibnamehash}{f3833eed8ef776fc6c89f407c7d66c34}
      \strng{authornamehash}{203b115999f73097be6ab32411fb1f42}
      \strng{authorfullhash}{f3833eed8ef776fc6c89f407c7d66c34}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
      \field{day}{3}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{8}
      \field{pubstate}{prepublished}
      \field{title}{On {{Calibration}} of {{Modern Neural Networks}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1706.04599
      \endverb
      \verb{eprint}
      \verb 1706.04599
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/M6YFZKNL/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.04599
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.04599
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{hansenNeuralNetworkEnsembles1990}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=595f487d59f1ffa8650f0c3634fc6fd6}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={L.K.},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=62254655bd783aba006e75820c0b5611}{%
           family={Salamon},
           familyi={S\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c9c03b15e2e8e6cecd7826b32bb489fb}
      \strng{fullhash}{c9c03b15e2e8e6cecd7826b32bb489fb}
      \strng{bibnamehash}{c9c03b15e2e8e6cecd7826b32bb489fb}
      \strng{authorbibnamehash}{c9c03b15e2e8e6cecd7826b32bb489fb}
      \strng{authornamehash}{c9c03b15e2e8e6cecd7826b32bb489fb}
      \strng{authorfullhash}{c9c03b15e2e8e6cecd7826b32bb489fb}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.}
      \field{issn}{01628828}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{10}
      \field{shortjournal}{IEEE Trans. Pattern Anal. Machine Intell.}
      \field{title}{Neural Network Ensembles}
      \field{urlday}{14}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{volume}{12}
      \field{year}{1990}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{993\bibrangedash 1001}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/34.58871
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/ETCU9XEC/Hansen and Salamon - 1990 - Neural network ensembles.pdf
      \endverb
      \verb{urlraw}
      \verb http://ieeexplore.ieee.org/document/58871/
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/document/58871/
      \endverb
    \endentry
    \entry{hardisonSecondlanguageSpokenWord2005}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=1237a296978ca780cea8d988f1c2e8bc}{%
           family={Hardison},
           familyi={H\bibinitperiod},
           given={Debra\bibnamedelima M.},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{1237a296978ca780cea8d988f1c2e8bc}
      \strng{fullhash}{1237a296978ca780cea8d988f1c2e8bc}
      \strng{bibnamehash}{1237a296978ca780cea8d988f1c2e8bc}
      \strng{authorbibnamehash}{1237a296978ca780cea8d988f1c2e8bc}
      \strng{authornamehash}{1237a296978ca780cea8d988f1c2e8bc}
      \strng{authorfullhash}{1237a296978ca780cea8d988f1c2e8bc}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Experiments using the gating paradigm investigated the effects of auditory–visual (AV) and auditoryonly perceptual training on second-language spoken-word identification by Japanese and Korean learners of English. Stimuli were familiar bisyllabic words beginning with /p/, /f/, /ô/, /l/, and /s, t, k/ combined with high, low, and rounded vowels. Results support the priming role of visual cues in AV speech processing. Identification was earlier with visual cues and following training, especially for words beginning with /ô/ and /l/, which also showed significant effects of adjacent vowel. For the Japanese, the AV advantage in identifying /ô/- and /l/-initial words was accentuated following training. Findings are discussed within a multimodal episodic model of learning.}
      \field{issn}{0142-7164, 1469-1817}
      \field{journaltitle}{Applied Psycholinguistics}
      \field{langid}{english}
      \field{month}{10}
      \field{number}{4}
      \field{shortjournal}{Applied Psycholinguistics}
      \field{shorttitle}{Second-Language Spoken Word Identification}
      \field{title}{Second-Language Spoken Word Identification: {{Effects}} of Perceptual Training, Visual Cues, and Phonetic Environment}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{26}
      \field{year}{2005}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{579\bibrangedash 596}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1017/S0142716405050319
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/294NNY9V/Hardison - 2005 - Second-language spoken word identification Effects of perceptual training, visual cues, and phoneti.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/product/identifier/S0142716405050319/type/journal_article
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/product/identifier/S0142716405050319/type/journal_article
      \endverb
    \endentry
    \entry{hedderichSurveyRecentApproaches2021}{online}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=91d8659efb45fc4b4497af15ad3d09e1}{%
           family={Hedderich},
           familyi={H\bibinitperiod},
           given={Michael\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8da45ea26125cf5fa6868b62f46040e8}{%
           family={Lange},
           familyi={L\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8c9a8dad2ae2739191d1554f7597bcdb}{%
           family={Adel},
           familyi={A\bibinitperiod},
           given={Heike},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=647b33fa6024aba71fae94c0b0914c80}{%
           family={Strötgen},
           familyi={S\bibinitperiod},
           given={Jannik},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f440856a11ae15184a2f1802e005ea47}{%
           family={Klakow},
           familyi={K\bibinitperiod},
           given={Dietrich},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4be00ab1a9fdc726558b68fd2ceecdc4}
      \strng{fullhash}{89df11acc283a7c1b578f9dd1a191893}
      \strng{bibnamehash}{89df11acc283a7c1b578f9dd1a191893}
      \strng{authorbibnamehash}{89df11acc283a7c1b578f9dd1a191893}
      \strng{authornamehash}{4be00ab1a9fdc726558b68fd2ceecdc4}
      \strng{authorfullhash}{89df11acc283a7c1b578f9dd1a191893}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.}
      \field{day}{9}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{title}{A {{Survey}} on {{Recent Approaches}} for {{Natural Language Processing}} in {{Low-Resource Scenarios}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2010.12309
      \endverb
      \verb{eprint}
      \verb 2010.12309
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/Q5GCKUZY/Hedderich et al. - 2021 - A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2010.12309
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2010.12309
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Machine Learning}
    \endentry
    \entry{jalalvandAutomaticQualityEstimation2018}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=d2b3ec13bd980b44ffda8a0923293676}{%
           family={Jalalvand},
           familyi={J\bibinitperiod},
           given={Shahab},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0f16b299409e285e906d16569d6b44af}{%
           family={Negri},
           familyi={N\bibinitperiod},
           given={Matteo},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=121cc06517329fdc64d0d73a86f9d3f0}{%
           family={Falavigna},
           familyi={F\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7c481c9fa52dbd24e8ded8dbb342e01d}{%
           family={Matassoni},
           familyi={M\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=abebe32f4d5b1a29c5f5dddab4544482}{%
           family={Turchi},
           familyi={T\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{9493c99e83ed487c697da85f480eb289}
      \strng{fullhash}{cd99c8ab182e8289998d87d6bf73f631}
      \strng{bibnamehash}{cd99c8ab182e8289998d87d6bf73f631}
      \strng{authorbibnamehash}{cd99c8ab182e8289998d87d6bf73f631}
      \strng{authornamehash}{9493c99e83ed487c697da85f480eb289}
      \strng{authorfullhash}{cd99c8ab182e8289998d87d6bf73f631}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recognizer Output Voting Error Reduction (ROVER) has been widely used for system combination in automatic speech recognition (ASR). In order to select the most appropriate words to insert at each position in the output transcriptions, some ROVER extensions rely on critical information such as confidence scores and other ASR decoder features. This information, which is not always available, highly depends on the decoding process and sometimes tends to overestimate the real quality of the recognized words. In this paper we propose a novel variant of ROVER that takes advantage of ASR quality estimation (QE) for ranking the transcriptions at “segment level” instead of: i) relying on confidence scores, or ii) feeding ROVER with randomly ordered hypotheses. We first introduce an effective set of features to compensate for the absence of ASR decoder information. Then, we apply QE techniques to perform accurate hypothesis ranking at segment-level before starting the fusion process. The evaluation is carried out on two different tasks, in which we respectively combine hypotheses coming from independent ASR systems and multi-microphone recordings. In both tasks, it is assumed that the ASR decoder information is not available. The proposed approach significantly outperforms standard ROVER and it is competitive with two strong oracles that exploit prior knowledge about the real quality of the hypotheses to be combined. Compared to standard ROVER, the absolute WER improvements in the two evaluation scenarios range from 0.5\% to 7.3\%.}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{issn}{08852308}
      \field{journaltitle}{Computer Speech \& Language}
      \field{langid}{english}
      \field{month}{1}
      \field{shortjournal}{Computer Speech \& Language}
      \field{title}{Automatic {{Quality Estimation}} for {{ASR System Combination}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{volume}{47}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{214\bibrangedash 239}
      \range{pages}{26}
      \verb{doi}
      \verb 10.1016/j.csl.2017.06.003
      \endverb
      \verb{eprint}
      \verb 1706.07238
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/T4J62QNS/Jalalvand et al. - 2018 - Automatic Quality Estimation for ASR System Combination.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.07238
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.07238
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{joshiStateFateLinguistic2021}{online}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=a71c462a03a7d6e462146be99aa2198b}{%
           family={Joshi},
           familyi={J\bibinitperiod},
           given={Pratik},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d360ddafdeee83df8cf1fd9284e3b834}{%
           family={Santy},
           familyi={S\bibinitperiod},
           given={Sebastin},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1d88603e0d71dfb04a604b7a2c8fdf1c}{%
           family={Budhiraja},
           familyi={B\bibinitperiod},
           given={Amar},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4e7a16b33b42e17420a922fd0fca193b}{%
           family={Bali},
           familyi={B\bibinitperiod},
           given={Kalika},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7a7a17fc7626c6be3b71a694952301e6}{%
           family={Choudhury},
           familyi={C\bibinitperiod},
           given={Monojit},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f5e11edfa5aa66430b2f7eda7b4fdb00}
      \strng{fullhash}{1a8cf437fd3c8babf4a66fe90d7e24b0}
      \strng{bibnamehash}{1a8cf437fd3c8babf4a66fe90d7e24b0}
      \strng{authorbibnamehash}{1a8cf437fd3c8babf4a66fe90d7e24b0}
      \strng{authornamehash}{f5e11edfa5aa66430b2f7eda7b4fdb00}
      \strng{authorfullhash}{1a8cf437fd3c8babf4a66fe90d7e24b0}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.}
      \field{day}{27}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{1}
      \field{pubstate}{prepublished}
      \field{title}{The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2004.09095
      \endverb
      \verb{eprint}
      \verb 2004.09095
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/LSSBQHM5/Joshi et al. - 2021 - The State and Fate of Linguistic Diversity and Inclusion in the NLP World.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2004.09095
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2004.09095
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{jurafskySpeechLanguageProcessing2025}{book}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=3147296c99a3f829087becd1a4eaec08}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9ed8bee488b2703da341cded66bf83cb}{%
           family={Martin},
           familyi={M\bibinitperiod},
           given={James\bibnamedelima H.},
           giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{1098524cc7b74c43e679459ba6a87270}
      \strng{fullhash}{1098524cc7b74c43e679459ba6a87270}
      \strng{bibnamehash}{1098524cc7b74c43e679459ba6a87270}
      \strng{authorbibnamehash}{1098524cc7b74c43e679459ba6a87270}
      \strng{authornamehash}{1098524cc7b74c43e679459ba6a87270}
      \strng{authorfullhash}{1098524cc7b74c43e679459ba6a87270}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{edition}{3rd}
      \field{title}{Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}, with {{Language Models}}}
      \field{year}{2025}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/U3H5C8P2/ed3book_aug25.pdf
      \endverb
      \verb{urlraw}
      \verb https://web.stanford.edu/~jurafsky/slp3/
      \endverb
      \verb{url}
      \verb https://web.stanford.edu/~jurafsky/slp3/
      \endverb
    \endentry
    \entry{khareLowResourceASR2021a}{inproceedings}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=0d41a42f2d548ac2df3cc5c9f85aaa40}{%
           family={Khare},
           familyi={K\bibinitperiod},
           given={Shreya},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=921d590a4a56a7a4d98572ff519a1cc1}{%
           family={Mittal},
           familyi={M\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=16e9afb1408b4271e0cae208bc0078cd}{%
           family={Diwan},
           familyi={D\bibinitperiod},
           given={Anuj},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bd76b27a80da382635760381e5711cfd}{%
           family={Sarawagi},
           familyi={S\bibinitperiod},
           given={Sunita},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a7ab1c163583a97fc87973d4f9423285}{%
           family={Jyothi},
           familyi={J\bibinitperiod},
           given={Preethi},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=778a777c1e999b6bf41e176d6b6de6c5}{%
           family={Bharadwaj},
           familyi={B\bibinitperiod},
           given={Samarth},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{1e7390ba9e60cd023c519e30d231853a}
      \strng{fullhash}{ffe312569ff3cef54786d7cfe03b6002}
      \strng{bibnamehash}{ffe312569ff3cef54786d7cfe03b6002}
      \strng{authorbibnamehash}{ffe312569ff3cef54786d7cfe03b6002}
      \strng{authornamehash}{1e7390ba9e60cd023c519e30d231853a}
      \strng{authorfullhash}{ffe312569ff3cef54786d7cfe03b6002}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Cross-lingual transfer of knowledge from high-resource languages to low-resource languages is an important research problem in automatic speech recognition (ASR). We propose a new strategy of transfer learning by pretraining using large amounts of speech in the high-resource language but with its text transliterated to the target low-resource language. This simple mapping of scripts explicitly encourages increased sharing between the output spaces of both languages and is surprisingly effective even when the high-resource and low-resource languages are from unrelated language families. The utility of our proposed technique is more evident in very low-resource scenarios, where better initializations are more beneficial. We evaluate our technique on a transformer ASR architecture and the state-ofthe-art wav2vec2.0 ASR architecture, with English as the highresource language and six languages as low-resource targets. With access to 1 hour of target speech, we obtain relative WER reductions of up to 8.2\% compared to existing transfer-learning approaches.}
      \field{booktitle}{Interspeech 2021}
      \field{day}{30}
      \field{eventtitle}{Interspeech 2021}
      \field{langid}{english}
      \field{month}{8}
      \field{shorttitle}{Low {{Resource ASR}}}
      \field{title}{Low {{Resource ASR}}: {{The Surprising Effectiveness}} of {{High Resource Transliteration}}}
      \field{urlday}{17}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1529\bibrangedash 1533}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2021-2062
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/8PB5RI2F/Khare et al. - 2021 - Low Resource ASR The Surprising Effectiveness of High Resource Transliteration.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2021/khare21_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2021/khare21_interspeech.html
      \endverb
    \endentry
    \entry{kheirAutomaticPronunciationAssessment2023}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=a6f9b8f7fe4d6b1a66162ee44f2f18bc}{%
           family={Kheir},
           familyi={K\bibinitperiod},
           given={Yassine\bibnamedelima El},
           giveni={Y\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fc6b779fd58dd748836db7347acb08d6}{%
           family={Ali},
           familyi={A\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b4bface8fc882cc6891c3e6e6b84f1f5}{%
           family={Chowdhury},
           familyi={C\bibinitperiod},
           given={Shammur\bibnamedelima Absar},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2717c87823d358b18480521d0b792188}
      \strng{fullhash}{751b2acd09a98d6b04bf6ec1a7867d8d}
      \strng{bibnamehash}{751b2acd09a98d6b04bf6ec1a7867d8d}
      \strng{authorbibnamehash}{751b2acd09a98d6b04bf6ec1a7867d8d}
      \strng{authornamehash}{2717c87823d358b18480521d0b792188}
      \strng{authorfullhash}{751b2acd09a98d6b04bf6ec1a7867d8d}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available resources. This is followed by a discussion of the remaining challenges and possible directions for future work.}
      \field{day}{21}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Automatic {{Pronunciation Assessment}} -- {{A Review}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2310.13974
      \endverb
      \verb{eprint}
      \verb 2310.13974
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/8VY2N9BF/Kheir et al. - 2023 - Automatic Pronunciation Assessment -- A Review.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2310.13974
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2310.13974
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{korzekwaComputerassistedPronunciationTraining2022}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=d6f2d5b085340bf4faa87e419694bf1f}{%
           family={Korzekwa},
           familyi={K\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=487dee4d02326569c9a3d5dfb62c6c98}{%
           family={Lorenzo-Trueba},
           familyi={L\bibinithyphendelim T\bibinitperiod},
           given={Jaime},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cc1e00c8800b9aa1412d2b25783a3844}{%
           family={Drugman},
           familyi={D\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8b9471c61598bbd36a32ca75fcec3d10}{%
           family={Kostek},
           familyi={K\bibinitperiod},
           given={Bozena},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Elsevier BV}%
      }
      \strng{namehash}{61bf961fdab359db9c94d8ba6aa2a305}
      \strng{fullhash}{db2716b7a9e4575980bf8d8a5e9b899c}
      \strng{bibnamehash}{db2716b7a9e4575980bf8d8a5e9b899c}
      \strng{authorbibnamehash}{db2716b7a9e4575980bf8d8a5e9b899c}
      \strng{authornamehash}{61bf961fdab359db9c94d8ba6aa2a305}
      \strng{authorfullhash}{db2716b7a9e4575980bf8d8a5e9b899c}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0167-6393}
      \field{journaltitle}{Speech Communication}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Computer-Assisted Pronunciation Training—{{Speech}} Synthesis Is Almost All You Need}
      \field{urlday}{18}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{volume}{142}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{22\bibrangedash 33}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1016/j.specom.2022.06.003
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/QCIRJ73S/Korzekwa et al. - 2022 - Computer-assisted pronunciation training—Speech synthesis is almost all you need.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0167639322000863
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S0167639322000863
      \endverb
    \endentry
    \entry{krashenPrinciplesPracticeSecond1984}{book}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=8c7d8e92e8726fb99a6b186ccc98cad1}{%
           family={Krashen},
           familyi={K\bibinitperiod},
           given={Stephen\bibnamedelima D.},
           giveni={S\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Oxford}%
      }
      \list{publisher}{1}{%
        {Pergamon Press}%
      }
      \strng{namehash}{8c7d8e92e8726fb99a6b186ccc98cad1}
      \strng{fullhash}{8c7d8e92e8726fb99a6b186ccc98cad1}
      \strng{bibnamehash}{8c7d8e92e8726fb99a6b186ccc98cad1}
      \strng{authorbibnamehash}{8c7d8e92e8726fb99a6b186ccc98cad1}
      \strng{authornamehash}{8c7d8e92e8726fb99a6b186ccc98cad1}
      \strng{authorfullhash}{8c7d8e92e8726fb99a6b186ccc98cad1}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{edition}{Reprinted}
      \field{isbn}{978-0-08-028628-0}
      \field{langid}{english}
      \field{pagetotal}{202}
      \field{series}{Language Teaching Methodology Series}
      \field{title}{Principles and Practice in Second Language Acquisition}
      \field{year}{1984}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/FSTYTSP2/Krashen - 1984 - Principles and practice in second language acquisition.pdf
      \endverb
    \endentry
    \entry{krishenbaumRepresentingIPAPhonetics}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=f107db57a1c2a2c1c7eb35f51fa0bf81}{%
           family={Krishenbaum},
           familyi={K\bibinitperiod},
           given={Evan},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f107db57a1c2a2c1c7eb35f51fa0bf81}
      \strng{fullhash}{f107db57a1c2a2c1c7eb35f51fa0bf81}
      \strng{bibnamehash}{f107db57a1c2a2c1c7eb35f51fa0bf81}
      \strng{authorbibnamehash}{f107db57a1c2a2c1c7eb35f51fa0bf81}
      \strng{authornamehash}{f107db57a1c2a2c1c7eb35f51fa0bf81}
      \strng{authorfullhash}{f107db57a1c2a2c1c7eb35f51fa0bf81}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{langid}{english}
      \field{title}{Representing {{IPA Phonetics}} in {{ASCII}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/D8A9W6JI/Krishenbaum - Representing IPA Phonetics in ASCII.pdf
      \endverb
    \endentry
    \entry{kurzingerCTCSegmentationLargeCorpora2020}{incollection}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=52ff54aeb792c96bdcd0b0f11c659470}{%
           family={Kürzinger},
           familyi={K\bibinitperiod},
           given={Ludwig},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7930df42e2a02b65ff17fd97475eb7dc}{%
           family={Winkelbauer},
           familyi={W\bibinitperiod},
           given={Dominik},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2c3a631af7f3bf630f6f920c8798901f}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Lujun},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=984f7218784a616fbd2673d27be32625}{%
           family={Watzel},
           familyi={W\bibinitperiod},
           given={Tobias},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a412590ea8e1a24b3b627f33dce92dc1}{%
           family={Rigoll},
           familyi={R\bibinitperiod},
           given={Gerhard},
           giveni={G\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{2d09ee0d421b102f591039749f14b5b2}
      \strng{fullhash}{01e813e9dc7fd1000eff27e98be6813f}
      \strng{bibnamehash}{01e813e9dc7fd1000eff27e98be6813f}
      \strng{authorbibnamehash}{01e813e9dc7fd1000eff27e98be6813f}
      \strng{authornamehash}{2d09ee0d421b102f591039749f14b5b2}
      \strng{authorfullhash}{01e813e9dc7fd1000eff27e98be6813f}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent end-to-end Automatic Speech Recognition (ASR) systems demonstrated the ability to outperform conventional hybrid DNN/ HMM ASR. Aside from architectural improvements in those systems, those models grew in terms of depth, parameters and model capacity. However, these models also require more training data to achieve comparable performance.}
      \field{eprintclass}{eess}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{title}{{{CTC-Segmentation}} of {{Large Corpora}} for {{German End-to-end Speech Recognition}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{12335}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{267\bibrangedash 278}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1007/978-3-030-60276-5_27
      \endverb
      \verb{eprint}
      \verb 2007.09127
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/RGBGBXLM/Kürzinger et al. - 2020 - CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2007.09127
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2007.09127
      \endverb
      \keyw{Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{laptevFastEntropyBasedMethods2023}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=be6aebc721bfde5ca511ee8447424b5c}{%
           family={Laptev},
           familyi={L\bibinitperiod},
           given={Aleksandr},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=57b6ee8703cf89267a2d51429a4e61a7}{%
           family={Ginsburg},
           familyi={G\bibinitperiod},
           given={Boris},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0f415fe84c2f10e553b7bd55f0c3a7b8}
      \strng{fullhash}{0f415fe84c2f10e553b7bd55f0c3a7b8}
      \strng{bibnamehash}{0f415fe84c2f10e553b7bd55f0c3a7b8}
      \strng{authorbibnamehash}{0f415fe84c2f10e553b7bd55f0c3a7b8}
      \strng{authornamehash}{0f415fe84c2f10e553b7bd55f0c3a7b8}
      \strng{authorfullhash}{0f415fe84c2f10e553b7bd55f0c3a7b8}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents a class of new fast non-trainable entropy-based confidence estimation methods for automatic speech recognition. We show how per-frame entropy values can be normalized and aggregated to obtain a confidence measure per unit and per word for Connectionist Temporal Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) models. Proposed methods have similar computational complexity to the traditional method based on the maximum per-frame probability, but they are more adjustable, have a wider effective threshold range, and better push apart the confidence distributions of correct and incorrect words. We evaluate the proposed confidence measures on LibriSpeech test sets, and show that they are up to 2 and 4 times better than confidence estimation based on the maximum per-frame probability at detecting incorrect words for Conformer-CTC and Conformer-RNN-T models, respectively.}
      \field{booktitle}{2022 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})}
      \field{day}{9}
      \field{eprintclass}{eess}
      \field{eprinttype}{arXiv}
      \field{month}{1}
      \field{title}{Fast {{Entropy-Based Methods}} of {{Word-Level Confidence Estimation}} for {{End-To-End Automatic Speech Recognition}}}
      \field{urlday}{14}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{152\bibrangedash 159}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/SLT54892.2023.10022960
      \endverb
      \verb{eprint}
      \verb 2212.08703
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/ZZLL4CDB/Laptev and Ginsburg - 2023 - Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-To-End Automatic Speech Recog.pdf;/home/pccady/Zotero/storage/PQNVQW7J/2212.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2212.08703
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2212.08703
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Mathematics - Information Theory}
    \endentry
    \entry{leeMassivelyMultilingualPronunciation}{article}{}
      \name{author}{8}{}{%
        {{un=0,uniquepart=base,hash=5748c2b892be1d82413c87f22fc1ca6a}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Jackson\bibnamedelima L},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d7e4ef8024316ea3b6b4dbf372bf07bd}{%
           family={Ashby},
           familyi={A\bibinitperiod},
           given={Lucas\bibnamedelimb F\bibnamedelima E},
           giveni={L\bibinitperiod\bibinitdelim F\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b3c00e4fb3d470ebb763c0a8960d84e8}{%
           family={Garza},
           familyi={G\bibinitperiod},
           given={M\bibnamedelima Elizabeth},
           giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5f05b63a39ed7d702e7ab066835c1018}{%
           family={Lee-Sikka},
           familyi={L\bibinithyphendelim S\bibinitperiod},
           given={Yeonju},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9cf3c55965c2ebbe8630bf89dfb072e6}{%
           family={Miller},
           familyi={M\bibinitperiod},
           given={Sean},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f6c96f7512f062c016f4319a5d30ef02}{%
           family={Wong},
           familyi={W\bibinitperiod},
           given={Alan},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=264375f1d2aa510997e3012558f9fb6d}{%
           family={McCarthy},
           familyi={M\bibinitperiod},
           given={Arya\bibnamedelima D},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0f46b782a9c4717a7bd161c47f4e57a4}{%
           family={Gorman},
           familyi={G\bibinitperiod},
           given={Kyle},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{63341672a062cb05e50c91b60b0faf28}
      \strng{fullhash}{45582dcb56e8a934576879eb550c20e5}
      \strng{bibnamehash}{45582dcb56e8a934576879eb550c20e5}
      \strng{authorbibnamehash}{45582dcb56e8a934576879eb550c20e5}
      \strng{authornamehash}{63341672a062cb05e50c91b60b0faf28}
      \strng{authorfullhash}{45582dcb56e8a934576879eb550c20e5}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce WikiPron, an open-source command-line tool for extracting pronunciation data from Wiktionary, a collaborative multilingual online dictionary. We first describe the design and use of WikiPron. We then discuss the challenges faced scaling this tool to create an automatically-generated database of 1.7 million pronunciations from 165 languages. Finally, we validate the pronunciation database by using it to train and evaluating a collection of generic grapheme-to-phoneme models. The software, pronunciation data, and models are all made available under permissive open-source licenses.}
      \field{langid}{english}
      \field{title}{Massively {{Multilingual Pronunciation Mining}} with {{WikiPron}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/H79IA2SE/Lee et al. - Massively Multilingual Pronunciation Mining with WikiPron.pdf
      \endverb
    \endentry
    \entry{leungCNNRNNCTCBasedEndtoend2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=f60eaedf0859f064e3be6c223474f4ad}{%
           family={Leung},
           familyi={L\bibinitperiod},
           given={Wai-Kim},
           giveni={W\bibinithyphendelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4504f8cb1cdc2f43bf841919c3a265e6}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Xunying},
           giveni={X\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8659b27d7c10074faa44f75f234ade20}{%
           family={Meng},
           familyi={M\bibinitperiod},
           given={Helen},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Brighton, United Kingdom}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{72a986be6c5692f76ec295c31e7f4d04}
      \strng{fullhash}{9d6cc5fcc69ecc9c45a076b4acede56d}
      \strng{bibnamehash}{9d6cc5fcc69ecc9c45a076b4acede56d}
      \strng{authorbibnamehash}{9d6cc5fcc69ecc9c45a076b4acede56d}
      \strng{authornamehash}{72a986be6c5692f76ec295c31e7f4d04}
      \strng{authorfullhash}{9d6cc5fcc69ecc9c45a076b4acede56d}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper focuses on using Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Connectionist Temporal Classification (CTC) to build an end-to-end speech recognition for Mispronunciation Detection and Diagnosis (MDD) task. Our approach is end-to-end models, while phonemic or graphemic information, or forced alignment between different linguistic units, are not required. We conduct experiments that compare the proposed CNN-RNNCTC approach with alternative mispronunciation detection and diagnoses (MDD) approaches. The F-measure of our approach is 74.65\%, which significantly outperforms the Extended Recognition Network (ERN) (S-AM) by 44.75\% and State-level Acoustic Model (S-AM) by 32.28\% relatively. The relative improvement in F-measure when over Acoustic-Phonemic Model (APM), Acoustic-Graphemic Model (AGM) and Acoustic-Phonemic-Graphemic Model (APGM) are 9.57\%, 5.04\% and 2.77\% respectively.}
      \field{booktitle}{{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
      \field{eventtitle}{{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})}
      \field{isbn}{978-1-4799-8131-1}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{{{CNN-RNN-CTC Based End-to-end Mispronunciation Detection}} and {{Diagnosis}}}
      \field{urlday}{12}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{8132\bibrangedash 8136}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICASSP.2019.8682654
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/8TPKZ7SE/Leung et al. - 2019 - CNN-RNN-CTC Based End-to-end Mispronunciation Detection and Diagnosis.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/8682654/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/8682654/
      \endverb
    \endentry
    \entry{liuEndtoEndUnsupervisedSpeech2023}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=f6dd48342cba0f6385fb3e4ac6d66b33}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Alexander\bibnamedelima H.},
           giveni={A\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=caee6b3fa31c9aa140f1d90048cd989f}{%
           family={Hsu},
           familyi={H\bibinitperiod},
           given={Wei-Ning},
           giveni={W\bibinithyphendelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d3ff0eff0a1d1258618c88f2ec57095f}{%
           family={Auli},
           familyi={A\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e478046d4abc39e9814f6e364a736511}{%
           family={Baevski},
           familyi={B\bibinitperiod},
           given={Alexei},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Doha, Qatar}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{fc540f72dcefc288dabdb01697a71a7d}
      \strng{fullhash}{f1a6737a3b74eee4763172b2d6c3d2d6}
      \strng{bibnamehash}{f1a6737a3b74eee4763172b2d6c3d2d6}
      \strng{authorbibnamehash}{f1a6737a3b74eee4763172b2d6c3d2d6}
      \strng{authornamehash}{fc540f72dcefc288dabdb01697a71a7d}
      \strng{authorfullhash}{f1a6737a3b74eee4763172b2d6c3d2d6}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Unsupervised speech recognition has shown great potential to make Automatic Speech Recognition (ASR) systems accessible to every language. However, existing methods still heavily rely on hand-crafted pre-processing. Similar to the trend of making supervised speech recognition end-to-end, we introduce wav2vec-U 2.0 which does away with all audio-side preprocessing and improves accuracy through better architecture. In addition, we introduce an auxiliary self-supervised objective that ties model predictions back to the input. Experiments show that wav2vec-U 2.0 improves unsupervised recognition results across different languages while being conceptually simpler.}
      \field{booktitle}{2022 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})}
      \field{day}{9}
      \field{eventtitle}{2022 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})}
      \field{isbn}{979-8-3503-9690-4}
      \field{langid}{english}
      \field{month}{1}
      \field{title}{Towards {{End-to-End Unsupervised Speech Recognition}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{221\bibrangedash 228}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/SLT54892.2023.10023187
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/L94ZBXD8/Liu et al. - 2023 - Towards End-to-End Unsupervised Speech Recognition.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/10023187/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/10023187/
      \endverb
    \endentry
    \entry{loEffectiveEndtoEndModeling2020}{online}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=2c3872b127fd05f104f9b0106aae54cc}{%
           family={Lo},
           familyi={L\bibinitperiod},
           given={Tien-Hong},
           giveni={T\bibinithyphendelim H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=36ecdddff8dc6ece837f96196bd108b8}{%
           family={Weng},
           familyi={W\bibinitperiod},
           given={Shi-Yan},
           giveni={S\bibinithyphendelim Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dcc29dd1f42299251722950cd472844c}{%
           family={Chang},
           familyi={C\bibinitperiod},
           given={Hsiu-Jui},
           giveni={H\bibinithyphendelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=713d98413170c37dbfbbc10392d117a4}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Berlin},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8914fefd9bf6de1b28e081223e42cf8c}
      \strng{fullhash}{e60f9c0fd44a79836471b9e4b054611f}
      \strng{bibnamehash}{e60f9c0fd44a79836471b9e4b054611f}
      \strng{authorbibnamehash}{e60f9c0fd44a79836471b9e4b054611f}
      \strng{authornamehash}{8914fefd9bf6de1b28e081223e42cf8c}
      \strng{authorfullhash}{e60f9c0fd44a79836471b9e4b054611f}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, end-to-end (E2E) automatic speech recognition (ASR) systems have garnered tremendous attention because of their great success and unified modeling paradigms in comparison to conventional hybrid DNN-HMM ASR systems. Despite the widespread adoption of E2E modeling frameworks on ASR, there still is a dearth of work on investigating the E2E frameworks for use in computer-assisted pronunciation learning (CAPT), particularly for Mispronunciation detection (MD). In response, we first present a novel use of hybrid CTCAttention approach to the MD task, taking advantage of the strengths of both CTC and the attention-based model meanwhile getting around the need for phone-level forced alignment. Second, we perform input augmentation with text prompt information to make the resulting E2E model more tailored for the MD task. On the other hand, we adopt two MD decision methods so as to better cooperate with the proposed framework: 1) decision-making based on a recognition confidence measure or 2) simply based on speech recognition results. A series of Mandarin MD experiments demonstrate that our approach not only simplifies the processing pipeline of existing hybrid DNN-HMM systems but also brings about systematic and substantial performance improvements. Furthermore, input augmentation with text prompts seems to hold excellent promise for the E2E-based MD approach.}
      \field{day}{18}
      \field{eprintclass}{eess}
      \field{eprinttype}{arXiv}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{title}{An {{Effective End-to-End Modeling Approach}} for {{Mispronunciation Detection}}}
      \field{urlday}{30}
      \field{urlmonth}{8}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2005.08440
      \endverb
      \verb{eprint}
      \verb 2005.08440
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/WDQ3H9TX/Lo et al. - 2020 - An Effective End-to-End Modeling Approach for Mispronunciation Detection.pdf;/home/pccady/Zotero/storage/9DIA6Y8K/2005.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2005.08440
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2005.08440
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{lonerganAutomaticSpeechRecognition}{article}{}
      \name{author}{8}{}{%
        {{un=0,uniquepart=base,hash=9db4af13432a594613e8efe0ea532287}{%
           family={Lonergan},
           familyi={L\bibinitperiod},
           given={Liam},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9c58381cb863012cd1df917bb20d39ee}{%
           family={Qian},
           familyi={Q\bibinitperiod},
           given={Mengjie},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4ca87a8b7ddfc49d2811ec5f34f86af}{%
           family={Berthelsen},
           familyi={B\bibinitperiod},
           given={Harald},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c18def86d87b42fc64a7f14ad616b8d1}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a09def41ef54cd6582b67d86ae3785c1}{%
           family={Wendler},
           familyi={W\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f0032cd1cf7c14995537d7ee5b8edc21}{%
           family={Chiaráin},
           familyi={C\bibinitperiod},
           given={Neasa\bibnamedelima Ní},
           giveni={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f08b34b12657a55637cced651ec89cc3}{%
           family={Gobl},
           familyi={G\bibinitperiod},
           given={Christer},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f2c38cf0d08aca358e56773170a2fe9d}{%
           family={Chasaide},
           familyi={C\bibinitperiod},
           given={Ailbhe\bibnamedelima Ní},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4d34d0315f0b2cb31a08817721dbcd48}
      \strng{fullhash}{c0c6f8a622e8bfe0ad0545ef01b54198}
      \strng{bibnamehash}{c0c6f8a622e8bfe0ad0545ef01b54198}
      \strng{authorbibnamehash}{c0c6f8a622e8bfe0ad0545ef01b54198}
      \strng{authornamehash}{4d34d0315f0b2cb31a08817721dbcd48}
      \strng{authorfullhash}{c0c6f8a622e8bfe0ad0545ef01b54198}
      \field{extraname}{1}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper describes ÉIST, automatic speech recogniser for Irish, developed as part of the ongoing ABAIR initiative, combining (1) acoustic models, (2) pronunciation lexicons and (3) language models into a hybrid system. A priority for now is a system that can deal with the multiple diverse native-speaker dialects. Consequently, (1) was built using predominately native-speaker speech, which included earlier recordings used for synthesis development as well as more diverse recordings obtained using the MíleGlór platform. The pronunciation variation across the dialects is a particular challenge in the development of (2) and is explored by testing both Transdialect and Multi-dialect letter-to-sound rules. Two approaches to language modelling (3) are used in the hybrid system, a simple n-gram model and recurrent neural network lattice rescoring, the latter garnering impressive performance improvements. The system is evaluated using a test set that is comprised of both native and non-native speakers, which allows for some inferences to be made on the performance of the system on both cohorts.}
      \field{langid}{english}
      \field{title}{Automatic {{Speech Recognition}} for {{Irish}}: The {{ABAIR-ÉIST System}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/R2PXEAQD/Lonergan et al. - Automatic Speech Recognition for Irish the ABAIR-ÉIST System.pdf
      \endverb
    \endentry
    \entry{lonerganLowresourceSpeechRecognition2024}{online}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=9db4af13432a594613e8efe0ea532287}{%
           family={Lonergan},
           familyi={L\bibinitperiod},
           given={Liam},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9c58381cb863012cd1df917bb20d39ee}{%
           family={Qian},
           familyi={Q\bibinitperiod},
           given={Mengjie},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f0032cd1cf7c14995537d7ee5b8edc21}{%
           family={Chiaráin},
           familyi={C\bibinitperiod},
           given={Neasa\bibnamedelima Ní},
           giveni={N\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f08b34b12657a55637cced651ec89cc3}{%
           family={Gobl},
           familyi={G\bibinitperiod},
           given={Christer},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f2c38cf0d08aca358e56773170a2fe9d}{%
           family={Chasaide},
           familyi={C\bibinitperiod},
           given={Ailbhe\bibnamedelima Ní},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4d34d0315f0b2cb31a08817721dbcd48}
      \strng{fullhash}{1e12ac34c1c167c26b012a569832ae68}
      \strng{bibnamehash}{1e12ac34c1c167c26b012a569832ae68}
      \strng{authorbibnamehash}{1e12ac34c1c167c26b012a569832ae68}
      \strng{authornamehash}{4d34d0315f0b2cb31a08817721dbcd48}
      \strng{authorfullhash}{1e12ac34c1c167c26b012a569832ae68}
      \field{extraname}{2}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper explores the use of Hybrid CTC/Attention encoderdecoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID). Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN). An optimal InterCTC setting is initially established using a Conformer encoder. This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared. A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion. The experiments yielded an improvement in DID accuracy of 10.8\% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{5}
      \field{pubstate}{prepublished}
      \field{title}{Low-Resource Speech Recognition and Dialect Identification of {{Irish}} in a Multi-Task Framework}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2405.01293
      \endverb
      \verb{eprint}
      \verb 2405.01293
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/7DGRSRXD/Lonergan et al. - 2024 - Low-resource speech recognition and dialect identification of Irish in a multi-task framework.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2405.01293
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2405.01293
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{maclinEmpiricalEvaluationBagging1997}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=67e7e9ccee4a8236d263b7c55fbc33d6}{%
           family={Maclin},
           familyi={M\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b5a42e2c5fa6f1d90400809185645e27}{%
           family={Opitz},
           familyi={O\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{6b6035f065dafd77b4cdbf726505d76d}
      \strng{fullhash}{6b6035f065dafd77b4cdbf726505d76d}
      \strng{bibnamehash}{6b6035f065dafd77b4cdbf726505d76d}
      \strng{authorbibnamehash}{6b6035f065dafd77b4cdbf726505d76d}
      \strng{authornamehash}{6b6035f065dafd77b4cdbf726505d76d}
      \strng{authorfullhash}{6b6035f065dafd77b4cdbf726505d76d}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{AAAI/IAAI}
      \field{title}{An Empirical Evaluation of Bagging and Boosting}
      \field{urlday}{17}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{volume}{1997}
      \field{year}{1997}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{546\bibrangedash 551}
      \range{pages}{6}
      \verb{file}
      \verb /home/pccady/Zotero/storage/8ZXEJKA6/An_Empirical_Evaluation_of_Bagging_and_B20220105-9764-16y0t8b.pdf
      \endverb
      \verb{urlraw}
      \verb https://d1wqtxts1xzle7.cloudfront.net/78173773/An_Empirical_Evaluation_of_Bagging_and_B20220105-9764-16y0t8b.pdf?1738464338=&response-content-disposition=inline%3B+filename%3DAn_empirical_evaluation_of_bagging_and_b.pdf&Expires=1758100511&Signature=ULr42EX~oPCHa6soKPN0avlV0aJeaV5Pq96lNHezXzPN3CaB2CYdPJi1atYw3dXBQF04naJPYSQicHHyAQrqLNgXwSF8xAx6EFU4g~CBnmOa5vnYQeQJTrcJlOvLUwbAHW0iZf8T8lNgeP-c~aImdq1OYrzWsEe2HQ6e-q77emb2VM9p~xsFxMKAn~5RU2w6d~x3iQ4n-YVaAeUbPdAtWFlysMTlSow56C4xK4KupsOSUA3qlXeq9PTBaWskjFVCcSAlfJpm06hDOLrZtwO3O821qdsrh1HsXxoTTWa2Qbh1~Ra-ZHwXQt-o2~XKDva8-HeQxOX7x~6FrBsf7HBo6A__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA
      \endverb
      \verb{url}
      \verb https://d1wqtxts1xzle7.cloudfront.net/78173773/An_Empirical_Evaluation_of_Bagging_and_B20220105-9764-16y0t8b.pdf?1738464338=&response-content-disposition=inline%3B+filename%3DAn_empirical_evaluation_of_bagging_and_b.pdf&Expires=1758100511&Signature=ULr42EX~oPCHa6soKPN0avlV0aJeaV5Pq96lNHezXzPN3CaB2CYdPJi1atYw3dXBQF04naJPYSQicHHyAQrqLNgXwSF8xAx6EFU4g~CBnmOa5vnYQeQJTrcJlOvLUwbAHW0iZf8T8lNgeP-c~aImdq1OYrzWsEe2HQ6e-q77emb2VM9p~xsFxMKAn~5RU2w6d~x3iQ4n-YVaAeUbPdAtWFlysMTlSow56C4xK4KupsOSUA3qlXeq9PTBaWskjFVCcSAlfJpm06hDOLrZtwO3O821qdsrh1HsXxoTTWa2Qbh1~Ra-ZHwXQt-o2~XKDva8-HeQxOX7x~6FrBsf7HBo6A__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA
      \endverb
    \endentry
    \entry{magueresseLowresourceLanguagesReview2020}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=547d6e06081308be36083db96b8edb76}{%
           family={Magueresse},
           familyi={M\bibinitperiod},
           given={Alexandre},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=450e744798cbc184fb1059a5477ff238}{%
           family={Carles},
           familyi={C\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6bde934679345b5af7be71ea444087eb}{%
           family={Heetderks},
           familyi={H\bibinitperiod},
           given={Evan},
           giveni={E\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{644116805dc710844d9d4236aaea88df}
      \strng{fullhash}{95e06356a259e31da21ef118d7bab191}
      \strng{bibnamehash}{95e06356a259e31da21ef118d7bab191}
      \strng{authorbibnamehash}{95e06356a259e31da21ef118d7bab191}
      \strng{authornamehash}{644116805dc710844d9d4236aaea88df}
      \strng{authorfullhash}{95e06356a259e31da21ef118d7bab191}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A current problem in NLP is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.}
      \field{day}{12}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Low-Resource {{Languages}}}
      \field{title}{Low-Resource {{Languages}}: {{A Review}} of {{Past Work}} and {{Future Challenges}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2006.07264
      \endverb
      \verb{eprint}
      \verb 2006.07264
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/CY6KQTVS/Magueresse et al. - 2020 - Low-resource Languages A Review of Past Work and Future Challenges.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2006.07264
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2006.07264
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{mcauliffeMontrealForcedAligner2017}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=ce2a2ead4a327127b7974e8890793132}{%
           family={McAuliffe},
           familyi={M\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4648923e13e3da9e84781c77909a9ae1}{%
           family={Socolof},
           familyi={S\bibinitperiod},
           given={Michaela},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e3f4502e4320978fe326c5dcf903b7ef}{%
           family={Mihuc},
           familyi={M\bibinitperiod},
           given={Sarah},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b6ea5a7032ae19a0dc619b901a2d57ac}{%
           family={Wagner},
           familyi={W\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0e231db1ecb79efa4eee1aa7f444f931}{%
           family={Sonderegger},
           familyi={S\bibinitperiod},
           given={Morgan},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{243e0211a6d6886eba872cacd33fc9ff}
      \strng{fullhash}{5050368be03577e65240aa94a70df784}
      \strng{bibnamehash}{5050368be03577e65240aa94a70df784}
      \strng{authorbibnamehash}{5050368be03577e65240aa94a70df784}
      \strng{authornamehash}{243e0211a6d6886eba872cacd33fc9ff}
      \strng{authorfullhash}{5050368be03577e65240aa94a70df784}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present the Montreal Forced Aligner (MFA), a new opensource system for speech-text alignment. MFA is an update to the Prosodylab-Aligner, and maintains its key functionality of trainability on new data, as well as incorporating improved architecture (triphone acoustic models and speaker adaptation), and other features. MFA uses Kaldi instead of HTK, allowing MFA to be distributed as a stand-alone package, and to exploit parallel processing for computationally-intensive training and scaling to larger datasets. We evaluate MFA’s performance on aligning word and phone boundaries in English conversational and laboratory speech, relative to human-annotated boundaries, focusing on the effects of aligner architecture and training on the data to be aligned. MFA performs well relative to two existing open-source aligners with simpler architecture (Prosodylab-Aligner and FAVE), and both its improved architecture and training on data to be aligned generally result in more accurate boundaries.}
      \field{booktitle}{Interspeech 2017}
      \field{day}{20}
      \field{eventtitle}{Interspeech 2017}
      \field{langid}{english}
      \field{month}{8}
      \field{shorttitle}{Montreal {{Forced Aligner}}}
      \field{title}{Montreal {{Forced Aligner}}: {{Trainable Text-Speech Alignment Using Kaldi}}}
      \field{urlday}{8}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{498\bibrangedash 502}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2017-1386
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/TZ5ZLLHY/McAuliffe et al. - 2017 - Montreal Forced Aligner Trainable Text-Speech Alignment Using Kaldi.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2017/mcauliffe17_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2017/mcauliffe17_interspeech.html
      \endverb
    \endentry
    \entry{mechuraIntroductionGramadanIrish}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=aecec86c01f11c26b57516f859b2731d}{%
           family={Měchura},
           familyi={M\bibinitperiod},
           given={Michal\bibnamedelima Boleslav},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{aecec86c01f11c26b57516f859b2731d}
      \strng{fullhash}{aecec86c01f11c26b57516f859b2731d}
      \strng{bibnamehash}{aecec86c01f11c26b57516f859b2731d}
      \strng{authorbibnamehash}{aecec86c01f11c26b57516f859b2731d}
      \strng{authornamehash}{aecec86c01f11c26b57516f859b2731d}
      \strng{authorfullhash}{aecec86c01f11c26b57516f859b2731d}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{langid}{english}
      \field{title}{Introduction to {{Gramadán}} and the {{Irish National Morphology Database}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/G68B8EGJ/Měchura - Introduction to Gramadán and the Irish National Morphology Database.pdf
      \endverb
    \endentry
    \entry{mortensenPanPhonResourceMapping}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=cfcb6ba1595b0f5110004216d07213ef}{%
           family={Mortensen},
           familyi={M\bibinitperiod},
           given={David\bibnamedelima R},
           giveni={D\bibinitperiod\bibinitdelim R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d8b3ccfeee2656a6a880b39b2e82da8d}{%
           family={Littell},
           familyi={L\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bcce6929cc46f034e5d0081f0cf3a4e9}{%
           family={Bharadwaj},
           familyi={B\bibinitperiod},
           given={Akash},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1a7a3857a2c57825329a92a115a0c7c9}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Kartik},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc0d6b065a6566cde5d6b93214be1b55}{%
           family={Dyer},
           familyi={D\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8c7cf93c3f39a697d31a692e7b289399}{%
           family={Levin},
           familyi={L\bibinitperiod},
           given={Lori},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{297c54b4ae4eadf3bde5b95429c73043}
      \strng{fullhash}{854aa5eb043b3b7787dedb3334efbe3d}
      \strng{bibnamehash}{854aa5eb043b3b7787dedb3334efbe3d}
      \strng{authorbibnamehash}{854aa5eb043b3b7787dedb3334efbe3d}
      \strng{authornamehash}{297c54b4ae4eadf3bde5b95429c73043}
      \strng{authorfullhash}{854aa5eb043b3b7787dedb3334efbe3d}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper contributes to a growing body of evidence that—when coupled with appropriate machine-learning techniques—linguistically motivated, information-rich representations can outperform one-hot encodings of linguistic data. In particular, we show that phonological features outperform character-based models using the PanPhon resource. PanPhon is a database relating over 5,000 IPA segments to 21 subsegmental articulatory features. We show that this database boosts performance in various NER-related tasks. Phonologically aware, neural CRF models built on PanPhon features are able to perform comparably to character-based models on monolingual Spanish and Turkish NER tasks. On transfer models (as between Uzbek and Turkish) they have been shown to perform better. Furthermore, PanPhon features also contribute measurably to Orthography-to-IPA conversion tasks.}
      \field{langid}{english}
      \field{title}{{{PanPhon}}: {{A Resource}} for {{Mapping IPA Segments}} to {{Articulatory Feature Vectors}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/CZLCJ9BG/Mortensen et al. - PanPhon A Resource for Mapping IPA Segments to Articulatory Feature Vectors.pdf
      \endverb
    \endentry
    \entry{needlemanGeneralMethodApplicable1970}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=51aa8aae5dbbb5be7b45030d1d1f4c55}{%
           family={Needleman},
           familyi={N\bibinitperiod},
           given={Saul\bibnamedelima B},
           giveni={S\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=17a8ad5574620bef562ad96adac8d254}{%
           family={Wunsch},
           familyi={W\bibinitperiod},
           given={Christian\bibnamedelima D},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b4f528fa64d350ccf1a3ce4cf7527e42}
      \strng{fullhash}{b4f528fa64d350ccf1a3ce4cf7527e42}
      \strng{bibnamehash}{b4f528fa64d350ccf1a3ce4cf7527e42}
      \strng{authorbibnamehash}{b4f528fa64d350ccf1a3ce4cf7527e42}
      \strng{authornamehash}{b4f528fa64d350ccf1a3ce4cf7527e42}
      \strng{authorfullhash}{b4f528fa64d350ccf1a3ce4cf7527e42}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of molecular biology}
      \field{number}{3}
      \field{title}{A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins}
      \field{volume}{48}
      \field{year}{1970}
      \field{dateera}{ce}
      \field{pages}{443\bibrangedash 453}
      \range{pages}{11}
      \verb{file}
      \verb /home/pccady/Zotero/storage/DXM3Z5PA/needleman-wunsch-orig-dyn-prog-paper.pdf
      \endverb
    \endentry
    \entry{neriEffectivenessComputerAssisted2008}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=b195770cf95016dea6d62eb2e6481271}{%
           family={Neri},
           familyi={N\bibinitperiod},
           given={Ambra},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c714d19c66ed5817bbb3624e884c2701}{%
           family={Mich},
           familyi={M\bibinitperiod},
           given={Ornella},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4ae276102dfc13c2ba79c60718c9d364}{%
           family={Gerosa},
           familyi={G\bibinitperiod},
           given={Matteo},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1771e6958f569e27cb59e5495c44fe4c}{%
           family={Giuliani},
           familyi={G\bibinitperiod},
           given={Diego},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{d31cc3a690039757180cbfba76f9a356}
      \strng{fullhash}{3a9763a127841339468336a93ab5f4f6}
      \strng{bibnamehash}{3a9763a127841339468336a93ab5f4f6}
      \strng{authorbibnamehash}{3a9763a127841339468336a93ab5f4f6}
      \strng{authornamehash}{d31cc3a690039757180cbfba76f9a356}
      \strng{authorfullhash}{3a9763a127841339468336a93ab5f4f6}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0958-8221, 1744-3210}
      \field{journaltitle}{Computer Assisted Language Learning}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{5}
      \field{shortjournal}{Computer Assisted Language Learning}
      \field{title}{The Effectiveness of Computer Assisted Pronunciation Training for Foreign Language Learning by Children}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{21}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{393\bibrangedash 408}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1080/09588220802447651
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/6QN6VKGQ/Neri et al. - 2008 - The effectiveness of computer assisted pronunciation training for foreign language learning by child.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.tandfonline.com/doi/full/10.1080/09588220802447651
      \endverb
      \verb{url}
      \verb https://www.tandfonline.com/doi/full/10.1080/09588220802447651
      \endverb
    \endentry
    \entry{nichasaideSPEECHTECHNOLOGYDOCUMENTATION2015}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=df42ebb5984d809e2c235d3381017d65}{%
           family={Ní\bibnamedelima Chasaide},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Ailbhe},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ff6612f89e173ef3f99da3d8e3198ec0}{%
           family={Ní\bibnamedelima Chiaráin},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Neasa},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4ca87a8b7ddfc49d2811ec5f34f86af}{%
           family={Berthelsen},
           familyi={B\bibinitperiod},
           given={Harald},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a09def41ef54cd6582b67d86ae3785c1}{%
           family={Wendler},
           familyi={W\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e1aebdbcb0261db75d2b0ad47acfc629}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8e831393bc76f0bb7a19553b5cdebb39}
      \strng{fullhash}{ab5d536e3e8a311c11a0ab85aef59c85}
      \strng{bibnamehash}{ab5d536e3e8a311c11a0ab85aef59c85}
      \strng{authorbibnamehash}{ab5d536e3e8a311c11a0ab85aef59c85}
      \strng{authornamehash}{8e831393bc76f0bb7a19553b5cdebb39}
      \strng{authorfullhash}{ab5d536e3e8a311c11a0ab85aef59c85}
      \field{extraname}{1}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Developing speech technology such as text-tospeech (TTS), requiring as it does a raft of phonetic and linguistic resources, can provide a powerful way to document endangered languages. Drawing on the experience of the ABAIR initiative, developing such resources for Irish [1], we illustrate how both the technology and the underpinning resources can be exploited in a variety of ways that can contribute to the preservation and revitalisation of these languages. By enabling new avenues of application, they can further help address the particular challenges that face the language users and learners. To maximise the immediate and downstream impact, resource development should ideally involve linguistically transparent, rule-based approaches, rather than the machine learning approaches typical of the commercially driven TTS systems for major world languages.}
      \field{langid}{english}
      \field{title}{{{SPEECH TECHNOLOGY AS DOCUMENTATION FOR ENDANGERED LANGUAGE PRESERVATION}}: {{THE CASE OF IRISH}}}
      \field{year}{2015}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/FUC4REGR/Chasaide et al. - SPEECH TECHNOLOGY AS DOCUMENTATION FOR ENDANGERED LANGUAGE PRESERVATION THE CASE OF IRISH.pdf
      \endverb
    \endentry
    \entry{nichasaideCanWeDefuse2019}{inproceedings}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=df42ebb5984d809e2c235d3381017d65}{%
           family={Ní\bibnamedelima Chasaide},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Ailbhe},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ff6612f89e173ef3f99da3d8e3198ec0}{%
           family={Ní\bibnamedelima Chiaráin},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Neasa},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4ca87a8b7ddfc49d2811ec5f34f86af}{%
           family={Berthelsen},
           familyi={B\bibinitperiod},
           given={Harald},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a09def41ef54cd6582b67d86ae3785c1}{%
           family={Wendler},
           familyi={W\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e1aebdbcb0261db75d2b0ad47acfc629}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=18bd2dc72c7073b77bf1974652a08a0b}{%
           family={Barnes},
           familyi={B\bibinitperiod},
           given={Emily},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f08b34b12657a55637cced651ec89cc3}{%
           family={Gobl},
           familyi={G\bibinitperiod},
           given={Christer},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {European Language Resources Association (ELRA)}%
      }
      \strng{namehash}{8e831393bc76f0bb7a19553b5cdebb39}
      \strng{fullhash}{a8de1c9e393d47859bb1c41fdc1df924}
      \strng{bibnamehash}{a8de1c9e393d47859bb1c41fdc1df924}
      \strng{authorbibnamehash}{a8de1c9e393d47859bb1c41fdc1df924}
      \strng{authornamehash}{8e831393bc76f0bb7a19553b5cdebb39}
      \strng{authorfullhash}{a8de1c9e393d47859bb1c41fdc1df924}
      \field{extraname}{2}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Does speech/language technology represent a 'digital timebomb' - or an unprecedented opportunity - for minority and indigenous languages? For successful outcomes, technology development must address linguistic challenges, answer to the needs of the local language communities, enlisting them as a central partner in development. The Irish language ABAIR initiative is building (i) linguistic resources, (ii) core technologies, and (iii) applications for public, educational and access/disability use. The Government’s Digital Plan for Irish Speech and Language Technology provides a model of the support needed by minority languages in the digital age, if the language is to feature in everyday community activities.}
      \field{booktitle}{Proceedings of the {{Language Technologies}} for {{All}} ({{LT4All}})}
      \field{eventtitle}{Proceedings of the {{Language Technologies}} for {{All}} ({{LT4All}})}
      \field{langid}{english}
      \field{title}{Can We Defuse the Digital Timebomb? Linguistics, Speech Technology and the Irish Language Community}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{177\bibrangedash 181}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/SpeechProsody.2016-73
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/HEZYJ5BN/O'Reilly and Ní Chasaide - 2016 - Modelling the timing and scaling of nuclear pitch accents of Connaught and Ulster Irish with the Fuj.pdf
      \endverb
      \verb{urlraw}
      \verb https://lt4all.elra.info/media/papers/O8/97.pdf
      \endverb
      \verb{url}
      \verb https://lt4all.elra.info/media/papers/O8/97.pdf
      \endverb
    \endentry
    \entry{niehuesModelingConfidenceSequencetoSequence2019}{online}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=fc7be2e5c38739e7e2193e9914dcf687}{%
           family={Niehues},
           familyi={N\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1f0f32a331520c55ce998e84d0cc876f}{%
           family={Pham},
           familyi={P\bibinitperiod},
           given={Ngoc-Quan},
           giveni={N\bibinithyphendelim Q\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{4581985def52a566f4b97816ae2b2114}
      \strng{fullhash}{4581985def52a566f4b97816ae2b2114}
      \strng{bibnamehash}{4581985def52a566f4b97816ae2b2114}
      \strng{authorbibnamehash}{4581985def52a566f4b97816ae2b2114}
      \strng{authornamehash}{4581985def52a566f4b97816ae2b2114}
      \strng{authorfullhash}{4581985def52a566f4b97816ae2b2114}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently, significant improvements have been achieved in various natural language processing tasks using neural sequence-to-sequence models. While aiming for the best generation quality is important, ultimately it is also necessary to develop models that can assess the quality of their output.}
      \field{day}{4}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Modeling {{Confidence}} in {{Sequence-to-Sequence Models}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1910.01859
      \endverb
      \verb{eprint}
      \verb 1910.01859
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/G7NFKM7V/Niehues and Pham - 2019 - Modeling Confidence in Sequence-to-Sequence Models.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1910.01859
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1910.01859
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{papadopoulosConfidenceEstimationMethods2001}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=9b2d737bc5f1b52cf5c3c8e2aa38dc24}{%
           family={Papadopoulos},
           familyi={P\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2210d266481278c94fe97920021ad0d3}{%
           family={Edwards},
           familyi={E\bibinitperiod},
           given={P.J.},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d8509939f0fd7733a2d5b4366483fc0}{%
           family={Murray},
           familyi={M\bibinitperiod},
           given={A.F.},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{ef90f8808baccba2d4be71ab4e654ed9}
      \strng{fullhash}{ae82c171de0f672330b78808e3d444f5}
      \strng{bibnamehash}{ae82c171de0f672330b78808e3d444f5}
      \strng{authorbibnamehash}{ae82c171de0f672330b78808e3d444f5}
      \strng{authornamehash}{ef90f8808baccba2d4be71ab4e654ed9}
      \strng{authorfullhash}{ae82c171de0f672330b78808e3d444f5}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Feedforward neural networks, particularly multilayer perceptrons, are widely used in regression and classification tasks. A reliable and practical measure of prediction confidence is essential. In this work three alternative approaches to prediction confidence estimation are presented and compared. The three methods are the maximum likelihood, approximate Bayesian, and the bootstrap technique. We consider prediction uncertainty owing to both data noise and model parameter misspecification. The methods are tested on a number of controlled artificial problems and a real, industrial regression application, the prediction of paper “curl.” Confidence estimation performance is assessed by calculating the mean and standard deviation of the prediction interval coverage probability. We show that treating data noise variance as a function of the inputs is appropriate for the curl prediction task. Moreover, we show that the mean coverage probability can only gauge confidence estimation performance as an average over the input space, i.e., global performance and that the standard deviation of the coverage is unreliable as a measure of local performance. The approximate Bayesian approach is found to perform better in terms of global performance.}
      \field{issn}{10459227}
      \field{journaltitle}{IEEE Transactions on Neural Networks}
      \field{langid}{english}
      \field{month}{11}
      \field{number}{6}
      \field{shortjournal}{IEEE Trans. Neural Netw.}
      \field{shorttitle}{Confidence Estimation Methods for Neural Networks}
      \field{title}{Confidence Estimation Methods for Neural Networks: A Practical Comparison}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{12}
      \field{year}{2001}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1278\bibrangedash 1287}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/72.963764
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/DJHQC45P/Papadopoulos et al. - 2001 - Confidence estimation methods for neural networks a practical comparison.pdf
      \endverb
      \verb{urlraw}
      \verb http://ieeexplore.ieee.org/document/963764/
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/document/963764/
      \endverb
    \endentry
    \entry{parikhEvaluatingLogitBasedGOP2025}{online}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=fc93c98a611040232e6069d29f8aae1b}{%
           family={Parikh},
           familyi={P\bibinitperiod},
           given={Aditya\bibnamedelima Kamlesh},
           giveni={A\bibinitperiod\bibinitdelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1bf7c1442dfe9876ec9313e10af2c2df}{%
           family={Tejedor-Garcia},
           familyi={T\bibinithyphendelim G\bibinitperiod},
           given={Cristian},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6a712139715a50e15afa4f4800444e61}{%
           family={Cucchiarini},
           familyi={C\bibinitperiod},
           given={Catia},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a872bcf73b844088551245ffed26cc56}{%
           family={Strik},
           familyi={S\bibinitperiod},
           given={Helmer},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{780b8649c2e49c452810caf6c8dbd81c}
      \strng{fullhash}{1e7346a472aa8aebe0ff706fc9baa5c6}
      \strng{bibnamehash}{1e7346a472aa8aebe0ff706fc9baa5c6}
      \strng{authorbibnamehash}{1e7346a472aa8aebe0ff706fc9baa5c6}
      \strng{authornamehash}{780b8649c2e49c452810caf6c8dbd81c}
      \strng{authorfullhash}{1e7346a472aa8aebe0ff706fc9baa5c6}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment.}
      \field{day}{8}
      \field{eprintclass}{eess}
      \field{eprinttype}{arXiv}
      \field{month}{7}
      \field{pubstate}{prepublished}
      \field{title}{Evaluating {{Logit-Based GOP Scores}} for {{Mispronunciation Detection}}}
      \field{urlday}{31}
      \field{urlmonth}{8}
      \field{urlyear}{2025}
      \field{year}{2025}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2506.12067
      \endverb
      \verb{eprint}
      \verb 2506.12067
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/ETG9BVAH/Parikh et al. - 2025 - Evaluating Logit-Based GOP Scores for Mispronunciation Detection.pdf;/home/pccady/Zotero/storage/AYCNULJN/2506.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2506.12067
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2506.12067
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{paszkePyTorchImperativeStyle2019}{inproceedings}{}
      \name{author}{21}{}{%
        {{un=0,uniquepart=base,hash=56bf0b340039cf8594436a624ff548a9}{%
           family={Paszke},
           familyi={P\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4ba5062e5919c814aceec188d54c01f2}{%
           family={Gross},
           familyi={G\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5dfae4582081d649e3a0d5342050016}{%
           family={Massa},
           familyi={M\bibinitperiod},
           given={Francisco},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b5815e1692fa2d0c1f44eecf509bd7c4}{%
           family={Lerer},
           familyi={L\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f897ed422c34d95af2e22778dfc2607e}{%
           family={Chanan},
           familyi={C\bibinitperiod},
           given={Gregory},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=046269e070246feb6f394141db80ed87}{%
           family={Killeen},
           familyi={K\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c40352c194e60a3ef458ee7e8685afb5}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Zeming},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6e45f49ec618e619efad90c8e8a61f0c}{%
           family={Gimelshein},
           familyi={G\bibinitperiod},
           given={Natalia},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f65a80959d520337ae99a0798515036c}{%
           family={Antiga},
           familyi={A\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=954cf7680b6ce14813973eccdca3c4bc}{%
           family={Desmaison},
           familyi={D\bibinitperiod},
           given={Alban},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c1b8f8db68d6667b9f2f9a9a3567721b}{%
           family={Kopf},
           familyi={K\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9e701339e56fd0b171145b08288a1b7}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f9535be511fd2fa346093e63b8e61a0}{%
           family={DeVito},
           familyi={D\bibinitperiod},
           given={Zachary},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d814afaa50b9e22ab92cc9f8f9a9e43a}{%
           family={Raison},
           familyi={R\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3feeeebee8583ecc208f7fb3e0a55068}{%
           family={Tejani},
           familyi={T\bibinitperiod},
           given={Alykhan},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e18536d5cb7543731fbf2ca1a4908732}{%
           family={Chilamkurthy},
           familyi={C\bibinitperiod},
           given={Sasank},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0a0b028c6b85c46f368317d0c5bfe3a0}{%
           family={Steiner},
           familyi={S\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=998a001f16bb57c079c1d5afb1cb02c8}{%
           family={Fang},
           familyi={F\bibinitperiod},
           given={Lu},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3f19c633bbfb847db6a0e71d3659eacd}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Junjie},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8ef51a0906e47d2b4472c4e714ed598f}{%
           family={Chintala},
           familyi={C\bibinitperiod},
           given={Soumith},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{fullhash}{ba1e2da270d08cb8de2856498a028fed}
      \strng{bibnamehash}{ba1e2da270d08cb8de2856498a028fed}
      \strng{authorbibnamehash}{ba1e2da270d08cb8de2856498a028fed}
      \strng{authornamehash}{724e74fc18651eb78eb82fbcd1d9dfb1}
      \strng{authorfullhash}{ba1e2da270d08cb8de2856498a028fed}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{shorttitle}{{{PyTorch}}}
      \field{title}{{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}}
      \field{urlday}{8}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{volume}{32}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/IVX7WQNM/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html
      \endverb
    \endentry
    \entry{pengStudyFineTuningWav2vec202021}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=f737eec8f3f2fb0e9d3b00c5f5aab5ba}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Linkai},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=215351f381b0789cbfa01174d2994890}{%
           family={Fu},
           familyi={F\bibinitperiod},
           given={Kaiqi},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86728d3a00a7b01605eba98ae7c69da3}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Binghuai},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=240852ee544f176c02ca5edaf106318f}{%
           family={Ke},
           familyi={K\bibinitperiod},
           given={Dengfeng},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a3c2910886a48ae317b98f66582d9c00}{%
           family={Zhan},
           familyi={Z\bibinitperiod},
           given={Jinsong},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{342e9ade5815c9036dbd5d60ff7c8d22}
      \strng{fullhash}{f08c52824649848cad0a343077d417f9}
      \strng{bibnamehash}{f08c52824649848cad0a343077d417f9}
      \strng{authorbibnamehash}{f08c52824649848cad0a343077d417f9}
      \strng{authornamehash}{342e9ade5815c9036dbd5d60ff7c8d22}
      \strng{authorfullhash}{f08c52824649848cad0a343077d417f9}
      \field{extraname}{1}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Mispronunciation detection and diagnosis (MDD) technology is a key component of computer-assisted pronunciation training system (CAPT). The mainstream method is based on deep neural network automatic speech recognition. Unfortunately, the technique requires massive human-annotated speech recordings for training. Due to the huge variations in mother tongue, age, and proficiency level among second language learners, it is difficult to gather a large amount of matching data for acoustic model training, which greatly limits the model performance. In this paper, we explore the use of Self-Supervised Pretraining (SSP) model wav2vec2.0 for MDD tasks. SSP utilizes a large unlabelled dataset to learn general representation and can be applied in downstream tasks. We conduct experiments using two publicly available datasets (TIMIT, L2-arctic) and our best system achieves 60.44\% f1-score. Moreover, our method is able to achieve 55.52\% f1-score with 3 times less data, which demonstrates the effectiveness of SSP on MDD1.}
      \field{booktitle}{Interspeech 2021}
      \field{day}{30}
      \field{eventtitle}{Interspeech 2021}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{A {{Study}} on {{Fine-Tuning}} Wav2vec2.0 {{Model}} for the {{Task}} of {{Mispronunciation Detection}} and {{Diagnosis}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4448\bibrangedash 4452}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2021-1344
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/4VWGN4LM/Peng et al. - 2021 - A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2021/peng21e_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2021/peng21e_interspeech.html
      \endverb
    \endentry
    \entry{pengEndtoEndMispronunciationDetection2023}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=f737eec8f3f2fb0e9d3b00c5f5aab5ba}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Linkai},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a535a6e206309a4dfa99e76d4fa57128}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Yingming},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1fb99a9f00d4bc94ba579e6709c254ec}{%
           family={Bao},
           familyi={B\bibinitperiod},
           given={Rian},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c15dca1dd47bb240c99f7cf6481272a5}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Ya},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=74f3067f20603e85bac325f904c6b552}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jinsong},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{342e9ade5815c9036dbd5d60ff7c8d22}
      \strng{fullhash}{fef3b9b57845bde63efa428d7f936bad}
      \strng{bibnamehash}{fef3b9b57845bde63efa428d7f936bad}
      \strng{authorbibnamehash}{fef3b9b57845bde63efa428d7f936bad}
      \strng{authornamehash}{342e9ade5815c9036dbd5d60ff7c8d22}
      \strng{authorfullhash}{fef3b9b57845bde63efa428d7f936bad}
      \field{extraname}{2}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{As an indispensable module of computer-aided pronunciation training (CAPT) systems, mispronunciation detection and diagnosis (MDD) techniques have attracted a lot of attention from academia and industry over the past decade. To train robust MDD models, this technique requires massive human-annotated speech recordings which are usually expensive and even hard to acquire. In this study, we propose to use transfer learning to tackle the problem of data scarcity from two aspects. First, from audio modality, we explore the use of the pretrained model wav2vec2.0 for MDD tasks by learning robust general acoustic representation. Second, from text modality, we explore transferring prior texts into MDD by learning associations between acoustic and textual modalities. We propose textual modulation gates that assign more importance to the relevant text information while suppressing irrelevant text information. Moreover, given the transcriptions, we propose an extra contrastive loss to reduce the difference of learning objectives between the phoneme recognition and MDD tasks. Conducting experiments on the L2-Arctic dataset showed that our wav2vec2.0 based models outperformed the conventional methods. The proposed textual modulation gate and contrastive loss further improved the F1-score by more than 2.88\% and our best model achieved an F1-score of 61.75\%.}
      \field{day}{2}
      \field{issn}{2076-3417}
      \field{journaltitle}{Applied Sciences}
      \field{langid}{english}
      \field{month}{6}
      \field{number}{11}
      \field{shortjournal}{Applied Sciences}
      \field{title}{End-to-{{End Mispronunciation Detection}} and {{Diagnosis Using Transfer Learning}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{volume}{13}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{6793}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/app13116793
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/D2YW35F7/Peng et al. - 2023 - End-to-End Mispronunciation Detection and Diagnosis Using Transfer Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.mdpi.com/2076-3417/13/11/6793
      \endverb
      \verb{url}
      \verb https://www.mdpi.com/2076-3417/13/11/6793
      \endverb
    \endentry
    \entry{pengTextAwareEndtoendMispronunciation2022}{online}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=f737eec8f3f2fb0e9d3b00c5f5aab5ba}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Linkai},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a535a6e206309a4dfa99e76d4fa57128}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Yingming},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86728d3a00a7b01605eba98ae7c69da3}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Binghuai},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=240852ee544f176c02ca5edaf106318f}{%
           family={Ke},
           familyi={K\bibinitperiod},
           given={Dengfeng},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9d5474a5562f892dc1a1f5e572721e21}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Yanlu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=74f3067f20603e85bac325f904c6b552}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Jinsong},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{342e9ade5815c9036dbd5d60ff7c8d22}
      \strng{fullhash}{a6f969cc710cef57b53d623f0030f0ac}
      \strng{bibnamehash}{a6f969cc710cef57b53d623f0030f0ac}
      \strng{authorbibnamehash}{a6f969cc710cef57b53d623f0030f0ac}
      \strng{authornamehash}{342e9ade5815c9036dbd5d60ff7c8d22}
      \strng{authorfullhash}{a6f969cc710cef57b53d623f0030f0ac}
      \field{extraname}{3}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Mispronunciation detection and diagnosis (MDD) technology is a key component of computer-assisted pronunciation training system (CAPT). In the field of assessing the pronunciation quality of constrained speech, the given transcriptions can play the role of a teacher. Conventional methods have fully utilized the prior texts for the model construction or improving the system performance, e.g. forced-alignment and extended recognition networks. Recently, some end-to-end based methods attempt to incorporate the prior texts into model training and preliminarily show the effectiveness. However, previous studies mostly consider applying raw attention mechanism to fuse audio representations with text representations, without taking possible text-pronunciation mismatch into account. In this paper, we present a gating strategy that assigns more importance to the relevant audio features while suppressing irrelevant text information. Moreover, given the transcriptions, we design an extra contrastive loss to reduce the gap between the learning objective of phoneme recognition and MDD. We conducted experiments using two publicly available datasets (TIMIT and L2Arctic) and our best model improved the F1 score from 57.51\% to 61.75\% compared to the baselines. Besides, we provide a detailed analysis to shed light on the effectiveness of gating mechanism and contrastive learning on MDD1.}
      \field{day}{15}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Text-{{Aware End-to-end Mispronunciation Detection}} and {{Diagnosis}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2206.07289
      \endverb
      \verb{eprint}
      \verb 2206.07289
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/BPCSWBTP/Peng et al. - 2022 - Text-Aware End-to-end Mispronunciation Detection and Diagnosis.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2206.07289
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2206.07289
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{poveyKaldiSpeechRecognition}{article}{}
      \name{author}{13}{}{%
        {{un=0,uniquepart=base,hash=e37bd3eba929f71a8a0351899f3870d5}{%
           family={Povey},
           familyi={P\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7317e829771fcdb6bffb1ce8c3b6c02f}{%
           family={Ghoshal},
           familyi={G\bibinitperiod},
           given={Arnab},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4da0ff554d1408c6508f7ae846654700}{%
           family={Boulianne},
           familyi={B\bibinitperiod},
           given={Gilles},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b54fb746be9f1030d1c6dc43c46b9345}{%
           family={Burget},
           familyi={B\bibinitperiod},
           given={Lukaˇs},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d219f381488e5233bb991b9a13f89856}{%
           family={Glembek},
           familyi={G\bibinitperiod},
           given={Ondˇrej},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=84224cceb127111325bf34d5bf6126bf}{%
           family={Goel},
           familyi={G\bibinitperiod},
           given={Nagendra},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=40eaa8a12b21dce5ba6c013910d5cc4a}{%
           family={Hannemann},
           familyi={H\bibinitperiod},
           given={Mirko},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3445ac8d82122c5ab55819eb219eb35a}{%
           family={Motlıˇcek},
           familyi={M\bibinitperiod},
           given={Petr},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8945f0f8c84f5bfcb4ba5f31e3b49586}{%
           family={Qian},
           familyi={Q\bibinitperiod},
           given={Yanmin},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4d5da0ee0f4359168b596b8ad93969d}{%
           family={Schwarz},
           familyi={S\bibinitperiod},
           given={Petr},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc8ceaecfbd95ac33eeb8dd33a7e04a1}{%
           family={Silovsky},
           familyi={S\bibinitperiod},
           given={Jan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c45a4751946223d635ac92a2ae9189ed}{%
           family={Stemmer},
           familyi={S\bibinitperiod},
           given={Georg},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7170093a97980c23772ad16a6a04d0c1}{%
           family={Vesely},
           familyi={V\bibinitperiod},
           given={Karel},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{58ab821c54bf7cddd915674ad5dd9f5a}
      \strng{fullhash}{d6b1adb09af2d899a3a8f5067c0eedb2}
      \strng{bibnamehash}{d6b1adb09af2d899a3a8f5067c0eedb2}
      \strng{authorbibnamehash}{d6b1adb09af2d899a3a8f5067c0eedb2}
      \strng{authornamehash}{58ab821c54bf7cddd915674ad5dd9f5a}
      \strng{authorfullhash}{d6b1adb09af2d899a3a8f5067c0eedb2}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.}
      \field{langid}{english}
      \field{title}{The {{Kaldi Speech Recognition Toolkit}}}
      \verb{file}
      \verb /home/pccady/Zotero/storage/MAC6UCHS/Povey et al. - The Kaldi Speech Recognition Toolkit.pdf
      \endverb
    \endentry
    \entry{prabhavalkarMinimumWordError2017}{online}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=93bb6027d9d5e18776d8d859c9da7e5e}{%
           family={Prabhavalkar},
           familyi={P\bibinitperiod},
           given={Rohit},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=28a3d000e5cd60656250014708db8ec1}{%
           family={Sainath},
           familyi={S\bibinitperiod},
           given={Tara\bibnamedelima N.},
           giveni={T\bibinitperiod\bibinitdelim N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1fc7a40d6072b1bb1d4e56d14ef88e2f}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yonghui},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=842040ccc784c0e00e9b177a7271883a}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c0d10aaf985cebf8d0497e1828f9313f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zhifeng},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a16597f6d78144232241757f5d6e401c}{%
           family={Chiu},
           familyi={C\bibinitperiod},
           given={Chung-Cheng},
           giveni={C\bibinithyphendelim C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e6e0baf52b06f58dfd24c76abe0b0495}{%
           family={Kannan},
           familyi={K\bibinitperiod},
           given={Anjuli},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{af7d5e642586b4bf247dee7652a2749d}
      \strng{fullhash}{2a0d6dadc50a46a7c7a4f56ce2293245}
      \strng{bibnamehash}{2a0d6dadc50a46a7c7a4f56ce2293245}
      \strng{authorbibnamehash}{2a0d6dadc50a46a7c7a4f56ce2293245}
      \strng{authornamehash}{af7d5e642586b4bf247dee7652a2749d}
      \strng{authorfullhash}{2a0d6dadc50a46a7c7a4f56ce2293245}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sequence-to-sequence models, such as attention-based models in automatic speech recognition (ASR), are typically trained to optimize the cross-entropy criterion which corresponds to improving the loglikelihood of the data. However, system performance is usually measured in terms of word error rate (WER), not log-likelihood. Traditional ASR systems benefit from discriminative sequence training which optimizes criteria such as the state-level minimum Bayes risk (sMBR) which are more closely related to WER.}
      \field{day}{5}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{12}
      \field{pubstate}{prepublished}
      \field{title}{Minimum {{Word Error Rate Training}} for {{Attention-based Sequence-to-Sequence Models}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1712.01818
      \endverb
      \verb{eprint}
      \verb 1712.01818
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/56J9L4X6/Prabhavalkar et al. - 2017 - Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1712.01818
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1712.01818
      \endverb
      \keyw{Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
    \endentry
    \entry{punjabiLanguageModelBootstrapping2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=32d105a4eafe9206dbcf0ba9335e12d8}{%
           family={Punjabi},
           familyi={P\bibinitperiod},
           given={Surabhi},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85bf51d525abc7817d3d7bc793f67291}{%
           family={Arsikere},
           familyi={A\bibinitperiod},
           given={Harish},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=25a969074e32d100b2658596cf2ac90d}{%
           family={Garimella},
           familyi={G\bibinitperiod},
           given={Sri},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {SG, Singapore}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{ec2cfb2a2cb4f9a413c4bee064a8a10b}
      \strng{fullhash}{fa53e6ceffbf8b895141a520dfc18f10}
      \strng{bibnamehash}{fa53e6ceffbf8b895141a520dfc18f10}
      \strng{authorbibnamehash}{fa53e6ceffbf8b895141a520dfc18f10}
      \strng{authornamehash}{ec2cfb2a2cb4f9a413c4bee064a8a10b}
      \strng{authorfullhash}{fa53e6ceffbf8b895141a520dfc18f10}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Building conversational speech recognition systems for new languages is constrained by the availability of utterances capturing user-device interactions. Data collection is expensive and limited by speed of manual transcription. In order to address this, we advocate the use of neural machine translation as a data augmentation technique for bootstrapping language models. Machine translation (MT) offers a systematic way of incorporating collections from mature, resource-rich conversational systems that may be available for a different language. However, ingesting raw translations from a general purpose MT system may not be effective owing to the presence of named entities, intra sentential code-switching and the domain mismatch between the conversational data being translated and the parallel text used for MT training. To circumvent this, we explore following domain adaptation techniques: (a) sentence embedding based data selection for MT training, (b) model finetuning, and (c) rescoring and filtering translated hypotheses. Using Hindi language as the experimental testbed, we supplement transcribed collections with translated US English utterances. We observe a relative word error rate reduction of 7.8-15.6\%, depending on the bootstrapping phase. Fine grained analysis reveals that translation particularly aids the interaction scenarios underrepresented in the transcribed data.}
      \field{booktitle}{2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})}
      \field{eventtitle}{2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})}
      \field{isbn}{978-1-7281-0306-8}
      \field{langid}{english}
      \field{month}{12}
      \field{title}{Language {{Model Bootstrapping Using Neural Machine Translation}} for {{Conversational Speech Recognition}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{487\bibrangedash 493}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1109/ASRU46091.2019.9003982
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/CPJQZQLD/Punjabi et al. - 2019 - Language Model Bootstrapping Using Neural Machine Translation for Conversational Speech Recognition.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9003982/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9003982/
      \endverb
    \endentry
    \entry{qianAutomaticSpeechRecognition2022}{inproceedings}{}
      \name{author}{8}{}{%
        {{un=0,uniquepart=base,hash=9c58381cb863012cd1df917bb20d39ee}{%
           family={Qian},
           familyi={Q\bibinitperiod},
           given={Mengjie},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f4ca87a8b7ddfc49d2811ec5f34f86af}{%
           family={Berthelsen},
           familyi={B\bibinitperiod},
           given={Harald},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9db4af13432a594613e8efe0ea532287}{%
           family={Lonergan},
           familyi={L\bibinitperiod},
           given={Liam},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c18def86d87b42fc64a7f14ad616b8d1}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={Andy},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a07538345876c54ba2426d3f56f2cfbc}{%
           family={O'Neill},
           familyi={O\bibinitperiod},
           given={Claire},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4832403ab21dc1506ada3840a69dc45b}{%
           family={Ni\bibnamedelima Chiarain},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Neasa},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f08b34b12657a55637cced651ec89cc3}{%
           family={Gobl},
           familyi={G\bibinitperiod},
           given={Christer},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b8bdb8d71c67fb20d6a082af2a756341}{%
           family={Ni\bibnamedelima Chasaide},
           familyi={N\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Ailbhe},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Cork, Ireland}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{906297d8c6ac2de604b6bf8bba41be88}
      \strng{fullhash}{dedfebca4c1e67f2373a2e30cc2a49b9}
      \strng{bibnamehash}{dedfebca4c1e67f2373a2e30cc2a49b9}
      \strng{authorbibnamehash}{dedfebca4c1e67f2373a2e30cc2a49b9}
      \strng{authornamehash}{906297d8c6ac2de604b6bf8bba41be88}
      \strng{authorfullhash}{dedfebca4c1e67f2373a2e30cc2a49b9}
      \field{sortinit}{Q}
      \field{sortinithash}{ce69a400a872ddd02ee7fdb3b38c6abd}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A range of lexicons and language models were tested in the development of ASR for Irish. One problem, common among minority languages, is the multiplicity of dialects, with no one spoken standard. To address this challenge, in a hybrid ASR system two alternative cross-dialect lexicons are tested, which draw on research in dialect phonology. First, individual lexicons were built for the three main dialects of Ulster (Ul), Connaught (Co) and Munster (Mu). With these, a Multi-dialect lexicon incorporated all dialect-varying word forms. An alternative Global lexicon, essentially a trans-dialect lexicon, used abstract representations of dialect-varying forms (phoneme or morpheme sized units). These two cross-dialect lexicons were tested along with the three dialect-specific lexicons. Several different language models were also tested. Results for the Global and Multi-dialect lexicons were found to yield the highest performance, with the lowest overall WER for the latter. There were considerable differences in results for the individual dialect lexicons: this may reflect a bias in the datasets used or could be indicators of the linguistic distance between the dialects — competing hypotheses that will need more rigorous testing. Results showed a strong effect of the language model used. Error patterns show frequent substitutions involving inflected forms.}
      \field{booktitle}{2022 33rd {{Irish Signals}} and {{Systems Conference}} ({{ISSC}})}
      \field{day}{9}
      \field{eventtitle}{2022 33rd {{Irish Signals}} and {{Systems Conference}} ({{ISSC}})}
      \field{isbn}{978-1-6654-5227-4}
      \field{langid}{english}
      \field{month}{6}
      \field{shorttitle}{Automatic {{Speech Recognition}} for {{Irish}}}
      \field{title}{Automatic {{Speech Recognition}} for {{Irish}}: Testing Lexicons and Language Models}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 6}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1109/ISSC55427.2022.9826201
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/ETGTXC54/Qian et al. - 2022 - Automatic Speech Recognition for Irish testing lexicons and language models.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/9826201/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/9826201/
      \endverb
    \endentry
    \entry{ranathungaNeuralMachineTranslation2021}{online}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=9e443800457acf0658615783189b864c}{%
           family={Ranathunga},
           familyi={R\bibinitperiod},
           given={Surangika},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1d73302bddcfccef41c5feb7798fe180}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={En-Shiun\bibnamedelima Annie},
           giveni={E\bibinithyphendelim S\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5907034a5337158db442887961f849b}{%
           family={Skenduli},
           familyi={S\bibinitperiod},
           given={Marjana\bibnamedelima Prifti},
           giveni={M\bibinitperiod\bibinitdelim P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0a2a3d4aa43f3a2dabd0a64882f9216b}{%
           family={Shekhar},
           familyi={S\bibinitperiod},
           given={Ravi},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d5ea65cd3bf1c7bf17a424c061512ca1}{%
           family={Alam},
           familyi={A\bibinitperiod},
           given={Mehreen},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a4fd7990bba4ac998dc66c8dc521c96d}{%
           family={Kaur},
           familyi={K\bibinitperiod},
           given={Rishemjit},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0f65f7ca78551fb3c96114466b4a0c35}
      \strng{fullhash}{eb200f6f49fec9f5c8d03ae83c008f14}
      \strng{bibnamehash}{eb200f6f49fec9f5c8d03ae83c008f14}
      \strng{authorbibnamehash}{eb200f6f49fec9f5c8d03ae83c008f14}
      \strng{authornamehash}{0f65f7ca78551fb3c96114466b4a0c35}
      \strng{authorfullhash}{eb200f6f49fec9f5c8d03ae83c008f14}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Neural Machine Translation (NMT) has seen a tremendous spurt of growth in less than ten years, and has already entered a mature phase. While considered as the most widely used solution for Machine Translation, its performance on low-resource language pairs still remains sub-optimal compared to the high-resource counterparts, due to the unavailability of large parallel corpora. Therefore, the implementation of NMT techniques for low-resource language pairs has been receiving the spotlight in the recent NMT research arena, thus leading to a substantial amount of research reported on this topic. This paper presents a detailed survey of research advancements in low-resource language NMT (LRL-NMT), along with a quantitative analysis aimed at identifying the most popular solutions. Based on our findings from reviewing previous work, this survey paper provides a set of guidelines to select the possible NMT technique for a given LRL data setting. It also presents a holistic view of the LRL-NMT research landscape and provides a list of recommendations to further enhance the research efforts on LRL-NMT. CCS Concepts: • Computing methodologies → Natural language processing; Neural networks; Machine translation; Language resources; Machine learning.}
      \field{day}{29}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Neural {{Machine Translation}} for {{Low-Resource Languages}}}
      \field{title}{Neural {{Machine Translation}} for {{Low-Resource Languages}}: {{A Survey}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2106.15115
      \endverb
      \verb{eprint}
      \verb 2106.15115
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/SC639869/Ranathunga et al. - 2021 - Neural Machine Translation for Low-Resource Languages A Survey.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2106.15115
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2106.15115
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
    \endentry
    \entry{rogerson-revellComputerAssistedPronunciationTraining2021}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=6116c522bb60bdf6648fee04dee72adc}{%
           family={Rogerson-Revell},
           familyi={R\bibinithyphendelim R\bibinitperiod},
           given={Pamela\bibnamedelima M},
           giveni={P\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{6116c522bb60bdf6648fee04dee72adc}
      \strng{fullhash}{6116c522bb60bdf6648fee04dee72adc}
      \strng{bibnamehash}{6116c522bb60bdf6648fee04dee72adc}
      \strng{authorbibnamehash}{6116c522bb60bdf6648fee04dee72adc}
      \strng{authornamehash}{6116c522bb60bdf6648fee04dee72adc}
      \strng{authorfullhash}{6116c522bb60bdf6648fee04dee72adc}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This viewpoint essay considers the current status of computer-assisted pronunciation training (CAPT) before examining some of the current issues and future directions in the field. The underlying premise is the pedagogic potential of CAPT systems and resources for teaching and learning, and the need for greater synergy between technological design and functionality on the one hand, and pedagogic purpose on the other. Some of the key issues examined include providing accurate and individualised automated feedback for pronunciation, for both learning and assessment, and evaluating the effectiveness of CAPT tools and systems. When considering future directions, the discussion focuses on what aspects of pedagogy are likely to be at the forefront of developments, including ubiquitous learning; intelligent tutoring and authentic interaction; and goal-oriented, task-based learning.}
      \field{issn}{0033-6882, 1745-526X}
      \field{journaltitle}{RELC Journal}
      \field{langid}{english}
      \field{month}{4}
      \field{number}{1}
      \field{shortjournal}{RELC Journal}
      \field{shorttitle}{Computer-{{Assisted Pronunciation Training}} ({{CAPT}})}
      \field{title}{Computer-{{Assisted Pronunciation Training}} ({{CAPT}}): {{Current Issues}} and {{Future Directions}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{52}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{189\bibrangedash 205}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1177/0033688220977406
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/EJLED84K/Rogerson-Revell - 2021 - Computer-Assisted Pronunciation Training (CAPT) Current Issues and Future Directions.pdf
      \endverb
      \verb{urlraw}
      \verb https://journals.sagepub.com/doi/10.1177/0033688220977406
      \endverb
      \verb{url}
      \verb https://journals.sagepub.com/doi/10.1177/0033688220977406
      \endverb
    \endentry
    \entry{rosenblumSpeechPerceptionMultimodal2008}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=6990b24deadc93e1bf527ae1841c8926}{%
           family={Rosenblum},
           familyi={R\bibinitperiod},
           given={Lawrence\bibnamedelima D.},
           giveni={L\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{6990b24deadc93e1bf527ae1841c8926}
      \strng{fullhash}{6990b24deadc93e1bf527ae1841c8926}
      \strng{bibnamehash}{6990b24deadc93e1bf527ae1841c8926}
      \strng{authorbibnamehash}{6990b24deadc93e1bf527ae1841c8926}
      \strng{authornamehash}{6990b24deadc93e1bf527ae1841c8926}
      \strng{authorfullhash}{6990b24deadc93e1bf527ae1841c8926}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Speech perception is inherently multimodal. Visual speech (lip-reading) information is used by all perceivers and readily integrates with auditory speech. Imaging research suggests that the brain treats auditory and visual speech similarly. These findings have led some researchers to consider that speech perception works by extracting amodal information that takes the same form across modalities. From this perspective, speech integration is a property of the input information itself. Amodal speech information could explain the reported automaticity, immediacy, and completeness of audiovisual speech integration. However, recent findings suggest that speech integration can be influenced by higher cognitive properties such as lexical status and semantic context. Proponents of amodal accounts will need to explain these results.}
      \field{issn}{0963-7214, 1467-8721}
      \field{journaltitle}{Current Directions in Psychological Science}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{6}
      \field{shortjournal}{Curr Dir Psychol Sci}
      \field{title}{Speech {{Perception}} as a {{Multimodal Phenomenon}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{17}
      \field{year}{2008}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{405\bibrangedash 409}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1111/j.1467-8721.2008.00615.x
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/HA5WE24Z/Rosenblum - 2008 - Speech Perception as a Multimodal Phenomenon.pdf
      \endverb
      \verb{urlraw}
      \verb https://journals.sagepub.com/doi/10.1111/j.1467-8721.2008.00615.x
      \endverb
      \verb{url}
      \verb https://journals.sagepub.com/doi/10.1111/j.1467-8721.2008.00615.x
      \endverb
    \endentry
    \entry{rouditchenkoComparisonMultilingualSelfSupervised2023}{inproceedings}{}
      \name{author}{9}{}{%
        {{un=0,uniquepart=base,hash=4061fbecb11a224923e0caad26faf56d}{%
           family={Rouditchenko},
           familyi={R\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ed1ec7bfbb89b56b1e64e1b48efdaa6}{%
           family={Khurana},
           familyi={K\bibinitperiod},
           given={Sameer},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0a327007a0b097a2d8bfa1b4eb2086d1}{%
           family={Thomas},
           familyi={T\bibinitperiod},
           given={Samuel},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b28550979e1f46d5c67bcd9212bab845}{%
           family={Feris},
           familyi={F\bibinitperiod},
           given={Rogerio},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f946c74905bbd1310ba1609f93e47413}{%
           family={Karlinsky},
           familyi={K\bibinitperiod},
           given={Leonid},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=abbb2b3478036776ebfd1dec522c0091}{%
           family={Kuehne},
           familyi={K\bibinitperiod},
           given={Hilde},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0bd189af57105df4f4e2d749c815bf13}{%
           family={Harwath},
           familyi={H\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8f101126b3acf3a5a0f51d86f0fe89b8}{%
           family={Kingsbury},
           familyi={K\bibinitperiod},
           given={Brian},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=079738ec97fefceec5036d7c3657c667}{%
           family={Glass},
           familyi={G\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{b9405e31b90fba6482be163bac61d85d}
      \strng{fullhash}{77888588c7a00e13891af165ac8c3e88}
      \strng{bibnamehash}{77888588c7a00e13891af165ac8c3e88}
      \strng{authorbibnamehash}{77888588c7a00e13891af165ac8c3e88}
      \strng{authornamehash}{b9405e31b90fba6482be163bac61d85d}
      \strng{authorfullhash}{77888588c7a00e13891af165ac8c3e88}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent models such as XLS-R and Whisper have made multilingual speech technologies more accessible by pre-training on audio from around 100 spoken languages each. However, there are thousands of spoken languages worldwide, and adapting to new languages is an important problem. In this work, we aim to understand which model adapts better to languages unseen during pre-training. We fine-tune both models on 13 unseen languages and 18 seen languages. Our results show that the number of hours seen per language and language family during pre-training is predictive of how the models compare, despite the significant differences in the pre-training methods.}
      \field{booktitle}{{{INTERSPEECH}} 2023}
      \field{day}{20}
      \field{eventtitle}{{{INTERSPEECH}} 2023}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{Comparison of {{Multilingual Self-Supervised}} and {{Weakly-Supervised Speech Pre-Training}} for {{Adaptation}} to {{Unseen Languages}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2268\bibrangedash 2272}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2023-1061
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/SYX7C589/Rouditchenko et al. - 2023 - Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2023/rouditchenko23_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2023/rouditchenko23_interspeech.html
      \endverb
    \endentry
    \entry{shahinPhonologicalLevelMispronunciationDetection2024}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=0c7156b7d12b3f54bf73144e3f8df1cb}{%
           family={Shahin},
           familyi={S\bibinitperiod},
           given={Mostafa},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=01669a796f1778a99974ace6951d8c8f}{%
           family={Ahmed},
           familyi={A\bibinitperiod},
           given={Beena},
           giveni={B\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{056bcd09ae12be14cc10fedd6d106d04}
      \strng{fullhash}{056bcd09ae12be14cc10fedd6d106d04}
      \strng{bibnamehash}{056bcd09ae12be14cc10fedd6d106d04}
      \strng{authorbibnamehash}{056bcd09ae12be14cc10fedd6d106d04}
      \strng{authornamehash}{056bcd09ae12be14cc10fedd6d106d04}
      \strng{authorfullhash}{056bcd09ae12be14cc10fedd6d106d04}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The automatic identification and analysis of pronunciation errors, known as mispronunciation detection and diagnosis (MDD), is vital in computer-aided pronunciation learning (CAPL) tools for second-language (L2) learning. Existing MDD methods focus on analyzing phonemes, but they can only detect categorical errors for phonemes with sufficient training data. Due to the unpredictable nature of non-native speakers’ pronunciation errors and limited training datasets, modelling all mispronunciations becomes impractical. Additionally, phoneme-level MDD approaches provide limited diagnostic information. In our proposed approach, we detect phonological features, breaking down phoneme production into elementary components related to the articulatory system, offering more informative feedback to learners. Applied to L2 English speech data, it outperformed traditional phoneme-level methods, reducing false acceptance rate (FAR), false rejection rate (FRR), and diagnostic error rate (DER).}
      \field{booktitle}{Interspeech 2024}
      \field{day}{1}
      \field{eventtitle}{Interspeech 2024}
      \field{langid}{english}
      \field{month}{9}
      \field{title}{Phonological-{{Level Mispronunciation Detection}} and {{Diagnosis}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{307\bibrangedash 311}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2024-2217
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/F4SEIXYG/Shahin and Ahmed - 2024 - Phonological-Level Mispronunciation Detection and Diagnosis.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2024/shahin24_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2024/shahin24_interspeech.html
      \endverb
    \endentry
    \entry{shenNaturalTTSSynthesis2018}{online}{}
      \name{author}{13}{}{%
        {{un=0,uniquepart=base,hash=7f1a9ef81d98cf013191d02e3e5c98ce}{%
           family={Shen},
           familyi={S\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=79b11327e6066baac1563390e41c13f2}{%
           family={Pang},
           familyi={P\bibinitperiod},
           given={Ruoming},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=79806915dad52902a35bf2249c791110}{%
           family={Weiss},
           familyi={W\bibinitperiod},
           given={Ron\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=29ea22df63f174ac629e9ef100b40484}{%
           family={Schuster},
           familyi={S\bibinitperiod},
           given={Mike},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5a9f30e0c0441d2aea255916ff375e8c}{%
           family={Jaitly},
           familyi={J\bibinitperiod},
           given={Navdeep},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9cebeff55132aa441f26afb504ef6a29}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Zongheng},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c0d10aaf985cebf8d0497e1828f9313f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Zhifeng},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9a4f4a1ff661cd600eb26523a5ba8bb4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fc399f85ccdd18f7c16d2cd99a42d132}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yuxuan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=12dfe1f59c012973b161655e19ed9754}{%
           family={Skerry-Ryan},
           familyi={S\bibinithyphendelim R\bibinitperiod},
           given={R.\bibnamedelimi J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7195887e1fff23cbe4d4606880c8f0da}{%
           family={Saurous},
           familyi={S\bibinitperiod},
           given={Rif\bibnamedelima A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ba97da1b498d9addeebed6465a90f567}{%
           family={Agiomyrgiannakis},
           familyi={A\bibinitperiod},
           given={Yannis},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1fc7a40d6072b1bb1d4e56d14ef88e2f}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Yonghui},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{8d4617b05e2f79838cb66a3e3cb5d42a}
      \strng{fullhash}{6ef17f2a9192d257352423e822cee667}
      \strng{bibnamehash}{6ef17f2a9192d257352423e822cee667}
      \strng{authorbibnamehash}{6ef17f2a9192d257352423e822cee667}
      \strng{authornamehash}{8d4617b05e2f79838cb66a3e3cb5d42a}
      \strng{authorfullhash}{6ef17f2a9192d257352423e822cee667}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.}
      \field{day}{16}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{2}
      \field{pubstate}{prepublished}
      \field{title}{Natural {{TTS Synthesis}} by {{Conditioning WaveNet}} on {{Mel Spectrogram Predictions}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1712.05884
      \endverb
      \verb{eprint}
      \verb 1712.05884
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/LFGHGH2P/Shen et al. - 2018 - Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1712.05884
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1712.05884
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{sjons2022articulation}{thesis}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=9de2927425255c2e8eb8cfe32cdb5204}{%
           family={Sjons},
           familyi={S\bibinitperiod},
           given={Johan},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{institution}{1}{%
        {Stockholm University}%
      }
      \strng{namehash}{9de2927425255c2e8eb8cfe32cdb5204}
      \strng{fullhash}{9de2927425255c2e8eb8cfe32cdb5204}
      \strng{bibnamehash}{9de2927425255c2e8eb8cfe32cdb5204}
      \strng{authorbibnamehash}{9de2927425255c2e8eb8cfe32cdb5204}
      \strng{authornamehash}{9de2927425255c2e8eb8cfe32cdb5204}
      \strng{authorfullhash}{9de2927425255c2e8eb8cfe32cdb5204}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Articulation Rate and Surprisal in Swedish Child-Directed Speech}
      \field{type}{phdthesis}
      \field{year}{2022}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/ANDVW38Q/Sjons - 2022 - Articulation rate and surprisal in swedish child-directed speech.pdf
      \endverb
    \endentry
    \entry{snesarevaPalatalizationDublinIrish2016}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=eed11c3c57e1ab35beb1d22b87f1cfc1}{%
           family={Snesareva},
           familyi={S\bibinitperiod},
           given={Marina},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{eed11c3c57e1ab35beb1d22b87f1cfc1}
      \strng{fullhash}{eed11c3c57e1ab35beb1d22b87f1cfc1}
      \strng{bibnamehash}{eed11c3c57e1ab35beb1d22b87f1cfc1}
      \strng{authorbibnamehash}{eed11c3c57e1ab35beb1d22b87f1cfc1}
      \strng{authornamehash}{eed11c3c57e1ab35beb1d22b87f1cfc1}
      \strng{authorfullhash}{eed11c3c57e1ab35beb1d22b87f1cfc1}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper focuses on palatalization in Irish spoken by Dublin-based bilinguals with English as their first language. It has already been pointed out that English phonetics affects Irish speakers even when Irish is their first language, especially in case of palatalization. The extent of English influence on palatalization in Dublin Irish and the possible reasons behind its inconsistent use acquire special prominence not only in terms of phonetics, but also because in Irish palatalization performs phonological functions.}
      \field{issn}{18770428}
      \field{journaltitle}{Procedia - Social and Behavioral Sciences}
      \field{langid}{english}
      \field{month}{12}
      \field{shortjournal}{Procedia - Social and Behavioral Sciences}
      \field{shorttitle}{Palatalization in {{Dublin Irish}}}
      \field{title}{Palatalization in {{Dublin Irish}}: {{The Extent}} of {{Phonetic Interference}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{volume}{236}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{213\bibrangedash 218}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1016/j.sbspro.2016.12.009
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/JZPBE6KT/Snesareva - 2016 - Palatalization in Dublin Irish The Extent of Phonetic Interference.pdf
      \endverb
      \verb{urlraw}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S1877042816316421
      \endverb
      \verb{url}
      \verb https://linkinghub.elsevier.com/retrieve/pii/S1877042816316421
      \endverb
    \endentry
    \entry{somekiESPnetEZPythononlyESPnet2024}{online}{}
      \name{author}{10}{}{%
        {{un=0,uniquepart=base,hash=627704a16af997fbbfd3488039e9f608}{%
           family={Someki},
           familyi={S\bibinitperiod},
           given={Masao},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c8056bcb2a161a85ebcf71c2076a2b3c}{%
           family={Choi},
           familyi={C\bibinitperiod},
           given={Kwanghee},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5418bb2be326f86dab7d1ea29e1a1a5e}{%
           family={Arora},
           familyi={A\bibinitperiod},
           given={Siddhant},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e5c55c48b710b997a1db5e7ee0418e99}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={William},
           giveni={W\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=58029d2373c2d82430340ae8dd0f6b69}{%
           family={Cornell},
           familyi={C\bibinitperiod},
           given={Samuele},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a453ed2155e33eb51567d79d16c2f460}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Jionghao},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=58f0be4b00c7f88fce41e14a40c7e7db}{%
           family={Peng},
           familyi={P\bibinitperiod},
           given={Yifan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=93ca06e4e7e7c2aa380766cf659c7901}{%
           family={Shi},
           familyi={S\bibinitperiod},
           given={Jiatong},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c7c94c13e5a7a71f0521a0978399dfc6}{%
           family={Srivastav},
           familyi={S\bibinitperiod},
           given={Vaibhav},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ae898cb785c2eecb4cda42d862e0d5d4}{%
           family={Watanabe},
           familyi={W\bibinitperiod},
           given={Shinji},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f9a0660747f1e4f58d154894626c8c36}
      \strng{fullhash}{fb9594960032ed6c50326282f4460455}
      \strng{bibnamehash}{fb9594960032ed6c50326282f4460455}
      \strng{authorbibnamehash}{fb9594960032ed6c50326282f4460455}
      \strng{authornamehash}{f9a0660747f1e4f58d154894626c8c36}
      \strng{authorfullhash}{fb9594960032ed6c50326282f4460455}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce ESPnet-EZ, an extension of the open-source speech processing toolkit ESPnet, aimed at quick and easy development of speech models. ESPnet-EZ focuses on two major aspects: (i) easy fine-tuning and inference of existing ESPnet models on various tasks and (ii) easy integration with popular deep neural network frameworks such as PyTorch-Lightning, Hugging Face transformers and datasets, and Lhotse. By replacing ESPnet design choices inherited from Kaldi with a Python-only, Bash-free interface, we dramatically reduce the effort required to build, debug, and use a new model. For example, to fine-tune a speech foundation model, ESPnet-EZ, compared to ESPnet, reduces the number of newly written code by 2.7x and the amount of dependent code by 6.7x while dramatically reducing the Bash script dependencies. The codebase of ESPnet-EZ is publicly available.}
      \field{day}{14}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{9}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{ESPnet-EZ}}}
      \field{title}{{{ESPnet-EZ}}: {{Python-only ESPnet}} for {{Easy Fine-tuning}} and {{Integration}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2024}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2409.09506
      \endverb
      \verb{eprint}
      \verb 2409.09506
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/UAN4YHZP/Someki et al. - 2024 - ESPnet-EZ Python-only ESPnet for Easy Fine-tuning and Integration.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2409.09506
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2409.09506
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{spolskyHandbookEducationalLinguistics2008}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=d70d72a072756f61ef838040ef14abb2}{%
           family={Spolsky},
           familyi={S\bibinitperiod},
           given={Bernard},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=27b73653771329d6d12cf36a5658de54}{%
           family={Hult},
           familyi={H\bibinitperiod},
           given={Francis\bibnamedelima M},
           giveni={F\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{362163d1620fbbce8e905b68d2900a91}
      \strng{fullhash}{362163d1620fbbce8e905b68d2900a91}
      \strng{bibnamehash}{362163d1620fbbce8e905b68d2900a91}
      \strng{authorbibnamehash}{362163d1620fbbce8e905b68d2900a91}
      \strng{authornamehash}{362163d1620fbbce8e905b68d2900a91}
      \strng{authorfullhash}{362163d1620fbbce8e905b68d2900a91}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Wiley Online Library}
      \field{langid}{english}
      \field{title}{The {{Handbook}} of {{Educational Linguistics}}}
      \field{year}{2008}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/U6S5NSSW/Spolsky and Hult - The Handbook of Educational Linguistics.pdf
      \endverb
    \endentry
    \entry{stanleyImprovingL1specificPhonological2012}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=b50b5bded680360b853d1d5b6d0116cb}{%
           family={Stanley},
           familyi={S\bibinitperiod},
           given={Theban},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=fe07bbdd675b552857076fd97a6157a4}{%
           family={Hacioglu},
           familyi={H\bibinitperiod},
           given={Kadri},
           giveni={K\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{b462c81534c678214ec1d1ee1c0a71ef}
      \strng{fullhash}{b462c81534c678214ec1d1ee1c0a71ef}
      \strng{bibnamehash}{b462c81534c678214ec1d1ee1c0a71ef}
      \strng{authorbibnamehash}{b462c81534c678214ec1d1ee1c0a71ef}
      \strng{authornamehash}{b462c81534c678214ec1d1ee1c0a71ef}
      \strng{authorfullhash}{b462c81534c678214ec1d1ee1c0a71ef}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{With the increasing use of technology in classrooms, computer assisted pronunciation training (CAPT) is becoming a vital tool in language learning. In this paper, we present a system that takes advantage of data from learners of a specific L1 to better model phonological errors at various levels in the system. At the lexical level, a statistical machine translation approach is used to model common phonological errors produced by a specific L1 population. At the acoustic level, L1-dependent maximum likelihood (ML) nonnative models and discriminative training are explored. In our experiments, use of a Korean language dependent nonnative lexicon gives us diagnostic abilities that did not exist in our baseline configuration. Replacing the native ML acoustic model with the L1-dependent nonnative model produces relative improvements of 27–37\% in precision for phone detection/identification tasks. We also propose a constrained variant of minimum phone error (MPE) training which is better adapted to phone detection/diagnosis. This technique produces 5–6\% relative improvement in precision in comparison to ML nonnative acoustic models.}
      \field{booktitle}{Interspeech 2012}
      \field{day}{9}
      \field{eventtitle}{Interspeech 2012}
      \field{langid}{english}
      \field{month}{9}
      \field{title}{Improving {{L1-specific}} Phonological Error Diagnosis in Computer Assisted Pronunciation Training}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{827\bibrangedash 830}
      \range{pages}{4}
      \verb{doi}
      \verb 10.21437/Interspeech.2012-251
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/2S32QJ7Y/Stanley and Hacioglu - 2012 - Improving L1-specific phonological error diagnosis in computer assisted pronunciation training.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2012/stanley12_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2012/stanley12_interspeech.html
      \endverb
    \endentry
    \entry{stensonModernIrishComprehensive2020}{book}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=aeab60ee2a2aea121a5a5c961283fa1e}{%
           family={Stenson},
           familyi={S\bibinitperiod},
           given={Nancy},
           giveni={N\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {London ; New York}%
      }
      \list{publisher}{1}{%
        {Routledge, Taylor \& Francis}%
      }
      \strng{namehash}{aeab60ee2a2aea121a5a5c961283fa1e}
      \strng{fullhash}{aeab60ee2a2aea121a5a5c961283fa1e}
      \strng{bibnamehash}{aeab60ee2a2aea121a5a5c961283fa1e}
      \strng{authorbibnamehash}{aeab60ee2a2aea121a5a5c961283fa1e}
      \strng{authornamehash}{aeab60ee2a2aea121a5a5c961283fa1e}
      \strng{authorfullhash}{aeab60ee2a2aea121a5a5c961283fa1e}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{"Modern Irish: A Comprehensive Grammar is a complete reference guide to modern Irish grammar, providing a thorough overview of the language. Key features include: - highly systematic coverage of all levels of structure: sound system, word formation, sentence construction and connection of sentences - authentic examples and English translations which provide an accessible insight into the mechanics of the language - an extensive index, numbered sections, cross-references and summary charts which provide readers with easy access to the information. Modern Irish: A Comprehensive Grammar is an essential reference source for the learner and user of Irish. It is ideal for use in schools, colleges, universities, and adult classes of all types"--}
      \field{isbn}{978-1-138-23652-3 978-1-138-23651-6}
      \field{pagetotal}{304}
      \field{series}{Routledge Comprehensive Grammars}
      \field{shorttitle}{Modern {{Irish}}}
      \field{title}{Modern {{Irish}}: A Comprehensive Grammar}
      \field{year}{2020}
      \field{dateera}{ce}
      \keyw{Grammar,Irish language}
    \endentry
    \entry{thaiSyntheticDataAugmentation2019}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=b109323013ede613e47075cfb2e8b85b}{%
           family={Thai},
           familyi={T\bibinitperiod},
           given={Bao},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6082999c11c14a1c478bf93c19c67e62}{%
           family={Jimerson},
           familyi={J\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5566e4205049b50ecd589d493c8c3e49}{%
           family={Arcoraci},
           familyi={A\bibinitperiod},
           given={Dominic},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=241cd10f84de6866ffb234181249a0f1}{%
           family={Prud'hommeaux},
           familyi={P\bibinitperiod},
           given={Emily},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ad2d235a5329e443c91d7996dc449d69}{%
           family={Ptucha},
           familyi={P\bibinitperiod},
           given={Raymond},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Rochester, NY, USA}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{95b3f55b422f98170e0172f03158c919}
      \strng{fullhash}{3fae0b1256bc879ddfd8e94c041200f8}
      \strng{bibnamehash}{3fae0b1256bc879ddfd8e94c041200f8}
      \strng{authorbibnamehash}{3fae0b1256bc879ddfd8e94c041200f8}
      \strng{authornamehash}{95b3f55b422f98170e0172f03158c919}
      \strng{authorfullhash}{3fae0b1256bc879ddfd8e94c041200f8}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Although the application of deep learning to automatic speech recognition (ASR) has resulted in dramatic reductions in word error rate for languages with abundant training data, ASR for languages with few resources has yet to benefit from deep learning to the same extent. In this paper, we investigate various methods of acoustic modeling and data augmentation with the goal of improving the accuracy of a deep learning ASR framework for a low-resource language with a high baseline word error rate. We compare several methods of generating synthetic acoustic training data via voice transformation and signal distortion, and we explore several strategies for integrating this data into the acoustic training pipeline. We evaluate our methods on an indigenous language of North America with minimal training resources. We show that training initially via transfer learning from an existing high-resource language acoustic model, refining weights using a heavily concentrated synthetic dataset, and finally fine-tuning to the target language using limited synthetic data reduces WER by 15\% over just transfer learning using deep recurrent methods. Further, we show improvements over traditional frameworks by 19\% using a similar multistage training with deep convolutional approaches.}
      \field{booktitle}{2019 {{IEEE Western New York Image}} and {{Signal Processing Workshop}} ({{WNYISPW}})}
      \field{eventtitle}{2019 {{IEEE Western New York Image}} and {{Signal Processing Workshop}} ({{WNYISPW}})}
      \field{isbn}{978-1-7281-4352-1}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Synthetic {{Data Augmentation}} for {{Improving Low-Resource ASR}}}
      \field{urlday}{28}
      \field{urlmonth}{8}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 9}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/WNYIPW.2019.8923082
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/SC7Q7HGB/Thai et al. - 2019 - Synthetic Data Augmentation for Improving Low-Resource ASR.pdf
      \endverb
      \verb{urlraw}
      \verb https://ieeexplore.ieee.org/document/8923082/
      \endverb
      \verb{url}
      \verb https://ieeexplore.ieee.org/document/8923082/
      \endverb
    \endentry
    \entry{watanabeESPnetEndtoEndSpeech2018}{online}{}
      \name{author}{12}{}{%
        {{un=0,uniquepart=base,hash=ae898cb785c2eecb4cda42d862e0d5d4}{%
           family={Watanabe},
           familyi={W\bibinitperiod},
           given={Shinji},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=3d7f41dab838c920fb9511319762ddf6}{%
           family={Hori},
           familyi={H\bibinitperiod},
           given={Takaaki},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=879774ecbd2eb8c15e31a9bc2dca53c6}{%
           family={Karita},
           familyi={K\bibinitperiod},
           given={Shigeki},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d240bae1e3c88f765895ab4d83782eb4}{%
           family={Hayashi},
           familyi={H\bibinitperiod},
           given={Tomoki},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0e020a059bb3d3376d01613b02181886}{%
           family={Nishitoba},
           familyi={N\bibinitperiod},
           given={Jiro},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=993fa143252b72cf055f9ae6eb58bfb1}{%
           family={Unno},
           familyi={U\bibinitperiod},
           given={Yuya},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5d44d64aa24886aadf24aea066bbe644}{%
           family={Soplin},
           familyi={S\bibinitperiod},
           given={Nelson\bibnamedelimb Enrique\bibnamedelima Yalta},
           giveni={N\bibinitperiod\bibinitdelim E\bibinitperiod\bibinitdelim Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e516c4bc1fa1598297c1e5129475435c}{%
           family={Heymann},
           familyi={H\bibinitperiod},
           given={Jahn},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=83cf1b1bd47dd02fcca8b3c1bd2f932a}{%
           family={Wiesner},
           familyi={W\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=014a2645817376950f81c70f969a9fe2}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Nanxin},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d0d352cc1b6eabd3bcbec96f07279b20}{%
           family={Renduchintala},
           familyi={R\bibinitperiod},
           given={Adithya},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e63d32c01572ef268ba784cea109e876}{%
           family={Ochiai},
           familyi={O\bibinitperiod},
           given={Tsubasa},
           giveni={T\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{32b9ed5a931a1d6702b61be5623b2be9}
      \strng{fullhash}{5fd00af44ed0c34c8eca81f0088c7138}
      \strng{bibnamehash}{5fd00af44ed0c34c8eca81f0088c7138}
      \strng{authorbibnamehash}{5fd00af44ed0c34c8eca81f0088c7138}
      \strng{authornamehash}{32b9ed5a931a1d6702b61be5623b2be9}
      \strng{authorfullhash}{5fd00af44ed0c34c8eca81f0088c7138}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.}
      \field{day}{30}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{pubstate}{prepublished}
      \field{shorttitle}{{{ESPnet}}}
      \field{title}{{{ESPnet}}: {{End-to-End Speech Processing Toolkit}}}
      \field{urlday}{3}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1804.00015
      \endverb
      \verb{eprint}
      \verb 1804.00015
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/RHIAGLHI/Watanabe et al. - 2018 - ESPnet End-to-End Speech Processing Toolkit.pdf;/home/pccady/Zotero/storage/7YHGCAJ2/1804.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1804.00015
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1804.00015
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{weiMitigatingNeuralNetwork2022}{online}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=f84c52f1e3ed1b39d62e9f34cd8514e7}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Hongxin},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=785343510b882973c9054a0580f9787a}{%
           family={Xie},
           familyi={X\bibinitperiod},
           given={Renchunzi},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f72448b448f784a30f7d333a8eee90be}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=982000f6b712816d06ad05fb231cb6db}{%
           family={Feng},
           familyi={F\bibinitperiod},
           given={Lei},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=96a30a4deee57f534cac5fa8cea3ae4f}{%
           family={An},
           familyi={A\bibinitperiod},
           given={Bo},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=06f816641758a5f532df3e6f297cfc97}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yixuan},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{b85b71eca7bf23095a3d52a82a6e027b}
      \strng{fullhash}{8b0678dd2fb33425141c9cce4116e469}
      \strng{bibnamehash}{8b0678dd2fb33425141c9cce4116e469}
      \strng{authorbibnamehash}{8b0678dd2fb33425141c9cce4116e469}
      \strng{authornamehash}{b85b71eca7bf23095a3d52a82a6e027b}
      \strng{authorfullhash}{8b0678dd2fb33425141c9cce4116e469}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)—a simple fix to the cross-entropy loss—by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output’s norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30\% on common benchmarks.}
      \field{day}{24}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Mitigating {{Neural Network Overconfidence}} with {{Logit Normalization}}}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2205.09310
      \endverb
      \verb{eprint}
      \verb 2205.09310
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/DJ8YF4ZJ/Wei et al. - 2022 - Mitigating Neural Network Overconfidence with Logit Normalization.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2205.09310
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2205.09310
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{witt2014computer}{incollection}{}
      \name{author}{2}{}{%
        {{un=1,uniquepart=given,hash=9cfeed3b7403bbe850575db8230ebcf1}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Silke},
           giveni={S\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=4e4fb887c4c9f76e33e3eb48bbe7a7d6}{%
           family={Young},
           familyi={Y\bibinitperiod},
           given={Steve},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Routledge}%
      }
      \strng{namehash}{93db4fc1823c77a38c3e152fd7ac7adf}
      \strng{fullhash}{93db4fc1823c77a38c3e152fd7ac7adf}
      \strng{bibnamehash}{93db4fc1823c77a38c3e152fd7ac7adf}
      \strng{authorbibnamehash}{93db4fc1823c77a38c3e152fd7ac7adf}
      \strng{authornamehash}{93db4fc1823c77a38c3e152fd7ac7adf}
      \strng{authorfullhash}{93db4fc1823c77a38c3e152fd7ac7adf}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Language Teaching and Language Technology}
      \field{title}{Computer-Assisted Pronunciation Teaching Based on Automatic Speech Recognition}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{pages}{25\bibrangedash 35}
      \range{pages}{11}
      \verb{file}
      \verb /home/pccady/Zotero/storage/T7GISXBL/Witt and Young - 2014 - Computer-assisted pronunciation teaching based on automatic speech recognition.pdf
      \endverb
    \endentry
    \entry{wittAutomaticErrorDetection}{article}{}
      \name{author}{1}{}{%
        {{un=1,uniquepart=given,hash=8771981b917eebb0673ebba6c1fb8465}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Silke\bibnamedelima M},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=1}}%
      }
      \strng{namehash}{8771981b917eebb0673ebba6c1fb8465}
      \strng{fullhash}{8771981b917eebb0673ebba6c1fb8465}
      \strng{bibnamehash}{8771981b917eebb0673ebba6c1fb8465}
      \strng{authorbibnamehash}{8771981b917eebb0673ebba6c1fb8465}
      \strng{authornamehash}{8771981b917eebb0673ebba6c1fb8465}
      \strng{authorfullhash}{8771981b917eebb0673ebba6c1fb8465}
      \field{extraname}{1}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper discusses the state of the art of research in computer assisted pronunciation teaching as of early 2012. A discussion of all major components contributing to pronunciation assessment is presented. This is followed by a summary of existing research to date. Additionally, an overview is given on the use of this research in commercial language learning software. This is followed by a discussion of remaining challenges and possible directions of future research.}
      \field{langid}{english}
      \field{title}{Automatic {{Error Detection}} in {{Pronunciation Training}}: {{Where}} We Are and Where We Need to Go}
      \verb{file}
      \verb /home/pccady/Zotero/storage/LI6LMXJY/Witt - Automatic Error Detection in Pronunciation Training Where we are and where we need to go.pdf
      \endverb
    \endentry
    \entry{witt2000use}{thesis}{}
      \name{author}{1}{}{%
        {{un=1,uniquepart=given,hash=fb013fd0e9521b94f0aa97654431bb58}{%
           family={Witt},
           familyi={W\bibinitperiod},
           given={Silke\bibnamedelima Maren},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=1}}%
      }
      \strng{namehash}{fb013fd0e9521b94f0aa97654431bb58}
      \strng{fullhash}{fb013fd0e9521b94f0aa97654431bb58}
      \strng{bibnamehash}{fb013fd0e9521b94f0aa97654431bb58}
      \strng{authorbibnamehash}{fb013fd0e9521b94f0aa97654431bb58}
      \strng{authornamehash}{fb013fd0e9521b94f0aa97654431bb58}
      \strng{authorfullhash}{fb013fd0e9521b94f0aa97654431bb58}
      \field{extraname}{2}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Use of Speech Recognition in Computer-Assisted Language Learning.}
      \field{type}{phdthesis}
      \field{year}{2000}
      \field{dateera}{ce}
      \verb{file}
      \verb /home/pccady/Zotero/storage/ZR5EB4JA/Witt - 2000 - Use of speech recognition in computer-assisted language learning..pdf
      \endverb
    \endentry
    \entry{wolfTransformersStateoftheArtNatural2020}{inproceedings}{}
      \name{author}{22}{}{%
        {{un=0,uniquepart=base,hash=c34c67badfd5b3624027e9c8c77a69f6}{%
           family={Wolf},
           familyi={W\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4996c576ebaf91c8655cee3341764e3b}{%
           family={Debut},
           familyi={D\bibinitperiod},
           given={Lysandre},
           giveni={L\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b17a6e4a2e8a51a0d77017d50c2ef700}{%
           family={Sanh},
           familyi={S\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=79bf176bccb31f7489fcf1895b225764}{%
           family={Chaumond},
           familyi={C\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e62b33de0b0f2c7b4ac5e9dc6b77b754}{%
           family={Delangue},
           familyi={D\bibinitperiod},
           given={Clement},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a106785413050c01a6a0ac6f0abef459}{%
           family={Moi},
           familyi={M\bibinitperiod},
           given={Anthony},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d37a5a08104030cc7da7f707c8cca77f}{%
           family={Cistac},
           familyi={C\bibinitperiod},
           given={Pierric},
           giveni={P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=bc8b8c6f2d75eb26cbbd360ec7400fd5}{%
           family={Rault},
           familyi={R\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f8c2f1a6f93e56ceeed1aba8d5419cd3}{%
           family={Louf},
           familyi={L\bibinitperiod},
           given={Remi},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=91d34245e8eaf3e4cb599b6cb1c7b6aa}{%
           family={Funtowicz},
           familyi={F\bibinitperiod},
           given={Morgan},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7915e7c2f5406f21e3071c706ba303fe}{%
           family={Davison},
           familyi={D\bibinitperiod},
           given={Joe},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1ded257dc5971d5c24baab3098ad8cbf}{%
           family={Shleifer},
           familyi={S\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,useprefix=true,hash=0897406aed42d0962c35ddc9d5c76431}{%
           family={Platen},
           familyi={P\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod},
           givenun=0,
           prefix={von},
           prefixi={v\bibinitperiod}}}%
        {{un=0,uniquepart=base,hash=1287b8a94fef838bbc4c5082b35115ad}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Clara},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7ac0678b9ae57db7ec09752056af6dd4}{%
           family={Jernite},
           familyi={J\bibinitperiod},
           given={Yacine},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7eb8f72d050b556d9efd79bc5915bb23}{%
           family={Plu},
           familyi={P\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85c8ce7059401b93c2b0e6525a0f7732}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Canwen},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7cfeecd888d1cb971a1711b99011122e}{%
           family={Le\bibnamedelima Scao},
           familyi={L\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={Teven},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e8a58125af15e56d1db521647af99086}{%
           family={Gugger},
           familyi={G\bibinitperiod},
           given={Sylvain},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e52f4e4fdc397218d0c31660a7bef492}{%
           family={Drame},
           familyi={D\bibinitperiod},
           given={Mariama},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6d3a340cadbc002ee63f9d77109d8d59}{%
           family={Lhoest},
           familyi={L\bibinitperiod},
           given={Quentin},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1dd218ae435ec5c0143b3fbbc5da192e}{%
           family={Rush},
           familyi={R\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{2}{}{%
        {{hash=e51b2feac8da559d0f1d95788a6eaea4}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Qun},
           giveni={Q\bibinitperiod}}}%
        {{hash=8145edc752707c44d42fd5a4a90049ff}{%
           family={Schlangen},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Online}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{255f44073360cf5497333775e34f00d6}
      \strng{fullhash}{e7c425bc380c81445a7ded4cbaaabfb3}
      \strng{bibnamehash}{e7c425bc380c81445a7ded4cbaaabfb3}
      \strng{authorbibnamehash}{e7c425bc380c81445a7ded4cbaaabfb3}
      \strng{authornamehash}{255f44073360cf5497333775e34f00d6}
      \strng{authorfullhash}{e7c425bc380c81445a7ded4cbaaabfb3}
      \strng{editorbibnamehash}{402fbdb4bdf6cc1bbd540793f0a161d7}
      \strng{editornamehash}{402fbdb4bdf6cc1bbd540793f0a161d7}
      \strng{editorfullhash}{402fbdb4bdf6cc1bbd540793f0a161d7}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.}
      \field{booktitle}{Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}}
      \field{month}{10}
      \field{shorttitle}{Transformers}
      \field{title}{Transformers: {{State-of-the-Art Natural Language Processing}}}
      \field{urlday}{8}
      \field{urlmonth}{9}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{38\bibrangedash 45}
      \range{pages}{8}
      \verb{doi}
      \verb 10.18653/v1/2020.emnlp-demos.6
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/2Q3Y87JC/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Processing.pdf
      \endverb
      \verb{urlraw}
      \verb https://aclanthology.org/2020.emnlp-demos.6/
      \endverb
      \verb{url}
      \verb https://aclanthology.org/2020.emnlp-demos.6/
      \endverb
    \endentry
    \entry{wuTransformerBasedEndtoEnd2021}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=2c032fffcaf61913e652860509600f1d}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Minglin},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7112bd5b38def4a55168243f6bc5e41b}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Kun},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f60eaedf0859f064e3be6c223474f4ad}{%
           family={Leung},
           familyi={L\bibinitperiod},
           given={Wai-Kim},
           giveni={W\bibinithyphendelim K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8659b27d7c10074faa44f75f234ade20}{%
           family={Meng},
           familyi={M\bibinitperiod},
           given={Helen},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{805773564618653aaf480740d0cae8e3}
      \strng{fullhash}{26d13fbdb997a861a2683f740627b1d2}
      \strng{bibnamehash}{26d13fbdb997a861a2683f740627b1d2}
      \strng{authorbibnamehash}{26d13fbdb997a861a2683f740627b1d2}
      \strng{authornamehash}{805773564618653aaf480740d0cae8e3}
      \strng{authorfullhash}{26d13fbdb997a861a2683f740627b1d2}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper introduces two Transformer-based architectures for Mispronunciation Detection and Diagnosis (MDD). The first Transformer architecture (T-1) is a standard setup with an encoder, a decoder, a projection part and the Cross Entropy (CE) loss. T-1 takes in Mel-Frequency Cepstral Coefficients (MFCC) as input. The second architecture (T-2) is based on wav2vec 2.0, a pretraining framework. T-2 is composed of a CNN feature encoder, several Transformer blocks capturing contextual speech representations, a projection part and the Connectionist Temporal Classification (CTC) loss. Unlike T-1, T-2 takes in raw audio data as input. Both models are trained in an end-to-end manner. Experiments are conducted on the CU-CHLOE corpus, where T-1 achieves a Phone Error Rate (PER) of 8.69\% and F-measure of 77.23\%; and T-2 achieves a PER of 5.97\% and F-measure of 80.98\%. Both models significantly outperform the previously proposed AGPM and CNN-RNN-CTC models, with PERs at 11.1\% and 12.1\% respectively, and F-measures at 72.61\% and 74.65\% respectively.}
      \field{booktitle}{Interspeech 2021}
      \field{day}{30}
      \field{eventtitle}{Interspeech 2021}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{Transformer {{Based End-to-End Mispronunciation Detection}} and {{Diagnosis}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{3954\bibrangedash 3958}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2021-1467
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/NDHLED7D/Wu et al. - 2021 - Transformer Based End-to-End Mispronunciation Detection and Diagnosis.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2021/wu21h_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2021/wu21h_interspeech.html
      \endverb
    \endentry
    \entry{xuIterativePseudoLabelingSpeech2020}{inproceedings}{}
      \name{author}{6}{}{%
        {{un=1,uniquepart=given,hash=f1f086561872e5a943563c890f3c3bef}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Qiantong},
           giveni={Q\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=ff52bf054c22b0389132223a63bdf688}{%
           family={Likhomanenko},
           familyi={L\bibinitperiod},
           given={Tatiana},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1894f466c3378a461fafebdc0915354d}{%
           family={Kahn},
           familyi={K\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b0b92970ca8a68ab09fa412b36dbf6f3}{%
           family={Hannun},
           familyi={H\bibinitperiod},
           given={Awni},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a345e20a460089c920bb74098ed450db}{%
           family={Synnaeve},
           familyi={S\bibinitperiod},
           given={Gabriel},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=133261178beda1dc3991e0e1dfd5a791}{%
           family={Collobert},
           familyi={C\bibinitperiod},
           given={Ronan},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{2c2cd7eb5e6757498dde62da21777076}
      \strng{fullhash}{4ad66bf54f2fabf279bfa5c6f9d5d10d}
      \strng{bibnamehash}{4ad66bf54f2fabf279bfa5c6f9d5d10d}
      \strng{authorbibnamehash}{4ad66bf54f2fabf279bfa5c6f9d5d10d}
      \strng{authornamehash}{2c2cd7eb5e6757498dde62da21777076}
      \strng{authorfullhash}{4ad66bf54f2fabf279bfa5c6f9d5d10d}
      \field{sortinit}{X}
      \field{sortinithash}{1965c258adceecf23ce3d67b05113442}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Interspeech 2020}
      \field{day}{25}
      \field{eventtitle}{Interspeech 2020}
      \field{langid}{english}
      \field{month}{10}
      \field{title}{Iterative {{Pseudo-Labeling}} for {{Speech Recognition}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1006\bibrangedash 1010}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2020-1800
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/9H4BDAHD/Xu et al. - 2020 - Iterative Pseudo-Labeling for Speech Recognition.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2020/xu20b_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2020/xu20b_interspeech.html
      \endverb
    \endentry
    \entry{xuExploreWav2vec202021}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=1,uniquepart=given,hash=735a0efe1b96f39e882754dd113acaed}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Xiaoshuo},
           giveni={X\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=7c94e42161b92c82625e88d3cc45b0c3}{%
           family={Kang},
           familyi={K\bibinitperiod},
           given={Yueteng},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0aae49ee0ebbc1cbff4221cb56d30fe7}{%
           family={Cao},
           familyi={C\bibinitperiod},
           given={Songjun},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86728d3a00a7b01605eba98ae7c69da3}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Binghuai},
           giveni={B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cc6bf34c92d9cf6e693ab9eeadabc22c}{%
           family={Ma},
           familyi={M\bibinitperiod},
           given={Long},
           giveni={L\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{40402bf1ca43187f788bc9f51f45f796}
      \strng{fullhash}{ef98da73391b2e51fe7ee2556f1847a0}
      \strng{bibnamehash}{ef98da73391b2e51fe7ee2556f1847a0}
      \strng{authorbibnamehash}{ef98da73391b2e51fe7ee2556f1847a0}
      \strng{authornamehash}{40402bf1ca43187f788bc9f51f45f796}
      \strng{authorfullhash}{ef98da73391b2e51fe7ee2556f1847a0}
      \field{sortinit}{X}
      \field{sortinithash}{1965c258adceecf23ce3d67b05113442}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents an initial attempt to use self-supervised learning for Mispronunciaiton Detection. Unlike existing methods that use speech recognition corpus to train models, we exploit unlabeled data and utilize a self-supervised learning technique, Wav2vec 2.0, for pretraining. After the pretraining process, the training process only requires a little pronunciationlabeled data for finetuning. Formulating Mispronunciation Detection as a binary classification task, we add convolutional and pooling layers on the top of the pretrained model to detect mispronunciations of the given prompted texts within the alignment segmentations. The training process is simple and effective. Several experiments are conducted to validate the effectiveness of the pretrained method. Our approach outperforms existing methods on a public dataset L2-ARCTIC with a F1 value of 0.610.}
      \field{booktitle}{Interspeech 2021}
      \field{day}{30}
      \field{eventtitle}{Interspeech 2021}
      \field{langid}{english}
      \field{month}{8}
      \field{title}{Explore Wav2vec 2.0 for {{Mispronunciation Detection}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4428\bibrangedash 4432}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2021-777
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/EMRXJ9JC/Xu et al. - 2021 - Explore wav2vec 2.0 for Mispronunciation Detection.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2021/xu21k_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2021/xu21k_interspeech.html
      \endverb
    \endentry
    \entry{yangImprovingMispronunciationDetection2022}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=894d24868762ea184b41b616f5ac6146}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Mu},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=229ccbc4de88b0894d6c8ccc1cc1a07d}{%
           family={Hirschi},
           familyi={H\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6b0ce9d2dfde9b4f5dc95ee77378fb1a}{%
           family={Looney},
           familyi={L\bibinitperiod},
           given={Stephen\bibnamedelima Daniel},
           giveni={S\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1c4ca69976252d241408a60f35ffbe0a}{%
           family={Kang},
           familyi={K\bibinitperiod},
           given={Okim},
           giveni={O\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c8cb2e9d80db3a476154be24112cb732}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={John\bibnamedelima H.L.},
           giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{cbc37a2e398833c404c5305ff7c0e41b}
      \strng{fullhash}{bef550dec247d5e1e8905ecdef4a6acf}
      \strng{bibnamehash}{bef550dec247d5e1e8905ecdef4a6acf}
      \strng{authorbibnamehash}{bef550dec247d5e1e8905ecdef4a6acf}
      \strng{authornamehash}{cbc37a2e398833c404c5305ff7c0e41b}
      \strng{authorfullhash}{bef550dec247d5e1e8905ecdef4a6acf}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Current leading mispronunciation detection and diagnosis (MDD) systems achieve promising performance via end-to-end phoneme recognition. One challenge of such end-to-end solutions is the scarcity of human-annotated phonemes on natural L2 speech. In this work, we leverage unlabeled L2 speech via a pseudo-labeling (PL) procedure and extend the fine-tuning approach based on pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec 2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples plus the created pseudo-labeled L2 speech samples. Our pseudo labels are dynamic and are produced by an ensemble of the online model on-the-fly, which ensures that our model is robust to pseudo label noise. We show that fine-tuning with pseudo labels achieves a 5.35\% phoneme error rate reduction and 2.48\% MDD F1 score improvement over a labeled-samples-only finetuning baseline. The proposed PL method is also shown to outperform conventional offline PL methods. Compared to the state-of-the-art MDD systems, our MDD solution produces a more accurate and consistent phonetic error diagnosis. In addition, we conduct an open test on a separate UTD-4Accents dataset, where our system recognition outputs show a strong correlation with human perception, based on accentedness and intelligibility.}
      \field{booktitle}{Interspeech 2022}
      \field{day}{18}
      \field{eventtitle}{Interspeech 2022}
      \field{langid}{english}
      \field{month}{9}
      \field{title}{Improving {{Mispronunciation Detection}} with {{Wav2vec2-based Momentum Pseudo-Labeling}} for {{Accentedness}} and {{Intelligibility Assessment}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4481\bibrangedash 4485}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2022-11039
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/YKI5Y5R7/Yang et al. - 2022 - Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness a.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2022/yang22v_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2022/yang22v_interspeech.html
      \endverb
    \endentry
    \entry{yuAutomaticSpeechRecognition2015}{book}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=93509a9a7ccede33feb1e702b5a8bb9f}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Dong},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=1,uniquepart=given,hash=2f5fbdc5c3cf91f62a64663cd72397b3}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod},
           givenun=1}}%
      }
      \list{location}{1}{%
        {London}%
      }
      \list{publisher}{1}{%
        {Springer London}%
      }
      \strng{namehash}{6cabcdf68cf375be9e75897c9f5ab74d}
      \strng{fullhash}{6cabcdf68cf375be9e75897c9f5ab74d}
      \strng{bibnamehash}{6cabcdf68cf375be9e75897c9f5ab74d}
      \strng{authorbibnamehash}{6cabcdf68cf375be9e75897c9f5ab74d}
      \strng{authornamehash}{6cabcdf68cf375be9e75897c9f5ab74d}
      \strng{authorfullhash}{6cabcdf68cf375be9e75897c9f5ab74d}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-1-4471-5778-6 978-1-4471-5779-3}
      \field{langid}{english}
      \field{series}{Signals and {{Communication Technology}}}
      \field{shorttitle}{Automatic {{Speech Recognition}}}
      \field{title}{Automatic {{Speech Recognition}}: {{A Deep Learning Approach}}}
      \field{urlday}{31}
      \field{urlmonth}{8}
      \field{urlyear}{2025}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1007/978-1-4471-5779-3
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/JE6MPKNW/Yu and Deng - 2015 - Automatic Speech Recognition A Deep Learning Approach.pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/10.1007/978-1-4471-5779-3
      \endverb
      \verb{url}
      \verb https://link.springer.com/10.1007/978-1-4471-5779-3
      \endverb
    \endentry
    \entry{zeyerWhyDoesCTC2021}{online}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=39a3c2f529d907699f9fa20711e1dac9}{%
           family={Zeyer},
           familyi={Z\bibinitperiod},
           given={Albert},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9c2eaa2f0f77590a890988afefaf75b}{%
           family={Schlüter},
           familyi={S\bibinitperiod},
           given={Ralf},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=cf68a4c2b64db77cc898cdc9fbdeb0c4}{%
           family={Ney},
           familyi={N\bibinitperiod},
           given={Hermann},
           giveni={H\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{e7e53c35217607a8aa392a5dd0dd78e9}
      \strng{fullhash}{93cb6a2a36b8ab6cdfa077f3eafee16b}
      \strng{bibnamehash}{93cb6a2a36b8ab6cdfa077f3eafee16b}
      \strng{authorbibnamehash}{93cb6a2a36b8ab6cdfa077f3eafee16b}
      \strng{authornamehash}{e7e53c35217607a8aa392a5dd0dd78e9}
      \strng{authorfullhash}{93cb6a2a36b8ab6cdfa077f3eafee16b}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100\% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.}
      \field{day}{3}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{title}{Why Does {{CTC}} Result in Peaky Behavior?}
      \field{urlday}{16}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2105.14849
      \endverb
      \verb{eprint}
      \verb 2105.14849
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/4ZZED8DN/Zeyer et al. - 2021 - Why does CTC result in peaky behavior.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2105.14849
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2105.14849
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Mathematics - Statistics Theory,Statistics - Statistics Theory}
    \endentry
    \entry{zhangL2GENNeuralPhoneme2022}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=1,uniquepart=given,hash=b48f4b3efffa9842e785d9b8414e08a1}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=5549973af41d7b156b09a6f0257c6d38}{%
           family={Ganesan},
           familyi={G\bibinitperiod},
           given={Ashwinkumar},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=03ffbb2f4e7d3e052b4d4d6ac05dd699}{%
           family={Campbell},
           familyi={C\bibinitperiod},
           given={Sarah},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d6f2d5b085340bf4faa87e419694bf1f}{%
           family={Korzekwa},
           familyi={K\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{eed09720b43ab7888e4ac97cae0d3073}
      \strng{fullhash}{2a039378ed415a7473432dfc68b3f7e5}
      \strng{bibnamehash}{2a039378ed415a7473432dfc68b3f7e5}
      \strng{authorbibnamehash}{2a039378ed415a7473432dfc68b3f7e5}
      \strng{authornamehash}{eed09720b43ab7888e4ac97cae0d3073}
      \strng{authorfullhash}{2a039378ed415a7473432dfc68b3f7e5}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this paper, we study the problem of generating mispronounced speech mimicking non-native (L2) speakers learning English as a Second Language (ESL) for the mispronunciation detection and diagnosis (MDD) task. The paper is motivated by the widely observed yet not well addressed data sparsity issue in MDD research where both L2 speech audio and its finegrained phonetic annotations are difficult to obtain, leading to unsatisfactory mispronunciation feedback accuracy. We propose L2-GEN, a new data augmentation framework to generate L2 phoneme sequences that capture realistic mispronunciation patterns by devising an unique machine translation-based sequence paraphrasing model. A novel diversified and preferenceaware decoding algorithm is proposed to generalize L2-GEN to handle both unseen words and new learner population with very limited L2 training data. A contrastive augmentation technique is further designed to optimize MDD performance improvements with the generated synthetic L2 data. We evaluate L2-GEN on public L2-ARCTIC and SpeechOcean762 datasets. The results have shown that L2-GEN leads to up to 3.9\%, and 5.0\% MDD F1-score improvements in in-domain and out-ofdomain scenarios respectively.}
      \field{booktitle}{Interspeech 2022}
      \field{day}{18}
      \field{eventtitle}{Interspeech 2022}
      \field{langid}{english}
      \field{month}{9}
      \field{shorttitle}{L2-{{GEN}}}
      \field{title}{L2-{{GEN}}: {{A Neural Phoneme Paraphrasing Approach}} to {{L2 Speech Synthesis}} for {{Mispronunciation Diagnosis}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4317\bibrangedash 4321}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2022-209
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/WTAWKCMN/Zhang et al. - 2022 - L2-GEN A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2022/zhang22_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2022/zhang22_interspeech.html
      \endverb
    \endentry
    \entry{zhangSpeechocean762OpenSourceNonnative2021}{online}{}
      \name{author}{9}{}{%
        {{un=1,uniquepart=given,hash=00c840ad7096e9810dfa1c382608124b}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Junbo},
           giveni={J\bibinitperiod},
           givenun=1}}%
        {{un=0,uniquepart=base,hash=13d224d3833718175fd79538c6aac650}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Zhiwen},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d99a74619b2dceb2e1ed1db25d90c50e}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yongqing},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=32989ee7d8e7a646576f194c804531af}{%
           family={Yan},
           familyi={Y\bibinitperiod},
           given={Zhiyong},
           giveni={Z\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ef915ab28d60d4cb634389aff136204d}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Qiong},
           giveni={Q\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=440327a7f8ff1aa438db8103b422624e}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Yukai},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=815a2cdb03036dc2884fcf0472f759cf}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Ke},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e37bd3eba929f71a8a0351899f3870d5}{%
           family={Povey},
           familyi={P\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a44af477e42cc4d9bbc346304bdd8525}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yujun},
           giveni={Y\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{a90f922302ff3ab243ac14d96c978f22}
      \strng{fullhash}{5b44fb2d2f7502bd90dbb55238f5b15e}
      \strng{bibnamehash}{5b44fb2d2f7502bd90dbb55238f5b15e}
      \strng{authorbibnamehash}{5b44fb2d2f7502bd90dbb55238f5b15e}
      \strng{authornamehash}{a90f922302ff3ab243ac14d96c978f22}
      \strng{authorfullhash}{5b44fb2d2f7502bd90dbb55238f5b15e}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This paper introduces a new open-source speech corpus named “speechocean762” designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children. Five experts annotated each of the utterances at sentence-level, wordlevel and phoneme-level. A baseline system is released in open source to illustrate the phoneme-level pronunciation assessment workflow on this corpus. This corpus is allowed to be used freely for commercial and non-commercial purposes. It is available for free download from OpenSLR, and the corresponding baseline system is published in the Kaldi speech recognition toolkit.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{langid}{english}
      \field{month}{6}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Speechocean762}
      \field{title}{Speechocean762: {{An Open-Source Non-native English Speech Corpus For Pronunciation Assessment}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2104.01378
      \endverb
      \verb{eprint}
      \verb 2104.01378
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/8YRR74DH/Zhang et al. - 2021 - speechocean762 An Open-Source Non-native English Speech Corpus For Pronunciation Assessment.pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2104.01378
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2104.01378
      \endverb
      \keyw{Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
    \endentry
    \entry{zhaoL2ARCTICNonnativeEnglish2018}{inproceedings}{}
      \name{author}{7}{}{%
        {{un=0,uniquepart=base,hash=7b514c29bb94ad3f6e0f8fa6d91ddc45}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Guanlong},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=62f5ee7c50b20b10748edcc6d6b2a75a}{%
           family={Sonsaat},
           familyi={S\bibinitperiod},
           given={Sinem},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=5f4d6b72078f3537a6490860e4e03440}{%
           family={Silpachai},
           familyi={S\bibinitperiod},
           given={Alif},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=aaee64873c4e7ca415346cc6dc22739a}{%
           family={Lucic},
           familyi={L\bibinitperiod},
           given={Ivana},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8437f7c06aa97452cc3eb069268ccdb7}{%
           family={Chukharev-Hudilainen},
           familyi={C\bibinithyphendelim H\bibinitperiod},
           given={Evgeny},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=224bb0e557cdfc275a192894f9c32603}{%
           family={Levis},
           familyi={L\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0e7c2c39d97457715a7465a75cb42fca}{%
           family={Gutierrez-Osuna},
           familyi={G\bibinithyphendelim O\bibinitperiod},
           given={Ricardo},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {ISCA}%
      }
      \strng{namehash}{6a48e59200872b0f83fee47d6fd53544}
      \strng{fullhash}{f90d2448d17a714a2ed9c6c12ff9c638}
      \strng{bibnamehash}{f90d2448d17a714a2ed9c6c12ff9c638}
      \strng{authorbibnamehash}{f90d2448d17a714a2ed9c6c12ff9c638}
      \strng{authornamehash}{6a48e59200872b0f83fee47d6fd53544}
      \strng{authorfullhash}{f90d2448d17a714a2ed9c6c12ff9c638}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{In this paper, we introduce L2-ARCTIC, a speech corpus of non-native English that is intended for research in voice conversion, accent conversion, and mispronunciation detection. This initial release includes recordings from ten non-native speakers of English whose first languages (L1s) are Hindi, Korean, Mandarin, Spanish, and Arabic, each L1 containing recordings from one male and one female speaker. Each speaker recorded approximately one hour of read speech from the Carnegie Mellon University ARCTIC prompts, from which we generated orthographic and forced-aligned phonetic transcriptions. In addition, we manually annotated 150 utterances per speaker to identify three types of mispronunciation errors: substitutions, deletions, and additions, making it a valuable resource not only for research in voice conversion and accent conversion but also in computer-assisted pronunciation training. The corpus is publicly accessible at https://psi.engr.tamu.edu/l2-arctic-corpus/.}
      \field{booktitle}{Interspeech 2018}
      \field{day}{2}
      \field{eventtitle}{Interspeech 2018}
      \field{langid}{english}
      \field{month}{9}
      \field{shorttitle}{L2-{{ARCTIC}}}
      \field{title}{L2-{{ARCTIC}}: {{A Non-native English Speech Corpus}}}
      \field{urlday}{4}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2783\bibrangedash 2787}
      \range{pages}{5}
      \verb{doi}
      \verb 10.21437/Interspeech.2018-1110
      \endverb
      \verb{file}
      \verb /home/pccady/Zotero/storage/IFZLMRKT/Zhao et al. - 2018 - L2-ARCTIC A Non-native English Speech Corpus.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.isca-archive.org/interspeech_2018/zhao18b_interspeech.html
      \endverb
      \verb{url}
      \verb https://www.isca-archive.org/interspeech_2018/zhao18b_interspeech.html
      \endverb
    \endentry
  \enddatalist
  \missing{Li2011ThePA}
  \missing{deichler2024mm}
  \missing{garofolo1993timit}
  \missing{garofoloDARPATIMITAcousticphonetic1993}
  \missing{graves2006connectionist}
\endrefsection
\endinput

