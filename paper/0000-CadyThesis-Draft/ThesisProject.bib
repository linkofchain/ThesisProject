@inproceedings{agrawalLearningWhenTrust2023,
  title = {Learning {{When}} to {{Trust Which Teacher}} for {{Weakly Supervised ASR}}},
  booktitle = {{{INTERSPEECH}} 2023},
  author = {Agrawal, Aakriti and Rao, Milind and Sahu, Anit Kumar and Chennupati, Gopinath and Stolcke, Andreas},
  date = {2023-08-20},
  pages = {381--385},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2023-2205},
  url = {https://www.isca-archive.org/interspeech_2023/agrawal23_interspeech.html},
  urldate = {2025-06-16},
  abstract = {Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25\% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER.},
  eventtitle = {{{INTERSPEECH}} 2023},
  langid = {english},
  file = {/home/pccady/Zotero/storage/LCGXDWE2/Agrawal et al. - 2023 - Learning When to Trust Which Teacher for Weakly Supervised ASR.pdf}
}

@article{alrashoudiImprovingMispronunciationDetection2025,
  title = {Improving Mispronunciation Detection and Diagnosis for Non- Native Learners of the {{Arabic}} Language},
  author = {Alrashoudi, Norah and Al-Khalifa, Hend and Alotaibi, Yousef},
  date = {2025-01-06},
  journaltitle = {Discover Computing},
  shortjournal = {Discov Computing},
  volume = {28},
  number = {1},
  pages = {1},
  issn = {2948-2992},
  doi = {10.1007/s10791-024-09489-8},
  url = {https://link.springer.com/10.1007/s10791-024-09489-8},
  urldate = {2025-06-16},
  abstract = {Mispronunciation detection and diagnosis (MDD) is a core component of computer-assisted pronunciation training (CAPT), which aims to provide opportunities for second language learners (L2) to learn and practice their speaking skills. Arabic is one of the most widespread languages in the world, with more than 422 million speakers. It is the language of the Holy Quran, which increases the importance of learning Arabic. Most existing Arabic MDD systems focus on learningbased techniques rather than state-of-the-art deep-learning methods. Most existing Arabic MDD systems are primarily relying on traditional learning techniques. However, integrating Transformer-based algorithms into the system is crucial, as it can significantly enhance the accuracy, efficiency, and overall performance of the Arabic MDD systems. This paper introduces an Arabic MDD system using transformer-based techniques for non-native learners of spoken Arabic language to enhance their learning of Arabic and help non-native speakers practice their pronunciation skills. The study focuses on detecting mispronunciation phonemes and analyzing these pronunciation errors, by identifying the type of each error, whether it was insertion, deletion, or substitution error. To train the MDD system, we constructed a speech dataset for native and non-native Arabic speakers. The performance evaluation was obtained based on the phoneme recognition and MDD performance, and we achieved a phoneme error rate of 3.1\%, 80.8\%, and 91.3\% for diagnosis accuracy and detection accuracy, respectively. Additionally, we conducted a human perceptual test to assess human proficiency in detecting pronunciation errors and compare their evaluations with automatic verification. The automatic verification surpassed human verification, achieving a detection accuracy of 97.0\% and a diagnosis accuracy of 80.4\%.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/399UNCQL/Alrashoudi et al. - 2025 - Improving mispronunciation detection and diagnosis for non- native learners of the Arabic language.pdf}
}

@article{amrateComputerassistedPronunciationTraining2025,
  title = {Computer-Assisted Pronunciation Training: {{A}} Systematic Review},
  shorttitle = {Computer-Assisted Pronunciation Training},
  author = {Amrate, Moustafa and Tsai, Pi-hua},
  date = {2025-01},
  journaltitle = {ReCALL},
  shortjournal = {ReCALL},
  volume = {37},
  number = {1},
  pages = {22--42},
  issn = {0958-3440, 1474-0109},
  doi = {10.1017/S0958344024000181},
  url = {https://www.cambridge.org/core/product/identifier/S0958344024000181/type/journal_article},
  urldate = {2025-04-04},
  abstract = {This systematic review maps the trends of computer-assisted pronunciation training (CAPT) research based on the pedagogy of second language (L2) pronunciation instruction and assessment. The review was limited to empirical studies investigating the effects of CAPT on healthy L2 learners’ pronunciation. Thirty peer-reviewed journal articles published between 1999 and 2022 were selected based on specific inclusion and exclusion criteria. Data were collected about the studies’ contexts, participants, experimental designs, CAPT systems, pronunciation training scopes and approaches, pronunciation assessment practices, and learning measures. Using a pedagogically informed codebook, the pronunciation training and assessment practices were classified and evaluated based on established L2 pronunciation teaching guidelines. The findings indicated that most of the studies focused on the pronunciation training of adult English learners with an emphasis on the production of segmental features (i.e. vowels and consonants) rather than suprasegmental features (i.e. stress, intonation, and rhythm). Despite the innovation promised by CAPT technology, pronunciation practice in the studies reviewed was characterized by the predominant use of drilling through listen-and-repeat and read-aloud activities. As for assessment, most CAPT studies relied on human listeners to measure the accurate production of discrete pronunciation features (i.e. segmental and suprasegmental accuracy). Meanwhile, few studies employed global pronunciation learning measures such as intelligibility and comprehensibility. Recommendations for future research are provided based on the discussion of these results.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/NS4AH864/Amrate and Tsai - 2025 - Computer-assisted pronunciation training A systematic review.pdf}
}

@inproceedings{ananthakrishnanUsingEnsembleClassifiers2011,
  title = {Using an Ensemble of Classifiers for Mispronunciation Feedback},
  booktitle = {Speech and {{Language Technology}} in {{Education}} ({{SLaTE}} 2011)},
  author = {Ananthakrishnan, Gopal and Wik, Preben and Engwall, Olov and Abdou, Sherif},
  date = {2011-08-24},
  pages = {49--52},
  publisher = {ISCA},
  doi = {10.21437/SLaTE.2011-13},
  url = {https://www.isca-archive.org/slate_2011/ananthakrishnan11_slate.html},
  urldate = {2025-09-17},
  eventtitle = {Speech and {{Language Technology}} in {{Education}} ({{SLaTE}} 2011)},
  langid = {english},
  file = {/home/pccady/Zotero/storage/2NR39XB8/ananthakrishnan11_slate.pdf}
}

@online{ardilaCommonVoiceMassivelyMultilingual2020,
  title = {Common {{Voice}}: {{A Massively-Multilingual Speech Corpus}}},
  shorttitle = {Common {{Voice}}},
  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
  date = {2020-03-05},
  eprint = {1912.06670},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1912.06670},
  url = {http://arxiv.org/abs/1912.06670},
  urldate = {2025-04-04},
  abstract = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla’s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 ± 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/DVASHKJ2/Ardila et al. - 2020 - Common Voice A Massively-Multilingual Speech Corpus.pdf}
}

@book{arnbjornsdottirIntelligentCALLGranular2022,
  title = {Intelligent {{CALL}}, Granular Systems and Learner Data: Short Papers from {{EUROCALL}} 2022},
  shorttitle = {Intelligent {{CALL}}, Granular Systems and Learner Data},
  editor = {Arnbjörnsdóttir, Birna and Bédi, Branislav and Bradley, Linda and Friðriksdóttir, Kolbrún and Garðarsdóttir, Hólmfríður and Thouësny, Sylvie and Whelpton, Matthew James},
  date = {2022-12-12},
  edition = {1},
  publisher = {Research-publishing.net},
  doi = {10.14705/rpnet.2022.61.9782383720157},
  url = {https://research-publishing.net/book?10.14705/rpnet.2022.61.9782383720157},
  urldate = {2025-04-04},
  abstract = {The 2022 EUROCALL conference was held in Reykjavik on 17-19 August 2022                         as a fully online event hosted by the Vigdís Finnbogadóttir Institute for                         Foreign Languages, the University of Iceland, and the Árni Magnússon                         Institute for Icelandic Studies. The conference theme was Intelligent CALL,                         granular systems and learner data. This theme reflects the newest                         developments in the field of technology for language learning. Subfields                         such as natural language processing and machine learning not only enable                         smoother spoken and written communication between human learners and                         computers, but also offer ways in which language learning can be tailored to                         the needs of individual learners. By adding components of automatic speech                         recognition, text-to-speech systems, automatic feedback mechanisms, and                         tracking systems monitoring learners’ progress and their use of tools,                         applications are becoming better targeted. All of this is used to optimise                         the learning experience of individual learners. This volume includes 66                         short papers by some of the EUROCALL 2022 presenters and it offers a                         combination of research studies and theoretical papers reflecting the                         subthemes of the conference. The articles are ordered alphabetically.},
  isbn = {978-2-38372-015-7},
  langid = {english},
  file = {/home/pccady/Zotero/storage/8PK8IQET/Arnbjörnsdóttir et al. - 2022 - Intelligent CALL, granular systems and learner data short papers from EUROCALL 2022.pdf}
}

@inproceedings{arunkumarInvestigationEnsembleFeatures2022,
  title = {Investigation of {{Ensemble}} Features of {{Self-Supervised Pretrained Models}} for {{Automatic Speech Recognition}}},
  booktitle = {Interspeech 2022},
  author = {Arunkumar, A. and Sukhadia, Vrunda N. and Umesh, S.},
  date = {2022-09-18},
  eprint = {2206.05518},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {5145--5149},
  doi = {10.21437/Interspeech.2022-11376},
  url = {http://arxiv.org/abs/2206.05518},
  urldate = {2025-08-29},
  abstract = {Self-supervised learning (SSL) based models have been shown to generate powerful representations that can be used to improve the performance of downstream speech tasks. Several state-of-the-art SSL models are available, and each of these models optimizes a different loss which gives rise to the possibility of their features being complementary. This paper proposes using an ensemble of such SSL representations and models, which exploits the complementary nature of the features extracted by the various pretrained models. We hypothesize that this results in a richer feature representation and shows results for the ASR downstream task. To this end, we use three SSL models that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the ensemble of features using the embeddings obtained from the pre-trained models for a downstream ASR task. We get improved performance over individual models and pre-trained features using Librispeech(100h) and WSJ dataset for the downstream tasks.},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/5WIILZ6Q/Arunkumar et al. - 2022 - Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recogni.pdf;/home/pccady/Zotero/storage/6H4JHM6G/2206.html}
}

@online{babuXLSRSelfsupervisedCrosslingual2021,
  title = {{{XLS-R}}: {{Self-supervised Cross-lingual Speech Representation Learning}} at {{Scale}}},
  shorttitle = {{{XLS-R}}},
  author = {Babu, Arun and Wang, Changhan and Tjandra, Andros and Lakhotia, Kushal and Xu, Qiantong and Goyal, Naman and Singh, Kritika and family=Platen, given=Patrick, prefix=von, useprefix=false and Saraf, Yatharth and Pino, Juan and Baevski, Alexei and Conneau, Alexis and Auli, Michael},
  date = {2021-12-16},
  eprint = {2111.09296},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.09296},
  url = {http://arxiv.org/abs/2111.09296},
  urldate = {2025-09-07},
  abstract = {This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34\% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/YEX5SNF2/Babu et al. - 2021 - XLS-R Self-supervised Cross-lingual Speech Representation Learning at Scale.pdf;/home/pccady/Zotero/storage/T8QG289D/2111.html}
}

@online{baevskiEffectivenessSelfsupervisedPretraining2020,
  title = {Effectiveness of Self-Supervised Pre-Training for Speech Recognition},
  author = {Baevski, Alexei and Auli, Michael and Mohamed, Abdelrahman},
  date = {2020-05-18},
  eprint = {1911.03912},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.03912},
  url = {http://arxiv.org/abs/1911.03912},
  urldate = {2025-04-04},
  abstract = {We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantization. We find the former to be more accurate since it builds a good vocabulary of the data through vq-wav2vec [1] to enable learning of effective representations in subsequent BERT training. Different to previous work, we directly fine-tune the pre-trained BERT models on transcribed speech using a Connectionist Temporal Classification (CTC) loss instead of feeding the representations into a task-specific model. We also propose a BERT-style model learning directly from the continuous audio data and compare pre-training on raw audio to spectral features. Fine-tuning a BERT model on 10 hour of labeled Librispeech data with a vq-wav2vec vocabulary is almost as good as the best known reported system trained on 100 hours of labeled data on testclean, while achieving a 25\% WER reduction on test-other. When using only 10 minutes of labeled data, WER is 25.2 on test-other and 16.3 on test-clean. This demonstrates that self-supervision can enable speech recognition systems trained on a near-zero amount of transcribed data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/M9V7A8QV/Baevski et al. - 2020 - Effectiveness of self-supervised pre-training for speech recognition.pdf}
}

@online{baevskiVqwav2vecSelfSupervisedLearning2020,
  title = {Vq-Wav2vec: {{Self-Supervised Learning}} of {{Discrete Speech Representations}}},
  shorttitle = {Vq-Wav2vec},
  author = {Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  date = {2020-02-16},
  eprint = {1910.05453},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.05453},
  url = {http://arxiv.org/abs/1910.05453},
  urldate = {2025-04-04},
  abstract = {We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/VXRQVZIT/Baevski et al. - 2020 - vq-wav2vec Self-Supervised Learning of Discrete Speech Representations.pdf}
}

@inproceedings{baevskiWav2vec20Framework2020a,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self-Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020},
  volume = {33},
  pages = {12449--12460},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html},
  urldate = {2025-09-09},
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  file = {/home/pccady/Zotero/storage/IYTHPXY4/Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learning of Speech Representations.pdf}
}

@article{bajorek2017l2,
  title = {L2 Pronunciation in {{CALL}}: {{The}} Unrealized Potential of {{Rosetta}} Stone, {{Duolingo}}, {{Babbel}}, and Mango Languages},
  author = {Bajorek, Joan Palmiter},
  date = {2017},
  journaltitle = {Issues and Trends in Educational Technology},
  volume = {5},
  number = {1},
  pages = {24--51},
  file = {/home/pccady/Zotero/storage/IQR5R4FR/Bajorek - 2017 - L2 pronunciation in CALL The unrealized potential of Rosetta stone, Duolingo, Babbel, and mango lan.pdf}
}

@article{ballierUsingWhisperLLM,
  title = {Using {{Whisper LLM}} for {{Automatic Phonetic Diagnosis}} of {{L2 Speech}}: {{A Case Study}} with {{French Learners}} of {{English}}},
  author = {Ballier, Nicolas and Méli, Adrien and Amand, Maelle and Yunès, Jean-Baptiste},
  abstract = {This paper reports on a pilot study to use Whisper’s large language model (LLM) as a tool for potential representation of segmental (phone) pronunciation errors. We compared the performance of the transcription outputs for the various models developed by the automatic speech recognition (ASR) system Whisper (Radford et al., 2022) ranging from 39 to 1,550 million parameters. We investigated 38 recordings of two paragraphs from Conrad’s Typhoon. The whisper transcriptions were compared to the original text that was read by these second-year French undergraduates. We used WER (Word Error Rate) and Levenshtein distance to assess the various graphic representations of Conrad’s reference text. We show how the differences can be transformed into operationalised feedback for learners. We used expert phonetic knowledge to check the plausibility of the phonetic interpretation with the signal (in particular the recall of H dropping produced by French learners). Our findings suggest that the transcriptions produced by the medium model converge with what a native speaker understands and that the tiny model produces alternate transcriptions that are plausible candidates for learner errors.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/Y7D8ILEC/Ballier et al. - Using Whisper LLM for Automatic Phonetic Diagnosis of L2 Speech A Case Study with French Learners o.pdf}
}

@online{bannoProficiencyAssessmentL22022,
  title = {Proficiency Assessment of {{L2}} Spoken {{English}} Using Wav2vec 2.0},
  author = {Bannò, Stefano and Matassoni, Marco},
  date = {2022-10-24},
  eprint = {2210.13168},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.13168},
  url = {http://arxiv.org/abs/2210.13168},
  urldate = {2025-04-04},
  abstract = {The increasing demand for learning English as a second language has led to a growing interest in methods for automatically assessing spoken language proficiency. Most approaches use hand-crafted features, but their efficacy relies on their particular underlying assumptions and they risk discarding potentially salient information about proficiency. Other approaches rely on transcriptions produced by ASR systems which may not provide a faithful rendition of a learner’s utterance in specific scenarios (e.g., non-native children’s spontaneous speech). Furthermore, transcriptions do not yield any information about relevant aspects such as intonation, rhythm or prosody. In this paper, we investigate the use of wav2vec 2.0 for assessing overall and individual aspects of proficiency on two small datasets, one of which is publicly available. We find that this approach significantly outperforms the BERT-based baseline system trained on ASR and manual transcriptions used for comparison.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/EGUZ3K24/Bannò and Matassoni - 2022 - Proficiency assessment of L2 spoken English using wav2vec 2.0.pdf}
}

@online{barteldsMakingMoreLittle2023,
  title = {Making {{More}} of {{Little Data}}: {{Improving Low-Resource Automatic Speech Recognition Using Data Augmentation}}},
  shorttitle = {Making {{More}} of {{Little Data}}},
  author = {Bartelds, Martijn and San, Nay and McDonnell, Bradley and Jurafsky, Dan and Wieling, Martijn},
  date = {2023-05-19},
  eprint = {2305.10951},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10951},
  url = {http://arxiv.org/abs/2305.10951},
  urldate = {2025-07-15},
  abstract = {The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training data from text-only sources. We find that using a self-training approach consistently yields improved performance (a relative WER reduction up to 20.5\% compared to using an ASR system trained on 24 minutes of manually transcribed speech). The performance gain from TTS augmentation for Gronings was even stronger (up to 25.5\% relative reduction in WER compared to a system based on 24 minutes of manually transcribed speech). In sum, our results show the benefit of using self-training or (if possible) TTS-generated data as an efficient solution to overcome the limitations of data availability for resource-scarce languages in order to improve ASR performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/JSBDMVYY/Bartelds et al. - 2023 - Making More of Little Data Improving Low-Resource Automatic Speech Recognition Using Data Augmentat.pdf;/home/pccady/Zotero/storage/UECVQWGQ/2305.html}
}

@online{baskarSemisupervisedSequencetosequenceASR2019,
  title = {Semi-Supervised {{Sequence-to-sequence ASR}} Using {{Unpaired Speech}} and {{Text}}},
  author = {Baskar, Murali Karthick and Watanabe, Shinji and Astudillo, Ramon and Hori, Takaaki and Burget, Lukáš and Černocký, Jan},
  date = {2019-08-20},
  eprint = {1905.01152},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.1905.01152},
  url = {http://arxiv.org/abs/1905.01152},
  urldate = {2025-09-24},
  abstract = {Sequence-to-sequence automatic speech recognition (ASR) models require large quantities of data to attain high performance. For this reason, there has been a recent surge in interest for unsupervised and semi-supervised training in such models. This work builds upon recent results showing notable improvements in semi-supervised training using cycle-consistency and related techniques. Such techniques derive training procedures and losses able to leverage unpaired speech and/or text data by combining ASR with Text-to-Speech (TTS) models. In particular, this work proposes a new semi-supervised loss combining an end-to-end differentiable ASR\$\textbackslash rightarrow\$TTS loss with TTS\$\textbackslash rightarrow\$ASR loss. The method is able to leverage both unpaired speech and text data to outperform recently proposed related techniques in terms of \textbackslash\%WER. We provide extensive results analyzing the impact of data quantity and speech and text modalities and show consistent gains across WSJ and Librispeech corpora. Our code is provided in ESPnet to reproduce the experiments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/UH5UGFWB/Baskar et al. - 2019 - Semi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text.pdf;/home/pccady/Zotero/storage/8YSR83AE/1905.html}
}

@article{benderAchievingEvaluatingLanguageIndependence2011,
  title = {On {{Achieving}} and {{Evaluating Language-Independence}} in {{NLP}}},
  author = {Bender, Emily M.},
  date = {2011-10-01},
  journaltitle = {Linguistic Issues in Language Technology},
  shortjournal = {LiLT},
  volume = {6},
  issn = {1945-3604},
  doi = {10.33011/lilt.v6i.1239},
  url = {https://journals.colorado.edu/index.php/lilt/article/view/1239},
  urldate = {2025-04-04},
  abstract = {Language independence is commonly presented as one of the advantages of modern, machine-learning approaches to NLP, and it is an important type of scalability.  In this position paper, I critically review the widespread approaches to achieving and evaluating language independence in the field of computational linguistics and argue that, on the one hand, we are not truly evaluating language independence with any systematicity and on the other hand, that truly language-independent technology requires more linguistic sophistication than is the norm.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/US6RJ7YH/Bender - 2011 - On Achieving and Evaluating Language-Independence in NLP.pdf}
}

@article{besacierAutomaticSpeechRecognition2014,
  title = {Automatic Speech Recognition for Under-Resourced Languages: {{A}} Survey},
  shorttitle = {Automatic Speech Recognition for Under-Resourced Languages},
  author = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  date = {2014-01},
  journaltitle = {Speech Communication},
  volume = {56},
  pages = {85--100},
  publisher = {Elsevier BV},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.07.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639313000988},
  urldate = {2025-07-15},
  abstract = {Speech processing for under-resourced languages is an active field of research, which has experienced significant progress during the past decade. We propose, in this paper, a survey that focuses on automatic speech recognition (ASR) for these languages. The definition of under-resourced languages and the challenges associated to them are first defined. The main part of the paper is a literature review of the recent (last 8 years) contributions made in ASR for under-resourced languages. Examples of past projects and future trends when dealing with under-resourced languages are also presented. We believe that this paper will be a good starting point for anyone interested to initiate research in (or operational development of) ASR for one or several under-resourced languages. It should be clear, however, that many of the issues and approaches presented here, apply to speech technology in general (text-to-speech synthesis for instance).},
  langid = {english},
  file = {/home/pccady/Zotero/storage/AKFY7UJQ/Besacier et al. - 2014 - Automatic speech recognition for under-resourced languages A survey.pdf}
}

@article{biewaldExperimentTrackingWeights2020,
  title = {Experiment Tracking with Weights and Biases},
  author = {family=Biewald, given=Lukas, prefix=and others, useprefix=false},
  date = {2020}
}

@inproceedings{birdNLTKNaturalLanguage2006,
  title = {{{NLTK}}: {{The Natural Language Toolkit}}},
  shorttitle = {{{NLTK}}},
  booktitle = {Proceedings of the {{COLING}}/{{ACL}} 2006 {{Interactive Presentation Sessions}}},
  author = {Bird, Steven},
  editor = {Curran, James},
  date = {2006-07},
  pages = {69--72},
  publisher = {Association for Computational Linguistics},
  location = {Sydney, Australia},
  doi = {10.3115/1225403.1225421},
  url = {https://aclanthology.org/P06-4018/},
  urldate = {2025-09-12},
  eventtitle = {{{COLING-ACL}} 2006},
  file = {/home/pccady/Zotero/storage/XVX9CWIQ/Bird - 2006 - NLTK The Natural Language Toolkit.pdf}
}

@article{bodnarEvaluatingMotivationalImpact2016,
  title = {Evaluating the Motivational Impact of {{CALL}} Systems: Current Practices and Future Directions},
  shorttitle = {Evaluating the Motivational Impact of {{CALL}} Systems},
  author = {Bodnar, Stephen and Cucchiarini, Catia and Strik, Helmer and Van Hout, Roeland},
  date = {2016-01-02},
  journaltitle = {Computer Assisted Language Learning},
  shortjournal = {Computer Assisted Language Learning},
  volume = {29},
  number = {1},
  pages = {186--212},
  issn = {0958-8221, 1744-3210},
  doi = {10.1080/09588221.2014.927365},
  url = {http://www.tandfonline.com/doi/full/10.1080/09588221.2014.927365},
  urldate = {2025-04-04},
  langid = {english},
  file = {/home/pccady/Zotero/storage/W5NCUPMW/Bodnar et al. - 2016 - Evaluating the motivational impact of CALL systems current practices and future directions.pdf}
}

@inproceedings{bouliannePhonemeTranscriptionEndangered2022,
  title = {Phoneme Transcription of Endangered Languages: An Evaluation of Recent {{ASR}} Architectures in the Single Speaker Scenario},
  shorttitle = {Phoneme Transcription of Endangered Languages},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Boulianne, Gilles},
  date = {2022},
  pages = {2301--2308},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.180},
  url = {https://aclanthology.org/2022.findings-acl.180},
  urldate = {2025-04-04},
  abstract = {Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies have reported encouraging results for phonetic transcription even with small amounts of training. Here we expand this body of work on speaker-dependent transcription by comparing four ASR approaches, notably recent transformer and pretrained multilingual models, on a common dataset of 11 languages. To automate data preparation, training and evaluation steps, we also developed a phoneme recognition setup which handles morphologically complex languages and writing systems for which no pronunciation dictionary exists. We find that fine-tuning a multilingual pretrained model yields an average phoneme error rate (PER) of 15\% for 6 languages with 99 minutes or less of transcribed data for training. For the 5 languages with between 100 and 192 minutes of training, we achieved a PER of 8.4\% or less. These results on a number of varied languages suggest that ASR can now significantly reduce transcription efforts in the speaker-dependent situation common in endangered language work.},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  langid = {english},
  file = {/home/pccady/Zotero/storage/JQZFFRPM/Boulianne - 2022 - Phoneme transcription of endangered languages an evaluation of recent ASR architectures in the sing.pdf}
}

@article{broinNewUrbanIrish2014,
  title = {New {{Urban Irish}}: {{Pidgin}}, {{Creole}}, or {{Bona Fide Dialect}}? {{The Phonetics}} and {{Morphology}} of {{City}} and {{Gaeltacht Speakers Systematically Compared}}},
  author = {Broin, Brian Ó},
  date = {2014},
  abstract = {This article compares the phonetics and morphology of Irish spoken in the Gaeltacht with that spoken in Irish cities. Informants were identified by randomly selecting newsreaders and chat show hosts on Gaeltacht and urban Irishlanguage radio stations. Recordings of the speakers were transcribed and then analysed for morphological and phonetic accuracy. City speakers demonstrated a move towards simplified morphology and phonology, making fewer than 50\% of expected changes, while Gaeltacht speakers retained the language’s traditional forms, making more than 90\% of expected changes. It was discovered that the city speakers, while apparently speaking stable idiolects, each returned very different rates, suggesting that the cities do not yet have stable Irish dialects. The Gaeltacht speakers all returned very similar rates.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/ULMAZYR4/Broin - New Urban Irish Pidgin, Creole, or Bona Fide Dialect The Phonetics and Morphology of City and Gael.pdf}
}

@online{calikEnsemblebasedFrameworkMispronunciation2023,
  title = {An Ensemble-Based Framework for Mispronunciation Detection of {{Arabic}} Phonemes},
  author = {Calik, Sukru Selim and Kucukmanisa, Ayhan and Kilimci, Zeynep Hilal},
  date = {2023-01-03},
  eprint = {2301.01378},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.01378},
  url = {http://arxiv.org/abs/2301.01378},
  urldate = {2025-09-06},
  abstract = {Determination of mispronunciations and ensuring feedback to users are maintained by computerassisted language learning (CALL) systems. In this work, we introduce an ensemble model that defines the mispronunciation of Arabic phonemes and assists learning of Arabic, effectively. To the best of our knowledge, this is the very first attempt to determine the mispronunciations of Arabic phonemes employing ensemble learning techniques and conventional machine learning models, comprehensively. In order to observe the effect of feature extraction techniques, melfrequency cepstrum coefficients (MFCC), and Mel spectrogram are blended with each learning algorithm. To show the success of proposed model, 29 letters in the Arabic phonemes, 8 of which are hafiz, are voiced by a total of 11 different person. The amount of data set has been enhanced employing the methods of adding noise, time shifting, time stretching, pitch shifting. Extensive experiment results demonstrate that the utilization of voting classifier as an ensemble algorithm with Mel spectrogram feature extraction technique exhibits remarkable classification result with 95.9\% of accuracy.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/C2HTVNGL/Calik et al. - 2023 - An ensemble-based framework for mispronunciation detection of Arabic phonemes.pdf}
}

@article{calikEnsemblebasedFrameworkMispronunciation2023a,
  title = {An Ensemble-Based Framework for Mispronunciation Detection of {{Arabic}} Phonemes},
  author = {Calık, Sükrü Selim and Kucukmanisa, Ayhan and Kilimci, Zeynep Hilal},
  date = {2023-09},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {212},
  pages = {109593},
  issn = {0003682X},
  doi = {10.1016/j.apacoust.2023.109593},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X23003912},
  urldate = {2025-09-17},
  langid = {english},
  file = {/home/pccady/Zotero/storage/MH9668SX/main.pdf}
}

@online{caoSegmentationfreeGoodnessPronunciation2025,
  title = {Segmentation-Free {{Goodness}} of {{Pronunciation}}},
  author = {Cao, Xinwei and Fan, Zijian and Svendsen, Torbjørn and Salvi, Giampiero},
  date = {2025-07-24},
  eprint = {2507.16838},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2507.16838},
  url = {http://arxiv.org/abs/2507.16838},
  urldate = {2025-09-19},
  abstract = {Mispronunciation detection and diagnosis (MDD) is a significant part in modern computer aided language learning (CALL) systems. Within MDD, phoneme-level pronunciation assessment is key to helping L2 learners improve their pronunciation. However, most systems are based on a form of goodness of pronunciation (GOP) which requires pre-segmentation of speech into phonetic units. This limits the accuracy of these methods and the possibility to use modern CTC-based acoustic models for their evaluation. In this study, we first propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR models for MDD. Next, we define a more general alignment-free method that takes all possible alignments of the target phoneme into account (GOP-AF). We give a theoretical account of our definition of GOP-AF, an implementation that solves potential numerical issues as well as a proper normalization which makes the method applicable with acoustic models with different peakiness over time. We provide extensive experimental results on the CMU Kids and Speechocean762 datasets comparing the different definitions of our methods, estimating the dependency of GOP-AF on the peakiness of the acoustic models and on the amount of context around the target phoneme. Finally, we compare our methods with recent studies over the Speechocean762 data showing that the feature vectors derived from the proposed method achieve state-of-the-art results on phoneme-level pronunciation assessment.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/SLFCWCD5/Cao et al. - 2025 - Segmentation-free Goodness of Pronunciation.pdf;/home/pccady/Zotero/storage/KP48LBYX/2507.html}
}

@book{chasaideABAIRInitiativeBringing2017,
  title = {The {{ABAIR}} Initiative: {{Bringing Spoken Irish}} into the {{Digital Space}}},
  shorttitle = {The {{ABAIR}} Initiative},
  author = {Chasaide, Ailbhe and Ní Chiaráin, Neasa and Wendler, Christoph and Berthelsen, Harald and Murphy, Andy and Gobl, Christer},
  date = {2017-08-20},
  doi = {10.21437/Interspeech.2017-1407},
  file = {/home/pccady/Zotero/storage/2JKBUKKG/Chasaide et al. - 2017 - The ABAIR initiative Bringing Spoken Irish into the Digital Space.pdf}
}

@inproceedings{chenComputerassistedPronunciationTraining2016,
  title = {Computer-Assisted Pronunciation Training: {{From}} Pronunciation Scoring towards Spoken Language Learning},
  shorttitle = {Computer-Assisted Pronunciation Training},
  booktitle = {2016 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA}})},
  author = {Chen, Nancy F. and Li, Haizhou},
  date = {2016-12},
  pages = {1--7},
  publisher = {IEEE},
  location = {Jeju, South Korea},
  doi = {10.1109/APSIPA.2016.7820782},
  url = {http://ieeexplore.ieee.org/document/7820782/},
  urldate = {2025-04-04},
  abstract = {This paper reviews the research approaches used in computer-assisted pronunciation training (CAPT), addresses the existing challenges, and discusses emerging trends and opportunities. To complement existing work, our analysis places more emphasis on pronunciation teaching and learning (as opposed to pronunciation assessment), prosodic error detection (as opposed to phonetic error detection), and research work from the past five years given the recent rapid development in spoken language technology.},
  eventtitle = {2016 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA}})},
  isbn = {978-988-14768-2-1},
  langid = {english},
  file = {/home/pccady/Zotero/storage/G5Y3MTKU/Chen and Li - 2016 - Computer-assisted pronunciation training From pronunciation scoring towards spoken language learnin.pdf}
}

@inproceedings{chenEndtoEndNeuralNetwork2018,
  title = {End-to-{{End Neural Network Based Automated Speech Scoring}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Lei and Tao, Jidong and Ghaffarzadegan, Shabnam and Qian, Yao},
  date = {2018-04},
  pages = {6234--6238},
  publisher = {IEEE},
  location = {Calgary, AB},
  doi = {10.1109/ICASSP.2018.8462562},
  url = {https://ieeexplore.ieee.org/document/8462562/},
  urldate = {2025-04-04},
  abstract = {In recent years, machine learning models for automated speech scoring systems were mainly built using data-driven approaches with handcrafted features as one of the main components. However, the remarkable successes of deep learning (DL) technology in a variety of machine learning tasks has demonstrated its effectiveness in extracting features. Although there have been some efforts in utilizing DL technology for the automated speech scoring task, a thorough investigation of learning useful features is still missing. In this paper, we propose an end-to-end solution that consists of using deep neural network models to encode both lexical and acoustical cues to learn predictive features automatically. Experiments also confirm the effectiveness of our proposed solution compared to conventional methods based on handcrafted features.},
  eventtitle = {{{ICASSP}} 2018 - 2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5386-4658-8},
  langid = {english},
  file = {/home/pccady/Zotero/storage/53IG6K6Q/Chen et al. - 2018 - End-to-End Neural Network Based Automated Speech Scoring.pdf}
}

@article{collinsGaeltechVRMeasuringImpact2019,
  title = {{{GaeltechVR}}: {{Measuring}} the {{Impact}} of an {{Immersive Virtual Environment}} to {{Promote Situated Identity}} in {{Irish Language Learning}}},
  author = {Collins, Naoise and Vaughan, Dr Brian and Cullen, Dr Charlie and Gardner, Dr Keith},
  date = {2019},
  volume = {12},
  number = {3},
  abstract = {This study investigates how a design-based research methodology is best suited to measuring the impact of a designed virtual reality experience to improve situated identity in Irish learners focusing on their attitudes, motivation, and confidence as Irish language learners. This paper describes the design of GaeltechVR: an immersive Irish language VR experience designed for the VIVE Pro. It also gives the results of a mixed-methods study to measure the impact in a local adult Irish language learner context. A questionnaire on situated attitudes and motivation to language learning (Ushioda \& Dörnyei, 2009) was adapted for the Irish context to investigate a small scale sample of the local context’s attitudes to Irish language learning. The participant’s gameplay was recorded for analysis along with questionnaires on presence (Witmer \& Singer, 1998), simulator sickness and an adapted questionnaire on their attitudes after the intervention.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/DVI524TZ/Collins et al. - 2019 - GaeltechVR Measuring the Impact of an Immersive Virtual Environment to Promote Situated Identity in.pdf}
}

@article{collinsSituatedImmersiveGaming2021,
  title = {Situated {{Immersive Gaming Environments}} for {{Irish Language Learning}}},
  author = {Collins, Naoise},
  date = {2021},
  publisher = {Technological University Dublin},
  doi = {10.21427/JKMJ-XM34},
  url = {https://arrow.tudublin.ie/tourdoc/35},
  urldate = {2025-04-04},
  abstract = {In this thesis, three cycles of design based research are outlined, implementing a situated immersive virtual reality game for Irish language learning. It was undertaken in order to investigate a potential technological solution to improve the limited number of daily Irish adult speakers in Ireland, 3\%. It examines the intersection between game based learning, Irish language learning and virtual reality technology and the methodological approach undertaken follows a design based research paradigm.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/JR69R49N/Collins - 2021 - Situated Immersive Gaming Environments for Irish Language Learning.pdf}
}

@online{conneauFLEURSFewshotLearning2022,
  title = {{{FLEURS}}: {{Few-shot Learning Evaluation}} of {{Universal Representations}} of {{Speech}}},
  shorttitle = {{{FLEURS}}},
  author = {Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},
  date = {2022-05-25},
  eprint = {2205.12446},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.12446},
  url = {http://arxiv.org/abs/2205.12446},
  urldate = {2025-04-04},
  abstract = {We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/ETIS2DLG/Conneau et al. - 2022 - FLEURS Few-shot Learning Evaluation of Universal Representations of Speech.pdf}
}

@online{conneauUnsupervisedCrosslingualRepresentation2020,
  title = {Unsupervised {{Cross-lingual Representation Learning}} for {{Speech Recognition}}},
  author = {Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  date = {2020-12-15},
  eprint = {2006.13979},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.13979},
  url = {http://arxiv.org/abs/2006.13979},
  urldate = {2025-04-04},
  abstract = {This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/KQLZCMY9/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning for Speech Recognition.pdf}
}

@online{deichlerMMConvMultimodalConversational2024,
  title = {{{MM-Conv}}: {{A Multi-modal Conversational Dataset}} for {{Virtual Humans}}},
  shorttitle = {{{MM-Conv}}},
  author = {Deichler, Anna and O'Regan, Jim and Beskow, Jonas},
  date = {2024-09-30},
  eprint = {2410.00253},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.00253},
  url = {http://arxiv.org/abs/2410.00253},
  urldate = {2025-04-04},
  abstract = {In this paper, we present a novel dataset captured using a VR headset to record conversations between participants within a physics simulator (AI2-THOR). Our primary objective is to extend the field of co-speech gesture generation by incorporating rich contextual information within referential settings. Participants engaged in various conversational scenarios, all based on referential communication tasks. The dataset provides a rich set of multimodal recordings such as motion capture, speech, gaze, and scene graphs. This comprehensive dataset aims to enhance the understanding and development of gesture generation models in 3D scenes by providing diverse and contextually rich data.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction},
  file = {/home/pccady/Zotero/storage/M5EG8TE2/Deichler et al. - 2024 - MM-Conv A Multi-modal Conversational Dataset for Virtual Humans.pdf}
}

@inproceedings{dengEnsembleDeepLearning2014,
  title = {Ensemble {{Deep Learning}} for {{Speech Recognition}}},
  booktitle = {Proc. Interspeech},
  author = {Deng, Li and Platt, John C.},
  date = {2014},
  abstract = {Ensemble Deep Learning for Speech Recognition Li Deng and John C. Platt    Microsoft Research, One Microsoft Way, Redmond, WA, USA deng@microsoft.com; jplatt@microsoft.com  Abstract Deep learning systems have dramatically improved the accuracy of speech recognition, and various deep architecture...},
  langid = {english},
  file = {/home/pccady/Zotero/storage/44E9M6S7/Deng and Platt - 2014 - Ensemble Deep Learning for Speech Recognition.pdf}
}

@article{dengScalableStackingLearning2012,
  title = {Scalable Stacking and Learning for Building Deep Architectures},
  author = {Deng, li and Yu, Dong and Platt, John},
  date = {2012-03-01},
  journaltitle = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on},
  shortjournal = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on},
  doi = {10.1109/ICASSP.2012.6288333},
  abstract = {Deep Neural Networks (DNNs) have shown remarkable success in pattern recognition tasks. However, parallelizing DNN training across computers has been dif?cult. We present the Deep Stack- ing Network (DSN), which overcomes the problem of paralleliz- ing learning algorithms for deep architectures. The DSN provides a method of stacking simple processing modules in buiding deep architectures, with a convex learning problem in each module. Ad- ditional ?ne tuning further improves the DSN, while introducing mi- nor non-convexity. Full learning in the DSN is batch-mode, making it amenable to parallel training over many machines and thus be scal- able over the potentially huge size of the training data. Experimental results on both the MNIST (image) and TIMIT (speech) classi?ca- tion tasks demonstrate that the DSN learning algorithm developed in this work is not only parallelizable in implementation but it also attains higher classi?cation accuracy than the DNN.},
  file = {/home/pccady/Zotero/storage/WN97TG6Y/Deng et al. - 2012 - Scalable stacking and learning for building deep architectures.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423/},
  urldate = {2025-08-28},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {/home/pccady/Zotero/storage/QRBYYV99/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@article{dickersonMiljopartietNeverendingNuclear,
  title = {Miljöpartiet and the Never-Ending Nuclear Energy Debate},
  author = {Dickerson, Claire},
  langid = {english},
  file = {/home/pccady/Zotero/storage/5FIIE56P/Dickerson - Miljöpartiet and the never-ending nuclear energy debate.pdf}
}

@inproceedings{dietterichEnsembleMethodsMachine2000,
  title = {Ensemble Methods in Machine Learning},
  author = {Dietterich, Thomas G},
  date = {2000},
  pages = {1--15},
  publisher = {Springer},
  url = {https://www2.cs.uh.edu/~ceick/7362/T5-3.pdf},
  urldate = {2025-09-06},
  eventtitle = {International Workshop on Multiple Classifier Systems},
  file = {/home/pccady/Zotero/storage/TGEEURGX/T5-3.pdf}
}

@article{duibhirResearchReportNovember,
  title = {Research {{Report November}} 2017},
  author = {Duibhir, Pádraig Ó and NigUidhir, Gabrielle and Cathalláin, Seán Ó and Thuairisg, Laoise Ní and Cosgrove, Jude},
  langid = {english},
  file = {/home/pccady/Zotero/storage/PU4BKJQ3/Duibhir et al. - Research Report November 2017.pdf}
}

@book{engstrandFonetikensGrunder2004,
  title = {Fonetikens Grunder},
  author = {Engstrand, Olle},
  date = {2004},
  publisher = {Studentlitteratur},
  location = {Lund},
  isbn = {978-91-44-04238-1},
  pagetotal = {355}
}

@article{eskenaziOverviewSpokenLanguage2009,
  title = {An Overview of Spoken Language Technology for Education},
  author = {Eskenazi, Maxine},
  date = {2009-10},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {51},
  number = {10},
  pages = {832--844},
  issn = {01676393},
  doi = {10.1016/j.specom.2009.04.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639309000673},
  urldate = {2025-04-04},
  abstract = {This paper reviews research in spoken language technology for education and more specifically for language learning. It traces the history of the domain and then groups main issues in the interaction with the student. It addresses the modalities of interaction and their implementation issues and algorithms. Then it discusses one user population – children – and an application for them. Finally it has a discussion of overall systems. It can be used as an introduction to the field and a source of reference materials.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/MEGHYA3U/Eskenazi - 2009 - An overview of spoken language technology for education.pdf}
}

@online{fazelSynthASRUnlockingSynthetic2021,
  title = {{{SynthASR}}: {{Unlocking Synthetic Data}} for {{Speech Recognition}}},
  shorttitle = {{{SynthASR}}},
  author = {Fazel, Amin and Yang, Wei and Liu, Yulan and Barra-Chicote, Roberto and Meng, Yixiong and Maas, Roland and Droppo, Jasha},
  date = {2021-06-14},
  eprint = {2106.07803},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.07803},
  url = {http://arxiv.org/abs/2106.07803},
  urldate = {2025-09-13},
  abstract = {End-to-end (E2E) automatic speech recognition (ASR) models have recently demonstrated superior performance over the traditional hybrid ASR models. Training an E2E ASR model requires a large amount of data which is not only expensive but may also raise dependency on production data. At the same time, synthetic speech generated by the state-of-the-art text-to-speech (TTS) engines has advanced to near-human naturalness. In this work, we propose to utilize synthetic speech for ASR training (SynthASR) in applications where data is sparse or hard to get for ASR model training. In addition, we apply continual learning with a novel multi-stage training strategy to address catastrophic forgetting, achieved by a mix of weighted multi-style training, data augmentation, encoder freezing, and parameter regularization. In our experiments conducted on in-house datasets for a new application of recognizing medication names, training ASR RNN-T models with synthetic audio via the proposed multi-stage training improved the recognition performance on new application by more than 65\% relative, without degradation on existing general applications. Our observations show that SynthASR holds great promise in training the state-of-the-art large-scale E2E ASR models for new applications while reducing the costs and dependency on production data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/5SBEK7VU/Fazel et al. - 2021 - SynthASR Unlocking Synthetic Data for Speech Recognition.pdf;/home/pccady/Zotero/storage/P8XNTKVX/2106.html}
}

@inproceedings{fiscus1997post,
  title = {A Post-Processing System to Yield Reduced Word Error Rates: {{Recognizer}} Output Voting Error Reduction ({{ROVER}})},
  booktitle = {1997 {{IEEE}} Workshop on Automatic Speech Recognition and Understanding Proceedings},
  author = {Fiscus, Jonathan G},
  date = {1997},
  pages = {347--354},
  publisher = {IEEE},
  file = {/home/pccady/Zotero/storage/X9QBVI6H/Fiscus - 1997 - A post-processing system to yield reduced word error rates Recognizer output voting error reduction.pdf}
}

@online{fuFullTextDependentEnd2021,
  title = {A {{Full Text-Dependent End}} to {{End Mispronunciation Detection}} and {{Diagnosis}} with {{Easy Data Augmentation Techniques}}},
  author = {Fu, Kaiqi and Lin, Jones and Ke, Dengfeng and Xie, Yanlu and Zhang, Jinsong and Lin, Binghuai},
  date = {2021-04-17},
  eprint = {2104.08428},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08428},
  url = {http://arxiv.org/abs/2104.08428},
  urldate = {2025-04-04},
  abstract = {Recently, end-to-end mispronunciation detection and diagnosis (MD\&D) systems has become a popular alternative to greatly simplify the model-building process of conventional hybrid DNN-HMM systems by representing complicated modules with a single deep network architecture. In this paper, in order to utilize the prior text in the end-to-end structure, we present a novel text-dependent model which is difference with sed-mdd, the model achieves a fully end-to-end system by aligning the audio with the phoneme sequences of the prior text inside the model through the attention mechanism. Moreover, the prior text as input will be a problem of imbalance between positive and negative samples in the phoneme sequence. To alleviate this problem, we propose three simple data augmentation methods, which effectively improve the ability of model to capture mispronounced phonemes. We conduct experiments on L2ARCTIC, and our best performance improved from 49.29\% to 56.08\% in F-measure metric compared to the CNN-RNN-CTC model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/Q9VYFCIG/Fu et al. - 2021 - A Full Text-Dependent End to End Mispronunciation Detection and Diagnosis with Easy Data Augmentatio.pdf}
}

@article{gabrieleEnglishInfluenceL2,
  title = {English {{Influence}} on {{L2 Speakers}}’ {{Production}} of {{Palatalization}} and {{Velarization}}},
  author = {Gabriele, Jennifer C},
  langid = {english},
  file = {/home/pccady/Zotero/storage/Y3E3RFSQ/Gabriele - English Influence on L2 Speakers’ Production of Palatalization and Velarization.pdf}
}

@inreference{Gaeltacht2025,
  title = {Gaeltacht},
  booktitle = {Wikipedia},
  date = {2025-06-27T11:20:47Z},
  url = {https://en.wikipedia.org/wiki/Gaeltacht#/media/File:Gaeltachtai_le_hainmneacha2.svg},
  urldate = {2025-08-19},
  abstract = {A Gaeltacht ( GAYL-təkht, Irish: [ˈɡeːl̪ˠt̪ˠəxt̪ˠ], pl. Gaeltachtaí) is a district of Ireland, either individually or collectively, where the Irish government recognises that the Irish language is the predominant vernacular, or language of the home. The Gaeltacht districts were first officially recognised during the 1920s in the early years of the Irish Free State, following the Gaelic revival, as part of a government policy aimed at restoring the Irish language. The Gaeltacht is threatened by serious language decline. Research published in 2015 showed that Irish is spoken on a daily basis by two-thirds or more of the population in only 21 of the 155 electoral divisions in the Gaeltacht. Daily language use by two-thirds or more of the population is regarded by some academics as a tipping point for language survival.},
  langid = {english},
  annotation = {Page Version ID: 1297621758},
  file = {/home/pccady/Zotero/storage/7IVVP94K/index.html}
}

@report{garofoloDARPATIMITAcousticphonetic1993,
  title = {{{DARPA TIMIT}}: Acoustic-Phonetic Continuous Speech Corpus {{CD-ROM}}, {{NIST}} Speech Disc 1-1.1},
  shorttitle = {{{DARPA TIMIT}}},
  author = {Garofolo, John S and Lamel, Lori F and Fisher, William M and Fiscus, Jonathan G and Pallett, David S and Dahlgren, Nancy L},
  date = {1993},
  number = {NIST IR 4930},
  pages = {NIST IR 4930},
  institution = {{National Institute of Standards and Technology}},
  location = {Gaithersburg, MD},
  doi = {10.6028/NIST.IR.4930},
  url = {https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir4930.pdf},
  urldate = {2025-09-09},
  langid = {english},
  file = {/home/pccady/Zotero/storage/LPFU545T/Garofolo et al. - 1993 - DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM, NIST speech disc 1-1.1.pdf}
}

@inproceedings{gaurMixtureInformedExperts2021,
  title = {Mixture of {{Informed Experts}} for {{Multilingual Speech Recognition}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gaur, Neeraj and Farris, Brian and Haghani, Parisa and Leal, Isabel and Moreno, Pedro J. and Prasad, Manasa and Ramabhadran, Bhuvana and Zhu, Yun},
  date = {2021-06},
  pages = {6234--6238},
  issn = {2379-190X},
  doi = {10.1109/ICASSP39728.2021.9414379},
  url = {https://ieeexplore.ieee.org/document/9414379/},
  urldate = {2025-09-15},
  abstract = {When trained on related or low-resource languages, multilingual speech recognition models often outperform their monolingual counterparts. However, these models can suffer from loss in performance for high resource or unrelated languages. We investigate the use of a mixture-of-experts approach to assign per-language parameters in the model to increase network capacity in a structured fashion. We introduce a novel variant of this approach, ‘informed experts’, which attempts to tackle inter-task conflicts by eliminating gradients from other tasks in these task-specific parameters. We conduct experiments on a real-world task with English, French and four dialects of Arabic to show the effectiveness of our approach. Our model matches or outperforms the monolingual models for almost all languages, with gains of as much as 31\% relative. Our model also outperforms the baseline multilingual model for all languages by up to 9\% relative.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Acoustics,Conferences,end-to-end speech recognition,language id,mixture of experts,multilingual,RNN-T,Signal processing,Speech processing,Speech recognition,Task analysis,Training},
  file = {/home/pccady/Zotero/storage/3M92YV3S/Gaur et al. - 2021 - Mixture of Informed Experts for Multilingual Speech Recognition.pdf}
}

@inproceedings{gitmanConfidencebasedEnsemblesEndtoEnd2023,
  title = {Confidence-Based {{Ensembles}} of {{End-to-End Speech Recognition Models}}},
  booktitle = {{{INTERSPEECH}} 2023},
  author = {Gitman, Igor and Lavrukhin, Vitaly and Laptev, Aleksandr and Ginsburg, Boris},
  date = {2023-08-20},
  eprint = {2306.15824},
  eprinttype = {arXiv},
  eprintclass = {eess},
  pages = {1414--1418},
  doi = {10.21437/Interspeech.2023-1281},
  url = {http://arxiv.org/abs/2306.15824},
  urldate = {2025-04-04},
  abstract = {The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models’ target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/HV8PGS5P/Gitman et al. - 2023 - Confidence-based Ensembles of End-to-End Speech Recognition Models.pdf}
}

@inproceedings{gongTransformerBasedMultiAspectMultiGranularity2022,
  title = {Transformer-{{Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gong, Yuan and Chen, Ziyi and Chu, Iek-Heng and Chang, Peng and Glass, James},
  date = {2022-05-23},
  eprint = {2205.03432},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {7262--7266},
  doi = {10.1109/ICASSP43922.2022.9746743},
  url = {http://arxiv.org/abs/2205.03432},
  urldate = {2025-04-04},
  abstract = {Automatic pronunciation assessment is an important technology to help self-directed language learners. While pronunciation quality has multiple aspects including accuracy, fluency, completeness, and prosody, previous efforts typically only model one aspect (e.g., accuracy) at one granularity (e.g., at the phoneme-level). In this work, we explore modeling multi-aspect pronunciation assessment at multiple granularities. Specifically, we train a Goodness Of Pronunciation feature-based Transformer (GOPT) with multi-task learning. Experiments show that GOPT achieves the best results on speechocean762 with a public automatic speech recognition (ASR) acoustic model trained on Librispeech. Code at https://github.com/YuanGongND/gopt.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/BS9CBM4Q/Gong et al. - 2022 - Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment.pdf}
}

@article{gravesConnectionistTemporalClassification,
  title = {Connectionist {{Temporal Classiﬁcation}}: {{Labelling Unsegmented Sequence Data}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/KY7HCUCD/Graves et al. - Connectionist Temporal Classiﬁcation Labelling Unsegmented Sequence Data with Recurrent Neural Netw.pdf}
}

@online{guoCalibrationModernNeural2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  date = {2017-08-03},
  eprint = {1706.04599},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.04599},
  url = {http://arxiv.org/abs/1706.04599},
  urldate = {2025-06-16},
  abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/M6YFZKNL/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@book{hansenedwardsPhonologySecondLanguage2008,
  title = {Phonology and {{Second Language Acquisition}}},
  editor = {Hansen Edwards, Jette G. and Zampini, Mary L.},
  date = {2008},
  series = {Studies in {{Bilingualism}}},
  publisher = {John Benjamins Publishing Company},
  location = {Amsterdam},
  doi = {10.1075/sibil.36},
  abstract = {This volume is a collection of 13 chapters, each devoted to a particular issue that is crucial to our understanding of the way learners acquire, learn, and use an L2 sound system. In addition, it spans both theory and application in L2 phonology. The book is divided into three parts, with each section unified by broad thematic content: Part I, “Theoretical Issues and Frameworks in L2 Phonology,” lays the groundwork for examining L2 phonological acquisition. Part II, “Second Language Speech Perception and Production,” examines these two aspects of L2 speech in more detail. Finally, Part III, “Technology, Training, and Curriculum,” bridges the gap between theory and practice. Each chapter examines theoretical frameworks, major research findings (both classic and recent), methodological issues and choices for conducting research in a particular area of L2 phonology, and major implications of the research findings for more general models of language acquisition and/or pedagogy},
  isbn = {978-90-272-4147-4 978-90-272-9139-4},
  langid = {english},
  pagetotal = {1},
  file = {/home/pccady/Zotero/storage/3N96SQE6/Hansen Edwards and Zampini - 2008 - Phonology and Second Language Acquisition.pdf}
}

@article{hansenNeuralNetworkEnsembles1990,
  title = {Neural Network Ensembles},
  author = {Hansen, L.K. and Salamon, P.},
  date = {1990-10},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Machine Intell.},
  volume = {12},
  number = {10},
  pages = {993--1001},
  issn = {01628828},
  doi = {10.1109/34.58871},
  url = {http://ieeexplore.ieee.org/document/58871/},
  urldate = {2025-09-14},
  abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/ETCU9XEC/Hansen and Salamon - 1990 - Neural network ensembles.pdf}
}

@article{hardisonSecondlanguageSpokenWord2005,
  title = {Second-Language Spoken Word Identification: {{Effects}} of Perceptual Training, Visual Cues, and Phonetic Environment},
  shorttitle = {Second-Language Spoken Word Identification},
  author = {Hardison, Debra M.},
  date = {2005-10},
  journaltitle = {Applied Psycholinguistics},
  shortjournal = {Applied Psycholinguistics},
  volume = {26},
  number = {4},
  pages = {579--596},
  issn = {0142-7164, 1469-1817},
  doi = {10.1017/S0142716405050319},
  url = {https://www.cambridge.org/core/product/identifier/S0142716405050319/type/journal_article},
  urldate = {2025-04-04},
  abstract = {Experiments using the gating paradigm investigated the effects of auditory–visual (AV) and auditoryonly perceptual training on second-language spoken-word identification by Japanese and Korean learners of English. Stimuli were familiar bisyllabic words beginning with /p/, /f/, /ô/, /l/, and /s, t, k/ combined with high, low, and rounded vowels. Results support the priming role of visual cues in AV speech processing. Identification was earlier with visual cues and following training, especially for words beginning with /ô/ and /l/, which also showed significant effects of adjacent vowel. For the Japanese, the AV advantage in identifying /ô/- and /l/-initial words was accentuated following training. Findings are discussed within a multimodal episodic model of learning.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/294NNY9V/Hardison - 2005 - Second-language spoken word identification Effects of perceptual training, visual cues, and phoneti.pdf}
}

@online{hayashiBackTranslationStyleDataAugmentation2018,
  title = {Back-{{Translation-Style Data Augmentation}} for {{End-to-End ASR}}},
  author = {Hayashi, Tomoki and Watanabe, Shinji and Zhang, Yu and Toda, Tomoki and Hori, Takaaki and Astudillo, Ramon and Takeda, Kazuya},
  date = {2018-07-28},
  eprint = {1807.10893},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1807.10893},
  url = {http://arxiv.org/abs/1807.10893},
  urldate = {2025-09-24},
  abstract = {In this paper we propose a novel data augmentation method for attention-based end-to-end automatic speech recognition (E2E-ASR), utilizing a large amount of text which is not paired with speech signals. Inspired by the back-translation technique proposed in the field of machine translation, we build a neural text-to-encoder model which predicts a sequence of hidden states extracted by a pre-trained E2E-ASR encoder from a sequence of characters. By using hidden states as a target instead of acoustic features, it is possible to achieve faster attention learning and reduce computational cost, thanks to sub-sampling in E2E-ASR encoder, also the use of the hidden states can avoid to model speaker dependencies unlike acoustic features. After training, the text-to-encoder model generates the hidden states from a large amount of unpaired text, then E2E-ASR decoder is retrained using the generated hidden states as additional training data. Experimental evaluation using LibriSpeech dataset demonstrates that our proposed method achieves improvement of ASR performance and reduces the number of unknown words without the need for paired data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/KKF67FYS/Hayashi et al. - 2018 - Back-Translation-Style Data Augmentation for End-to-End ASR.pdf;/home/pccady/Zotero/storage/B47YNEAY/1807.html}
}

@online{hedderichSurveyRecentApproaches2021,
  title = {A {{Survey}} on {{Recent Approaches}} for {{Natural Language Processing}} in {{Low-Resource Scenarios}}},
  author = {Hedderich, Michael A. and Lange, Lukas and Adel, Heike and Strötgen, Jannik and Klakow, Dietrich},
  date = {2021-04-09},
  eprint = {2010.12309},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.12309},
  url = {http://arxiv.org/abs/2010.12309},
  urldate = {2025-04-04},
  abstract = {Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/Q5GCKUZY/Hedderich et al. - 2021 - A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios.pdf}
}

@inproceedings{hoContrastiveLearningAdversarial2020,
  title = {Contrastive {{Learning}} with {{Adversarial Examples}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Chih-Hui and Nvasconcelos, Nuno},
  date = {2020},
  volume = {33},
  pages = {17081--17093},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/c68c9c8258ea7d85472dd6fd0015f047-Abstract.html},
  urldate = {2025-09-18},
  abstract = {Contrastive learning (CL) is a popular technique for self-supervised learning (SSL) of visual representations. It uses pairs of augmentations of unlabeled training examples to define a classification task for pretext learning of a deep embedding. Despite extensive works in augmentation procedures, prior works do not address the selection of challenging negative pairs, as images within a sampled batch are treated independently. This paper addresses the problem, by introducing a new family of adversarial examples for constrastive learning and using these examples to define a new adversarial training algorithm for SSL, denoted as CLAE. When compared to standard CL, the use of adversarial examples creates more challenging positive pairs and adversarial training produces harder negative pairs by accounting for all images in a batch during the optimization. CLAE is compatible with many CL methods in the literature. Experiments show that it improves the performance of several existing CL baselines on multiple datasets.},
  file = {/home/pccady/Zotero/storage/3KD3FS7V/Ho and Nvasconcelos - 2020 - Contrastive Learning with Adversarial Examples.pdf}
}

@article{holmbergDesigningAddedPedagogical,
  title = {Designing for Added Pedagogical Value},
  author = {Holmberg, Jörgen},
  abstract = {In an increasingly digitized world teachers are expected to take on the role of educational designers and use ICT to design in ways that add pedagogical value to teaching and learning. This thesis adopts a design-based research (DBR) approach to: (a) explore and contribute to the educational design processes of teachers of English as a foreign language in their efforts to use ICT for added pedagogical value, (b) examine how ICT is used in educational designs to create/contribute to what the teachers and students describe as added value and (c) explore, problematize and refine DBR as a research approach.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/EEJTFJTW/Holmberg - Designing for added pedagogical value.pdf}
}

@article{homaRoleFeedbackCategory,
  title = {Role of {{Feedback}}, {{Category Size}}, and {{Stimulus Distortion}} on the {{Acquisition}} and {{Utilization}} of {{Ill-Defined Categories}}},
  author = {Homa, Donald and Cultice, Joan},
  langid = {english},
  file = {/home/pccady/Zotero/storage/C5I8E6D7/Homa and Cultice - Role of Feedback, Category Size, and Stimulus Distortion on the Acquisition and Utilization of Ill-D.pdf}
}

@online{hosseini-kivananiExperimentsASRbasedMispronunciation2021,
  title = {Experiments of {{ASR-based}} Mispronunciation Detection for Children and Adult {{English}} Learners},
  author = {Hosseini-Kivanani, Nina and Gretter, Roberto and Matassoni, Marco and Falavigna, Giuseppe Daniele},
  date = {2021-04-13},
  eprint = {2104.05980},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.05980},
  url = {http://arxiv.org/abs/2104.05980},
  urldate = {2025-04-04},
  abstract = {Pronunciation is one of the fundamentals of language learning, and it is considered a primary factor of spoken language when it comes to an understanding and being understood by others. The persistent presence of high error rates in speech recognition domains resulting from mispronunciations motivates us to find alternative techniques for handling mispronunciations. In this study, we develop a mispronunciation assessment system that checks the pronunciation of non-native English speakers, identifies the commonly mispronounced phonemes of Italian learners of English, and presents an evaluation of the non-native pronunciation observed in phonetically annotated speech corpora. In this work, to detect mispronunciations, we used a phonebased ASR implemented using Kaldi. We used two non-native English labeled corpora; (i) a corpus of Italian adults contains 5,867 utterances from 46 speakers, and (ii) a corpus of Italian children consists of 5,268 utterances from 78 children. Our results show that the selected error model can discriminate correct sounds from incorrect sounds in both native and nonnative speech, and therefore can be used to detect pronunciation errors in non-native speech. The phone error rates show improvement in using the error language model. The ASR system shows better accuracy after applying the error model on our selected corpora.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/53ZL62HN/Hosseini-Kivanani et al. - 2021 - Experiments of ASR-based mispronunciation detection for children and adult English learners.pdf}
}

@online{hsuHuBERTSelfSupervisedSpeech2021,
  title = {{{HuBERT}}: {{Self-Supervised Speech Representation Learning}} by {{Masked Prediction}} of {{Hidden Units}}},
  shorttitle = {{{HuBERT}}},
  author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  date = {2021-06-14},
  eprint = {2106.07447},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.07447},
  url = {http://arxiv.org/abs/2106.07447},
  urldate = {2025-04-04},
  abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\% and 13\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/M6Q5JU52/Hsu et al. - 2021 - HuBERT Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.pdf}
}

@inproceedings{huangCrosslanguageKnowledgeTransfer2013,
  title = {Cross-Language Knowledge Transfer Using Multilingual Deep Neural Network with Shared Hidden Layers},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Huang, Jui-Ting and Li, Jinyu and Yu, Dong and Deng, Li and Gong, Yifan},
  date = {2013-05},
  pages = {7304--7308},
  publisher = {IEEE},
  location = {Vancouver, BC, Canada},
  doi = {10.1109/ICASSP.2013.6639081},
  url = {http://ieeexplore.ieee.org/document/6639081/},
  urldate = {2025-08-29},
  abstract = {In the deep neural network (DNN), the hidden layers can be considered as increasingly complex feature transformations and the final softmax layer as a log-linear classifier making use of the most abstract features computed in the hidden layers. While the loglinear classifier should be different for different languages, the feature transformations can be shared across languages. In this paper we propose a shared-hidden-layer multilingual DNN (SHLMDNN), in which the hidden layers are made common across many languages while the softmax layers are made language dependent. We demonstrate that the SHL-MDNN can reduce errors by 3-5\%, relatively, for all the languages decodable with the SHLMDNN, over the monolingual DNNs trained using only the language specific data. Further, we show that the learned hidden layers sharing across languages can be transferred to improve recognition accuracy of new languages, with relative error reductions ranging from 6\% to 28\% against DNNs trained without exploiting the transferred hidden layers. It is particularly interesting that the error reduction can be achieved for the target language that is in different families of the languages used to learn the hidden layers.},
  eventtitle = {{{ICASSP}} 2013 - 2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-0356-6},
  langid = {english},
  file = {/home/pccady/Zotero/storage/3HFV9XZK/Huang et al. - 2013 - Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers.pdf}
}

@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2025-04-04},
  abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/KJZLBLXH/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf}
}

@inproceedings{huNewDNNbasedHigh2013,
  title = {A New {{DNN-based}} High Quality Pronunciation Evaluation for Computer-Aided Language Learning ({{CALL}})},
  booktitle = {Interspeech 2013},
  author = {Hu, Wenping and Qian, Yao and Soong, Frank K.},
  date = {2013-08-25},
  pages = {1886--1890},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2013-458},
  url = {https://www.isca-archive.org/interspeech_2013/hu13_interspeech.html},
  urldate = {2025-04-04},
  abstract = {In this paper, we propose to use Deep Neural Net (DNN), which has been recently shown to reduce speech recognition errors significantly, in Computer-Aided Language Learning (CALL) to evaluate English learners’ pronunciations. Multi-layer, stacked Restricted Boltzman Machines (RBMs), are first trained as nonlinear basis functions to represent speech signals succinctly, and the output layer is discriminatively trained to optimize the posterior probabilities of correct, sub-phonemic “senone” states. Three Goodness of Pronunciation (GOP) scores, including: the likelihood-based posterior probability, averaged framelevel posteriors of the DNN output layer “senone” nodes, and log likelihood ratio of correct and competing models, are tested with recordings of both native and non-native speakers, along with manual grading of pronunciation quality. The experimental results show that the GOP estimated by averaged frame-level posteriors of “senones” correlate with human scores the best. Comparing with GOPs estimated with non-DNN, i.e. GMMHMM, based models, the new approach can improve the correlations relatively by 22.0\% or 15.6\%, at word or sentence levels, respectively. In addition, the frame-level posteriors, which doesn’t need a decoding lattice and its corresponding forwardbackward computations, is suitable for supporting fast, on-line, multi-channel applications.},
  eventtitle = {Interspeech 2013},
  langid = {english},
  file = {/home/pccady/Zotero/storage/42R6MEDS/Hu et al. - 2013 - A new DNN-based high quality pronunciation evaluation for computer-aided language learning (CALL).pdf}
}

@inproceedings{islamExploringSpeechRepresentations2023,
  title = {Exploring {{Speech Representations}} for {{Proficiency Assessment}} in {{Language Learning}}},
  booktitle = {9th {{Workshop}} on {{Speech}} and {{Language Technology}} in {{Education}} ({{SLaTE}})},
  author = {Islam, Elaf and Park, Chanho and Hain, Thomas},
  date = {2023-08-18},
  pages = {151--155},
  publisher = {ISCA},
  doi = {10.21437/SLaTE.2023-29},
  url = {https://www.isca-archive.org/slate_2023/islam23_slate.html},
  urldate = {2025-04-04},
  abstract = {Automatic proficiency assessment can be a useful tool in language learning, for self-evaluation of language skills and to enable educators to tailor instruction effectively. Often assessment methods use categorisation approaches. In this paper an exemplar based approach is chosen, and comparisons between utterances are made using different speech encodings. Such an approach has the advantage to avoid formal categorisation of errors by experts. Aside from a standard spectral representation pretrained model embeddings are investigated for the usefulness for this task. Experiments are conducted using speechocean762 database, which provides 3 levels of proficiency. Data was clustered and performance of different representations is assessed in terms of cluster purity as well as categorisation correctness. Cosine distance with Whisper representations yielded better clustering performance.},
  eventtitle = {9th {{Workshop}} on {{Speech}} and {{Language Technology}} in {{Education}} ({{SLaTE}})},
  langid = {english},
  file = {/home/pccady/Zotero/storage/5SWSP8ZI/Islam et al. - 2023 - Exploring Speech Representations for Proficiency Assessment in Language Learning.pdf}
}

@article{jalalvandAutomaticQualityEstimation2018,
  title = {Automatic {{Quality Estimation}} for {{ASR System Combination}}},
  author = {Jalalvand, Shahab and Negri, Matteo and Falavigna, Daniele and Matassoni, Marco and Turchi, Marco},
  date = {2018-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {47},
  eprint = {1706.07238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {214--239},
  issn = {08852308},
  doi = {10.1016/j.csl.2017.06.003},
  url = {http://arxiv.org/abs/1706.07238},
  urldate = {2025-06-16},
  abstract = {Recognizer Output Voting Error Reduction (ROVER) has been widely used for system combination in automatic speech recognition (ASR). In order to select the most appropriate words to insert at each position in the output transcriptions, some ROVER extensions rely on critical information such as confidence scores and other ASR decoder features. This information, which is not always available, highly depends on the decoding process and sometimes tends to overestimate the real quality of the recognized words. In this paper we propose a novel variant of ROVER that takes advantage of ASR quality estimation (QE) for ranking the transcriptions at “segment level” instead of: i) relying on confidence scores, or ii) feeding ROVER with randomly ordered hypotheses. We first introduce an effective set of features to compensate for the absence of ASR decoder information. Then, we apply QE techniques to perform accurate hypothesis ranking at segment-level before starting the fusion process. The evaluation is carried out on two different tasks, in which we respectively combine hypotheses coming from independent ASR systems and multi-microphone recordings. In both tasks, it is assumed that the ASR decoder information is not available. The proposed approach significantly outperforms standard ROVER and it is competitive with two strong oracles that exploit prior knowledge about the real quality of the hypotheses to be combined. Compared to standard ROVER, the absolute WER improvements in the two evaluation scenarios range from 0.5\% to 7.3\%.},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/T4J62QNS/Jalalvand et al. - 2018 - Automatic Quality Estimation for ASR System Combination.pdf}
}

@online{joshiStateFateLinguistic2021,
  title = {The {{State}} and {{Fate}} of {{Linguistic Diversity}} and {{Inclusion}} in the {{NLP World}}},
  author = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  date = {2021-01-27},
  eprint = {2004.09095},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.09095},
  url = {http://arxiv.org/abs/2004.09095},
  urldate = {2025-04-04},
  abstract = {Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/LSSBQHM5/Joshi et al. - 2021 - The State and Fate of Linguistic Diversity and Inclusion in the NLP World.pdf}
}

@book{jurafskySpeechLanguageProcessing2025,
  title = {Speech and {{Language Processing}}: {{An Introduction}} to {{Natural Language Processing}}, {{Computational Linguistics}}, and {{Speech Recognition}}, with {{Language Models}}},
  author = {Jurafsky, Dan and Martin, James H.},
  date = {2025},
  edition = {3rd},
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  file = {/home/pccady/Zotero/storage/U3H5C8P2/ed3book_aug25.pdf}
}

@inproceedings{khareLowResourceASR2021,
  title = {Low {{Resource ASR}}: {{The Surprising Effectiveness}} of {{High Resource Transliteration}}},
  shorttitle = {Low {{Resource ASR}}},
  booktitle = {Interspeech 2021},
  author = {Khare, Shreya and Mittal, Ashish and Diwan, Anuj and Sarawagi, Sunita and Jyothi, Preethi and Bharadwaj, Samarth},
  date = {2021-08-30},
  pages = {1529--1533},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2021-2062},
  url = {https://www.isca-archive.org/interspeech_2021/khare21_interspeech.html},
  urldate = {2025-08-28},
  abstract = {Cross-lingual transfer of knowledge from high-resource languages to low-resource languages is an important research problem in automatic speech recognition (ASR). We propose a new strategy of transfer learning by pretraining using large amounts of speech in the high-resource language but with its text transliterated to the target low-resource language. This simple mapping of scripts explicitly encourages increased sharing between the output spaces of both languages and is surprisingly effective even when the high-resource and low-resource languages are from unrelated language families. The utility of our proposed technique is more evident in very low-resource scenarios, where better initializations are more beneficial. We evaluate our technique on a transformer ASR architecture and the state-ofthe-art wav2vec2.0 ASR architecture, with English as the highresource language and six languages as low-resource targets. With access to 1 hour of target speech, we obtain relative WER reductions of up to 8.2\% compared to existing transfer-learning approaches.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/home/pccady/Zotero/storage/Y6S48AQ3/Khare et al. - 2021 - Low Resource ASR The Surprising Effectiveness of High Resource Transliteration.pdf}
}

@inproceedings{khareLowResourceASR2021a,
  title = {Low {{Resource ASR}}: {{The Surprising Effectiveness}} of {{High Resource Transliteration}}},
  shorttitle = {Low {{Resource ASR}}},
  booktitle = {Interspeech 2021},
  author = {Khare, Shreya and Mittal, Ashish and Diwan, Anuj and Sarawagi, Sunita and Jyothi, Preethi and Bharadwaj, Samarth},
  date = {2021-08-30},
  pages = {1529--1533},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2021-2062},
  url = {https://www.isca-archive.org/interspeech_2021/khare21_interspeech.html},
  urldate = {2025-09-17},
  abstract = {Cross-lingual transfer of knowledge from high-resource languages to low-resource languages is an important research problem in automatic speech recognition (ASR). We propose a new strategy of transfer learning by pretraining using large amounts of speech in the high-resource language but with its text transliterated to the target low-resource language. This simple mapping of scripts explicitly encourages increased sharing between the output spaces of both languages and is surprisingly effective even when the high-resource and low-resource languages are from unrelated language families. The utility of our proposed technique is more evident in very low-resource scenarios, where better initializations are more beneficial. We evaluate our technique on a transformer ASR architecture and the state-ofthe-art wav2vec2.0 ASR architecture, with English as the highresource language and six languages as low-resource targets. With access to 1 hour of target speech, we obtain relative WER reductions of up to 8.2\% compared to existing transfer-learning approaches.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/home/pccady/Zotero/storage/8PB5RI2F/Khare et al. - 2021 - Low Resource ASR The Surprising Effectiveness of High Resource Transliteration.pdf}
}

@online{kheirAutomaticPronunciationAssessment2023,
  title = {Automatic {{Pronunciation Assessment}} -- {{A Review}}},
  author = {Kheir, Yassine El and Ali, Ahmed and Chowdhury, Shammur Absar},
  date = {2023-10-21},
  eprint = {2310.13974},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.13974},
  url = {http://arxiv.org/abs/2310.13974},
  urldate = {2025-04-04},
  abstract = {Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available resources. This is followed by a discussion of the remaining challenges and possible directions for future work.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/8VY2N9BF/Kheir et al. - 2023 - Automatic Pronunciation Assessment -- A Review.pdf}
}

@online{kheirL1awareMultilingualMispronunciation2023,
  title = {L1-Aware {{Multilingual Mispronunciation Detection Framework}}},
  author = {Kheir, Yassine El and Chowdhury, Shammur Absar and Ali, Ahmed},
  date = {2023-09-21},
  eprint = {2309.07719},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.07719},
  url = {http://arxiv.org/abs/2309.07719},
  urldate = {2025-04-04},
  abstract = {The phonological discrepancies between a speaker’s native (L1) and the non-native language (L2) serves as a major factor for mispronunciation. This paper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched with L1aware speech representation. An end-to-end speech encoder is trained on the input signal and its corresponding reference phoneme sequence. First, an attention mechanism is deployed to align the input audio with the reference phoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an auxiliary model, pretrained in a multi-task setup identifying L1 and L2 language, and are infused with the primary network. Finally, the L1-MultiMDD is then optimized for a unified multilingual phoneme recognition task using connectionist temporal classification (CTC) loss for the target languages: English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of the proposed L1-MultiMDD framework on both seen – L2-ARTIC, LATIC, and AraVoiceL2v2; and unseen – EpaDB and Speechocean762 datasets. The consistent gains in PER, and false rejection rate (FRR) across all target languages confirm our approach’s robustness, efficacy, and generalizability.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/JYTSEG3W/Kheir et al. - 2023 - L1-aware Multilingual Mispronunciation Detection Framework.pdf}
}

@article{kheirMispronunciationDetectionSpeechBlender,
  title = {Mispronunciation {{Detection}} with {{SpeechBlender Data Augmentation Pipeline}}.},
  author = {Kheir, Yassine EL},
  langid = {english},
  file = {/home/pccady/Zotero/storage/AERWL4YF/Kheir - Mispronunciation Detection with SpeechBlender Data Augmentation Pipeline..pdf}
}

@online{kimAutomaticPronunciationAssessment2022,
  title = {Automatic {{Pronunciation Assessment}} Using {{Self-Supervised Speech Representation Learning}}},
  author = {Kim, Eesung and Jeon, Jae-Jin and Seo, Hyeji and Kim, Hoon},
  date = {2022-04-08},
  eprint = {2204.03863},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2204.03863},
  url = {http://arxiv.org/abs/2204.03863},
  urldate = {2025-04-04},
  abstract = {Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT models have shown promising results in various downstream tasks in the speech community. In particular, speech representations learned by SSL models have been shown to be effective for encoding various speech-related characteristics. In this context, we propose a novel automatic pronunciation assessment method based on SSL models. First, the proposed method fine-tunes the pre-trained SSL models with connectionist temporal classification to adapt the English pronunciation of English-as-a-second-language (ESL) learners in a data environment. Then, the layer-wise contextual representations are extracted from all across the transformer layers of the SSL models. Finally, the automatic pronunciation score is estimated using bidirectional long short-term memory with the layer-wise contextual representations and the corresponding text. We show that the proposed SSL model-based methods outperform the baselines, in terms of the Pearson correlation coefficient, on datasets of Korean ESL learner children and Speechocean762. Furthermore, we analyze how different representations of transformer layers in the SSL model affect the performance of the pronunciation assessment task.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/BMDC8LPB/Kim et al. - 2022 - Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning.pdf}
}

@online{koehnSixChallengesNeural2017,
  title = {Six {{Challenges}} for {{Neural Machine Translation}}},
  author = {Koehn, Philipp and Knowles, Rebecca},
  date = {2017-06-12},
  eprint = {1706.03872},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03872},
  url = {http://arxiv.org/abs/1706.03872},
  urldate = {2025-07-14},
  abstract = {We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/UHGU98NQ/Koehn and Knowles - 2017 - Six Challenges for Neural Machine Translation.pdf;/home/pccady/Zotero/storage/LLFF3CI2/1706.html}
}

@article{korzekwaComputerassistedPronunciationTraining2022,
  title = {Computer-Assisted Pronunciation Training—{{Speech}} Synthesis Is Almost All You Need},
  author = {Korzekwa, Daniel and Lorenzo-Trueba, Jaime and Drugman, Thomas and Kostek, Bozena},
  date = {2022-07},
  journaltitle = {Speech Communication},
  volume = {142},
  pages = {22--33},
  publisher = {Elsevier BV},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2022.06.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639322000863},
  urldate = {2025-07-18},
  langid = {english},
  file = {/home/pccady/Zotero/storage/QCIRJ73S/Korzekwa et al. - 2022 - Computer-assisted pronunciation training—Speech synthesis is almost all you need.pdf}
}

@online{korzekwaWeaklysupervisedWordlevelPronunciation2021,
  title = {Weakly-Supervised Word-Level Pronunciation Error Detection in Non-Native {{English}} Speech},
  author = {Korzekwa, Daniel and Lorenzo-Trueba, Jaime and Drugman, Thomas and Calamaro, Shira and Kostek, Bozena},
  date = {2021-06-07},
  eprint = {2106.03494},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2106.03494},
  url = {http://arxiv.org/abs/2106.03494},
  urldate = {2025-09-25},
  abstract = {We propose a weakly-supervised model for word-level mispronunciation detection in non-native (L2) English speech. To train this model, phonetically transcribed L2 speech is not required and we only need to mark mispronounced words. The lack of phonetic transcriptions for L2 speech means that the model has to learn only from a weak signal of word-level mispronunciations. Because of that and due to the limited amount of mispronounced L2 speech, the model is more likely to overfit. To limit this risk, we train it in a multi-task setup. In the first task, we estimate the probabilities of word-level mispronunciation. For the second task, we use a phoneme recognizer trained on phonetically transcribed L1 speech that is easily accessible and can be automatically annotated. Compared to state-of-the-art approaches, we improve the accuracy of detecting word-level pronunciation errors in AUC metric by 30\% on the GUT Isle Corpus of L2 Polish speakers, and by 21.5\% on the Isle Corpus of L2 German and Italian speakers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/83ICW8R2/Korzekwa et al. - 2021 - Weakly-supervised word-level pronunciation error detection in non-native English speech.pdf;/home/pccady/Zotero/storage/6IADSWLV/2106.html}
}

@book{krashenPrinciplesPracticeSecond1984,
  title = {Principles and Practice in Second Language Acquisition},
  author = {Krashen, Stephen D.},
  date = {1984},
  series = {Language Teaching Methodology Series},
  edition = {Reprinted},
  publisher = {Pergamon Press},
  location = {Oxford},
  isbn = {978-0-08-028628-0},
  langid = {english},
  pagetotal = {202},
  file = {/home/pccady/Zotero/storage/FSTYTSP2/Krashen - 1984 - Principles and practice in second language acquisition.pdf}
}

@article{krishenbaumRepresentingIPAPhonetics,
  title = {Representing {{IPA Phonetics}} in {{ASCII}}},
  author = {Krishenbaum, Evan},
  langid = {english},
  file = {/home/pccady/Zotero/storage/D8A9W6JI/Krishenbaum - Representing IPA Phonetics in ASCII.pdf}
}

@incollection{kurzingerCTCSegmentationLargeCorpora2020,
  title = {{{CTC-Segmentation}} of {{Large Corpora}} for {{German End-to-end Speech Recognition}}},
  author = {Kürzinger, Ludwig and Winkelbauer, Dominik and Li, Lujun and Watzel, Tobias and Rigoll, Gerhard},
  date = {2020},
  volume = {12335},
  eprint = {2007.09127},
  eprinttype = {arXiv},
  eprintclass = {eess},
  pages = {267--278},
  doi = {10.1007/978-3-030-60276-5_27},
  url = {http://arxiv.org/abs/2007.09127},
  urldate = {2025-04-04},
  abstract = {Recent end-to-end Automatic Speech Recognition (ASR) systems demonstrated the ability to outperform conventional hybrid DNN/ HMM ASR. Aside from architectural improvements in those systems, those models grew in terms of depth, parameters and model capacity. However, these models also require more training data to achieve comparable performance.},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/RGBGBXLM/Kürzinger et al. - 2020 - CTC-Segmentation of Large Corpora for German End-to-end Speech Recognition.pdf}
}

@inproceedings{kyriakopoulosDeepLearningApproach2018,
  title = {A {{Deep Learning Approach}} to {{Assessing Non-native Pronunciation}} of {{English Using Phone Distances}}},
  booktitle = {Interspeech 2018},
  author = {Kyriakopoulos, Konstantinos and Knill, Kate and Gales, Mark},
  date = {2018-09-02},
  pages = {1626--1630},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2018-1087},
  url = {https://www.isca-archive.org/interspeech_2018/kyriakopoulos18_interspeech.html},
  urldate = {2025-04-04},
  abstract = {The way a non-native speaker pronounces the phones of a language is an important predictor of their proficiency. In grading spontaneous speech, the pairwise distances between generative statistical models trained on each phone have been shown to be powerful features. This paper presents a deep learning alternative to model-based phone distances in the form of a tunable Siamese network feature extractor to extract distance metrics directly from the audio frame sequence. Features are extracted at the phone instance level and combined to phone-level representations using an attention mechanism. Pair-wise distances between phone features are then projected through a feed-forward layer to predict score. The extraction stage is initialised on either a binary phone instance-pair classification task, or to mimic the model-based features, then the whole system is fine-tuned end-to-end, optimising the learning of the distance metric to the score prediction task. This method is therefore more adaptable and more sensitive to phone instance level phenomena. Its performance is compared against a DNN trained on Gaussian phone model distance features.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/home/pccady/Zotero/storage/8835LD4X/Kyriakopoulos et al. - 2018 - A Deep Learning Approach to Assessing Non-native Pronunciation of English Using Phone Distances.pdf}
}

@inproceedings{laptevFastEntropyBasedMethods2023,
  title = {Fast {{Entropy-Based Methods}} of {{Word-Level Confidence Estimation}} for {{End-To-End Automatic Speech Recognition}}},
  booktitle = {2022 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Laptev, Aleksandr and Ginsburg, Boris},
  date = {2023-01-09},
  eprint = {2212.08703},
  eprinttype = {arXiv},
  eprintclass = {eess},
  pages = {152--159},
  doi = {10.1109/SLT54892.2023.10022960},
  url = {http://arxiv.org/abs/2212.08703},
  urldate = {2025-09-14},
  abstract = {This paper presents a class of new fast non-trainable entropy-based confidence estimation methods for automatic speech recognition. We show how per-frame entropy values can be normalized and aggregated to obtain a confidence measure per unit and per word for Connectionist Temporal Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) models. Proposed methods have similar computational complexity to the traditional method based on the maximum per-frame probability, but they are more adjustable, have a wider effective threshold range, and better push apart the confidence distributions of correct and incorrect words. We evaluate the proposed confidence measures on LibriSpeech test sets, and show that they are up to 2 and 4 times better than confidence estimation based on the maximum per-frame probability at detecting incorrect words for Conformer-CTC and Conformer-RNN-T models, respectively.},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Mathematics - Information Theory},
  file = {/home/pccady/Zotero/storage/ZZLL4CDB/Laptev and Ginsburg - 2023 - Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-To-End Automatic Speech Recog.pdf;/home/pccady/Zotero/storage/PQNVQW7J/2212.html}
}

@article{leeMassivelyMultilingualPronunciation,
  title = {Massively {{Multilingual Pronunciation Mining}} with {{WikiPron}}},
  author = {Lee, Jackson L and Ashby, Lucas F E and Garza, M Elizabeth and Lee-Sikka, Yeonju and Miller, Sean and Wong, Alan and McCarthy, Arya D and Gorman, Kyle},
  abstract = {We introduce WikiPron, an open-source command-line tool for extracting pronunciation data from Wiktionary, a collaborative multilingual online dictionary. We first describe the design and use of WikiPron. We then discuss the challenges faced scaling this tool to create an automatically-generated database of 1.7 million pronunciations from 165 languages. Finally, we validate the pronunciation database by using it to train and evaluating a collection of generic grapheme-to-phoneme models. The software, pronunciation data, and models are all made available under permissive open-source licenses.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/H79IA2SE/Lee et al. - Massively Multilingual Pronunciation Mining with WikiPron.pdf}
}

@inproceedings{leungCNNRNNCTCBasedEndtoend2019,
  title = {{{CNN-RNN-CTC Based End-to-end Mispronunciation Detection}} and {{Diagnosis}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Leung, Wai-Kim and Liu, Xunying and Meng, Helen},
  date = {2019-05},
  pages = {8132--8136},
  publisher = {IEEE},
  location = {Brighton, United Kingdom},
  doi = {10.1109/ICASSP.2019.8682654},
  url = {https://ieeexplore.ieee.org/document/8682654/},
  urldate = {2025-09-12},
  abstract = {This paper focuses on using Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Connectionist Temporal Classification (CTC) to build an end-to-end speech recognition for Mispronunciation Detection and Diagnosis (MDD) task. Our approach is end-to-end models, while phonemic or graphemic information, or forced alignment between different linguistic units, are not required. We conduct experiments that compare the proposed CNN-RNNCTC approach with alternative mispronunciation detection and diagnoses (MDD) approaches. The F-measure of our approach is 74.65\%, which significantly outperforms the Extended Recognition Network (ERN) (S-AM) by 44.75\% and State-level Acoustic Model (S-AM) by 32.28\% relatively. The relative improvement in F-measure when over Acoustic-Phonemic Model (APM), Acoustic-Graphemic Model (AGM) and Acoustic-Phonemic-Graphemic Model (APGM) are 9.57\%, 5.04\% and 2.77\% respectively.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-8131-1},
  langid = {english},
  file = {/home/pccady/Zotero/storage/8TPKZ7SE/Leung et al. - 2019 - CNN-RNN-CTC Based End-to-end Mispronunciation Detection and Diagnosis.pdf}
}

@article{levisCOMPUTERTECHNOLOGYTEACHING2007,
  title = {{{COMPUTER TECHNOLOGY IN TEACHING AND RESEARCHING PRONUNCIATION}}},
  author = {Levis, John},
  date = {2007-03},
  journaltitle = {Annual Review of Applied Linguistics},
  shortjournal = {APL},
  volume = {27},
  issn = {0267-1905, 1471-6356},
  doi = {10.1017/S0267190508070098},
  url = {http://www.journals.cambridge.org/abstract_S0267190508070098},
  urldate = {2025-04-04},
  langid = {english},
  file = {/home/pccady/Zotero/storage/BDRESYKU/Levis - 2007 - COMPUTER TECHNOLOGY IN TEACHING AND RESEARCHING PRONUNCIATION.pdf}
}

@article{levisTeachingIntonationDiscourse2004,
  title = {Teaching Intonation in Discourse Using Speech Visualization Technology},
  author = {Levis, John and Pickering, Lucy},
  date = {2004-12},
  journaltitle = {System},
  shortjournal = {System},
  volume = {32},
  number = {4},
  pages = {505--524},
  issn = {0346251X},
  doi = {10.1016/j.system.2004.09.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0346251X04000752},
  urldate = {2025-04-04},
  abstract = {Intonation, long thought to be a key to effectiveness in spoken language, is more and more commonly addressed in English language teaching through the use of speech visualization technology. While the use of visualization technology is a crucial advance in the teaching of intonation, such teaching can be further enhanced by connecting technology to an understanding of how intonation functions in discourse. This study examines the intonation of four readers reading out-of-context sentences and then the same sentences as part of coherent discourse-level texts. Two discourse-level uses of intonation, the use of intonational paragraph markers (paratones) and the distribution of tonal patterns, are discussed and implications for teaching intonation are addressed.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/YYWGVD4Q/Levis and Pickering - 2004 - Teaching intonation in discourse using speech visualization technology.pdf}
}

@article{liMispronunciationDetectionDiagnosis2017,
  title = {Mispronunciation {{Detection}} and {{Diagnosis}} in {{L2 English Speech Using Multidistribution Deep Neural Networks}}},
  author = {Li, Kun and Qian, Xiaojun and Meng, Helen},
  date = {2017-01},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {25},
  number = {1},
  pages = {193--207},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2016.2621675},
  url = {http://ieeexplore.ieee.org/document/7752846/},
  urldate = {2025-04-04},
  abstract = {This paper investigates the use of multidistribution deep neural networks (DNNs) for mispronunciation detection and diagnosis (MDD), to circumvent the difficulties encountered in an existing approach based on extended recognition networks (ERNs). The ERNs leverage existing automatic speech recognition technology by constraining the search space via including the likely phonetic error patterns of the target words in addition to the canonical transcriptions. MDDs are achieved by comparing the recognized transcriptions with the canonical ones. Although this approach performs reasonably well, it has the following issues: 1) Learning the error patterns of the target words to generate the ERNs remains a challenging task. Phones or phone errors missing from the ERNs cannot be recognized even if we have well-trained acoustic models; and 2) acoustic models and phonological rules are trained independently, and hence, contextual information is lost. To address these issues, we propose an acoustic-graphemicphonemic model (AGPM) using a multidistribution DNN, whose input features include acoustic features, as well as corresponding graphemes and canonical transcriptions (encoded as binary vectors). The AGPM can implicitly model both grapheme-to-likelypronunciation and phoneme-to-likely-pronunciation conversions, which are integrated into acoustic modeling. With the AGPM, we develop a unified MDD framework, which works much like freephone recognition. Experiments show that our method achieves a phone error rate (PER) of 11.1\%. The false rejection rate (FRR), false acceptance rate (FAR), and diagnostic error rate (DER) for MDD are 4.6\%, 30.5\%, and 13.5\%, respectively. It outperforms the ERN approach using DNNs as acoustic models, whose PER, FRR, FAR, and DER are 16.8\%, 11.0\%, 43.6\%, and 32.3\%, respectively.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/C4PALEVG/Li et al. - 2017 - Mispronunciation Detection and Diagnosis in L2 English Speech Using Multidistribution Deep Neural Ne.pdf}
}

@inproceedings{liPhonemeLevelArticulatorDynamics2011,
  title = {The {{Phoneme-Level Articulator Dynamics}} for {{Pronunciation Animation}}},
  booktitle = {2011 {{International Conference}} on {{Asian Language Processing}}},
  author = {Li, Sheng and Wang, Lan and Qi, En},
  date = {2011-11},
  pages = {283--286},
  publisher = {IEEE},
  location = {Penang, Malaysia},
  doi = {10.1109/IALP.2011.13},
  url = {http://ieeexplore.ieee.org/document/6121521/},
  urldate = {2025-04-04},
  abstract = {Speech visualization can be extended to a task of pronunciation animation for language learners. In this paper, a three dimensional English articulation database is recorded using Carstens Electro-Magnetic Articulograph (EMA AG500). A HMM-based visual synthesis method for continuous speech is implemented to recover some important articulatory information. The synthesized articulations are then compared to the EMA recordings for objective evaluation. With a data-driven 3D talking head, the distinctions between the confusable phonemes can be depicted through both external and internal articulatory movements. The experiments demonstrated that an EMA data driven 3D talking head, incorporated with HMMbased visual synthesis technique, is quite practical for computer-assisted second language pronunciation training.},
  eventtitle = {2011 {{International Conference}} on {{Asian Language Processing}} ({{IALP}})},
  isbn = {978-1-4577-1733-8},
  langid = {english},
  file = {/home/pccady/Zotero/storage/B86KFAGB/Li et al. - 2011 - The Phoneme-Level Articulator Dynamics for Pronunciation Animation.pdf}
}

@inproceedings{liuEndtoEndUnsupervisedSpeech2023,
  title = {Towards {{End-to-End Unsupervised Speech Recognition}}},
  booktitle = {2022 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Liu, Alexander H. and Hsu, Wei-Ning and Auli, Michael and Baevski, Alexei},
  date = {2023-01-09},
  pages = {221--228},
  publisher = {IEEE},
  location = {Doha, Qatar},
  doi = {10.1109/SLT54892.2023.10023187},
  url = {https://ieeexplore.ieee.org/document/10023187/},
  urldate = {2025-04-04},
  abstract = {Unsupervised speech recognition has shown great potential to make Automatic Speech Recognition (ASR) systems accessible to every language. However, existing methods still heavily rely on hand-crafted pre-processing. Similar to the trend of making supervised speech recognition end-to-end, we introduce wav2vec-U 2.0 which does away with all audio-side preprocessing and improves accuracy through better architecture. In addition, we introduce an auxiliary self-supervised objective that ties model predictions back to the input. Experiments show that wav2vec-U 2.0 improves unsupervised recognition results across different languages while being conceptually simpler.},
  eventtitle = {2022 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  isbn = {979-8-3503-9690-4},
  langid = {english},
  file = {/home/pccady/Zotero/storage/L94ZBXD8/Liu et al. - 2023 - Towards End-to-End Unsupervised Speech Recognition.pdf}
}

@online{loEffectiveEndtoEndModeling2020,
  title = {An {{Effective End-to-End Modeling Approach}} for {{Mispronunciation Detection}}},
  author = {Lo, Tien-Hong and Weng, Shi-Yan and Chang, Hsiu-Jui and Chen, Berlin},
  date = {2020-05-18},
  eprint = {2005.08440},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2005.08440},
  url = {http://arxiv.org/abs/2005.08440},
  urldate = {2025-08-30},
  abstract = {Recently, end-to-end (E2E) automatic speech recognition (ASR) systems have garnered tremendous attention because of their great success and unified modeling paradigms in comparison to conventional hybrid DNN-HMM ASR systems. Despite the widespread adoption of E2E modeling frameworks on ASR, there still is a dearth of work on investigating the E2E frameworks for use in computer-assisted pronunciation learning (CAPT), particularly for Mispronunciation detection (MD). In response, we first present a novel use of hybrid CTCAttention approach to the MD task, taking advantage of the strengths of both CTC and the attention-based model meanwhile getting around the need for phone-level forced alignment. Second, we perform input augmentation with text prompt information to make the resulting E2E model more tailored for the MD task. On the other hand, we adopt two MD decision methods so as to better cooperate with the proposed framework: 1) decision-making based on a recognition confidence measure or 2) simply based on speech recognition results. A series of Mandarin MD experiments demonstrate that our approach not only simplifies the processing pipeline of existing hybrid DNN-HMM systems but also brings about systematic and substantial performance improvements. Furthermore, input augmentation with text prompts seems to hold excellent promise for the E2E-based MD approach.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/WDQ3H9TX/Lo et al. - 2020 - An Effective End-to-End Modeling Approach for Mispronunciation Detection.pdf;/home/pccady/Zotero/storage/9DIA6Y8K/2005.html}
}

@article{lonerganAutomaticSpeechRecognition,
  title = {Automatic {{Speech Recognition}} for {{Irish}}: The {{ABAIR-ÉIST System}}},
  author = {Lonergan, Liam and Qian, Mengjie and Berthelsen, Harald and Murphy, Andy and Wendler, Christoph and Chiaráin, Neasa Ní and Gobl, Christer and Chasaide, Ailbhe Ní},
  abstract = {This paper describes ÉIST, automatic speech recogniser for Irish, developed as part of the ongoing ABAIR initiative, combining (1) acoustic models, (2) pronunciation lexicons and (3) language models into a hybrid system. A priority for now is a system that can deal with the multiple diverse native-speaker dialects. Consequently, (1) was built using predominately native-speaker speech, which included earlier recordings used for synthesis development as well as more diverse recordings obtained using the MíleGlór platform. The pronunciation variation across the dialects is a particular challenge in the development of (2) and is explored by testing both Transdialect and Multi-dialect letter-to-sound rules. Two approaches to language modelling (3) are used in the hybrid system, a simple n-gram model and recurrent neural network lattice rescoring, the latter garnering impressive performance improvements. The system is evaluated using a test set that is comprised of both native and non-native speakers, which allows for some inferences to be made on the performance of the system on both cohorts.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/R2PXEAQD/Lonergan et al. - Automatic Speech Recognition for Irish the ABAIR-ÉIST System.pdf}
}

@online{lonerganLowresourceSpeechRecognition2024,
  title = {Low-Resource Speech Recognition and Dialect Identification of {{Irish}} in a Multi-Task Framework},
  author = {Lonergan, Liam and Qian, Mengjie and Chiaráin, Neasa Ní and Gobl, Christer and Chasaide, Ailbhe Ní},
  date = {2024-05-02},
  eprint = {2405.01293},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.01293},
  url = {http://arxiv.org/abs/2405.01293},
  urldate = {2025-04-04},
  abstract = {This paper explores the use of Hybrid CTC/Attention encoderdecoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID). Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN). An optimal InterCTC setting is initially established using a Conformer encoder. This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared. A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion. The experiments yielded an improvement in DID accuracy of 10.8\% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/7DGRSRXD/Lonergan et al. - 2024 - Low-resource speech recognition and dialect identification of Irish in a multi-task framework.pdf}
}

@article{lysterOralCorrectiveFeedback2013,
  title = {Oral Corrective Feedback in Second Language Classrooms},
  author = {Lyster, Roy and Saito, Kazuya and Sato, Masatoshi},
  date = {2013-01},
  journaltitle = {Language Teaching},
  shortjournal = {Lang. Teach.},
  volume = {46},
  number = {1},
  pages = {1--40},
  issn = {0261-4448, 1475-3049},
  doi = {10.1017/S0261444812000365},
  url = {https://www.cambridge.org/core/product/identifier/S0261444812000365/type/journal_article},
  urldate = {2025-04-04},
  abstract = {This article reviews research on oral corrective feedback (CF) in second language (L2) classrooms. Various types of oral CF are first identified, and the results of research revealing CF frequency across instructional contexts are presented. Research on CF preferences is then reviewed, revealing a tendency for learners to prefer receiving CF more than teachers feel they should provide it. Next, theoretical perspectives in support of CF are presented and some contentious issues addressed related to the role of learner uptake, the role of instruction, and the overall purpose of CF: to initiate the acquisition of new knowledge or to consolidate already acquired knowledge. A brief review of laboratory studies assessing the effects of recasts is then presented before we focus on classroom studies assessing the effects of different types of CF. Many variables mediate CF effectiveness: of these, we discuss linguistic targets and learners' age in terms of both previous and prospective research. Finally, CF provided by learners and the potential benefits of strategy training for strengthening the role of CF during peer interaction are highlighted.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/7PVZ2Q5Z/Lyster et al. - 2013 - Oral corrective feedback in second language classrooms.pdf}
}

@thesis{mac2023learning,
  type = {phdthesis},
  title = {Learning to Express, Learning as Self-Expression: A Multimethod Investigation of the {{L2}} Selves of Distance Adult {{Irish L2}} Learners},
  author = {Mac Lochlainn, Conchúr},
  date = {2023},
  institution = {Dublin City University},
  file = {/home/pccady/Zotero/storage/LMHK7Y9J/Mac Lochlainn - 2023 - Learning to express, learning as self-expression a multimethod investigation of the L2 selves of di.pdf}
}

@article{maclinEmpiricalEvaluationBagging1997,
  title = {An Empirical Evaluation of Bagging and Boosting},
  author = {Maclin, Richard and Opitz, David},
  date = {1997},
  journaltitle = {AAAI/IAAI},
  volume = {1997},
  pages = {546--551},
  url = {https://d1wqtxts1xzle7.cloudfront.net/78173773/An_Empirical_Evaluation_of_Bagging_and_B20220105-9764-16y0t8b.pdf?1738464338=&response-content-disposition=inline%3B+filename%3DAn_empirical_evaluation_of_bagging_and_b.pdf&Expires=1758100511&Signature=ULr42EX~oPCHa6soKPN0avlV0aJeaV5Pq96lNHezXzPN3CaB2CYdPJi1atYw3dXBQF04naJPYSQicHHyAQrqLNgXwSF8xAx6EFU4g~CBnmOa5vnYQeQJTrcJlOvLUwbAHW0iZf8T8lNgeP-c~aImdq1OYrzWsEe2HQ6e-q77emb2VM9p~xsFxMKAn~5RU2w6d~x3iQ4n-YVaAeUbPdAtWFlysMTlSow56C4xK4KupsOSUA3qlXeq9PTBaWskjFVCcSAlfJpm06hDOLrZtwO3O821qdsrh1HsXxoTTWa2Qbh1~Ra-ZHwXQt-o2~XKDva8-HeQxOX7x~6FrBsf7HBo6A__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA},
  urldate = {2025-09-17},
  file = {/home/pccady/Zotero/storage/8ZXEJKA6/An_Empirical_Evaluation_of_Bagging_and_B20220105-9764-16y0t8b.pdf}
}

@article{maclochlainnClickingConnectingL22021,
  title = {Clicking, but Connecting? {{L2}} Learning Engagement on an Ab Initio {{Irish}} Language {{LMOOC}}},
  shorttitle = {Clicking, but Connecting?},
  author = {Mac Lochlainn, Conchúr and Nic Giolla Mhichíl, Mairéad and Beirne, Elaine},
  date = {2021-05},
  journaltitle = {ReCALL},
  shortjournal = {ReCALL},
  volume = {33},
  number = {2},
  pages = {111--127},
  issn = {0958-3440, 1474-0109},
  doi = {10.1017/S0958344021000100},
  url = {https://www.cambridge.org/core/product/identifier/S0958344021000100/type/journal_article},
  urldate = {2025-04-04},
  abstract = {Language massive open online courses (LMOOCs) represent an exciting prospect for language teachers and instructors around the globe (Bárcena \& Martín-Monje, 2014). In this paper, we report on the dynamics of participation and learner behaviour in an ab initio Irish language course. The course, Irish 101, ran during March 2019, and we used a mixed-methods approach to analyse both typical patterns of behaviour among course participants and learner reflections upon their reasons for doing so. Findings suggest that most learners use the course resources in an assessing and exploratory manner and are far less likely to produce, or to examine, second language (L2) output, either written or spoken. Learners were found to be selective and to demonstrate significant metacognitive awareness (Wenden, 1998) in their interactions and learning methods, displaying agency and exploiting affordances beyond the design of the course itself. Implications for LMOOC design, including the need to question whether courses should emphasise L2 production or resource provision, are considered, in addition to a general need for more granular, dynamic research, so as to better understand the types of learners who engage in LMOOCs and to better cater to diverse learning needs.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/RL4XRZHD/Mac Lochlainn et al. - 2021 - Clicking, but connecting L2 learning engagement on an ab initio Irish language LMOOC.pdf}
}

@online{magueresseLowresourceLanguagesReview2020,
  title = {Low-Resource {{Languages}}: {{A Review}} of {{Past Work}} and {{Future Challenges}}},
  shorttitle = {Low-Resource {{Languages}}},
  author = {Magueresse, Alexandre and Carles, Vincent and Heetderks, Evan},
  date = {2020-06-12},
  eprint = {2006.07264},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.07264},
  url = {http://arxiv.org/abs/2006.07264},
  urldate = {2025-04-04},
  abstract = {A current problem in NLP is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/CY6KQTVS/Magueresse et al. - 2020 - Low-resource Languages A Review of Past Work and Future Challenges.pdf}
}

@online{manoharWhatLostNormalization2024,
  title = {What Is Lost in {{Normalization}}? {{Exploring Pitfalls}} in {{Multilingual ASR Model Evaluations}}},
  shorttitle = {What Is Lost in {{Normalization}}?},
  author = {Manohar, Kavya and Pillai, Leena G. and Sherly, Elizabeth},
  date = {2024-11-09},
  eprint = {2409.02449},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.02449},
  url = {http://arxiv.org/abs/2409.02449},
  urldate = {2025-04-04},
  abstract = {This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta’s MMS, Seamless, and Assembly AI’s Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially improved performance metrics for Indic languages. We conclude by proposing a shift towards developing text normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/pccady/Zotero/storage/KB2Q7RZQ/Manohar et al. - 2024 - What is lost in Normalization Exploring Pitfalls in Multilingual ASR Model Evaluations.pdf}
}

@dataset{matthewdryerWorldAtlasLanguage2024,
  title = {The {{World Atlas}} of {{Language Structures Online}}},
  author = {Matthew Dryer and Martin Haspelmath and Dryer, Matthew and Haspelmath, Martin},
  namea = {Forkel, Robert},
  nameatype = {collaborator},
  date = {2024-10-18},
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.13950591},
  url = {https://zenodo.org/doi/10.5281/zenodo.13950591},
  urldate = {2025-07-08},
  abstract = {Cite the source of the dataset as: Dryer, Matthew S. \& Haspelmath, Martin (eds.) 2013. The World Atlas of Language Structures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. (Available online at https://wals.info)},
  version = {v2020.4},
  keywords = {cldf:StructureDataset,linguistics}
}

@inproceedings{mcauliffeMontrealForcedAligner2017,
  title = {Montreal {{Forced Aligner}}: {{Trainable Text-Speech Alignment Using Kaldi}}},
  shorttitle = {Montreal {{Forced Aligner}}},
  booktitle = {Interspeech 2017},
  author = {McAuliffe, Michael and Socolof, Michaela and Mihuc, Sarah and Wagner, Michael and Sonderegger, Morgan},
  date = {2017-08-20},
  pages = {498--502},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2017-1386},
  url = {https://www.isca-archive.org/interspeech_2017/mcauliffe17_interspeech.html},
  urldate = {2025-09-08},
  abstract = {We present the Montreal Forced Aligner (MFA), a new opensource system for speech-text alignment. MFA is an update to the Prosodylab-Aligner, and maintains its key functionality of trainability on new data, as well as incorporating improved architecture (triphone acoustic models and speaker adaptation), and other features. MFA uses Kaldi instead of HTK, allowing MFA to be distributed as a stand-alone package, and to exploit parallel processing for computationally-intensive training and scaling to larger datasets. We evaluate MFA’s performance on aligning word and phone boundaries in English conversational and laboratory speech, relative to human-annotated boundaries, focusing on the effects of aligner architecture and training on the data to be aligned. MFA performs well relative to two existing open-source aligners with simpler architecture (Prosodylab-Aligner and FAVE), and both its improved architecture and training on data to be aligned generally result in more accurate boundaries.},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/home/pccady/Zotero/storage/TZ5ZLLHY/McAuliffe et al. - 2017 - Montreal Forced Aligner Trainable Text-Speech Alignment Using Kaldi.pdf}
}

@article{mechuraIntroductionGramadanIrish,
  title = {Introduction to {{Gramadán}} and the {{Irish National Morphology Database}}},
  author = {Měchura, Michal Boleslav},
  langid = {english},
  file = {/home/pccady/Zotero/storage/G68B8EGJ/Měchura - Introduction to Gramadán and the Irish National Morphology Database.pdf}
}

@book{mihalicekLanguageFilesMaterials2012,
  title = {Language Files: Materials for an Introduction to Language and Linguistics},
  shorttitle = {Language Files},
  editor = {Mihalicek, Vedrana and Wilson, Christin},
  date = {2012},
  edition = {11th edition},
  publisher = {Bookman Books},
  location = {Taipei},
  isbn = {978-957-445-461-7},
  langid = {english},
  annotation = {OCLC: 897284575}
}

@book{mihalicekLanguageFilesMaterials2012a,
  title = {Language Files: Materials for an Introduction to Language and Linguistics},
  shorttitle = {Language Files},
  editor = {Mihalicek, Vedrana and Wilson, Christin},
  date = {2012},
  edition = {11th edition},
  publisher = {Bookman Books},
  location = {Taipei},
  isbn = {978-957-445-461-7},
  langid = {english},
  annotation = {OCLC: 897284575}
}

@article{mortensenPanPhonResourceMapping,
  title = {{{PanPhon}}: {{A Resource}} for {{Mapping IPA Segments}} to {{Articulatory Feature Vectors}}},
  author = {Mortensen, David R and Littell, Patrick and Bharadwaj, Akash and Goyal, Kartik and Dyer, Chris and Levin, Lori},
  abstract = {This paper contributes to a growing body of evidence that—when coupled with appropriate machine-learning techniques—linguistically motivated, information-rich representations can outperform one-hot encodings of linguistic data. In particular, we show that phonological features outperform character-based models using the PanPhon resource. PanPhon is a database relating over 5,000 IPA segments to 21 subsegmental articulatory features. We show that this database boosts performance in various NER-related tasks. Phonologically aware, neural CRF models built on PanPhon features are able to perform comparably to character-based models on monolingual Spanish and Turkish NER tasks. On transfer models (as between Uzbek and Turkish) they have been shown to perform better. Furthermore, PanPhon features also contribute measurably to Orthography-to-IPA conversion tasks.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/CZLCJ9BG/Mortensen et al. - PanPhon A Resource for Mapping IPA Segments to Articulatory Feature Vectors.pdf}
}

@misc{moseleyAtlasWorldsLanguages2010,
  title = {Atlas of the World's Languages in Danger},
  namea = {Moseley, Christopher and Nicolas, Alexandre},
  nameatype = {collaborator},
  date = {2010},
  edition = {3. ed., entirely rev., enlarged and updated},
  location = {Paris},
  isbn = {978-92-3-104096-2 978-92-3-104095-5},
  langid = {english},
  organization = {UNESCO Publishing},
  pagetotal = {62}
}

@article{needlemanGeneralMethodApplicable1970,
  title = {A General Method Applicable to the Search for Similarities in the Amino Acid Sequence of Two Proteins},
  author = {Needleman, Saul B and Wunsch, Christian D},
  date = {1970},
  journaltitle = {Journal of molecular biology},
  volume = {48},
  number = {3},
  pages = {443--453},
  file = {/home/pccady/Zotero/storage/DXM3Z5PA/needleman-wunsch-orig-dyn-prog-paper.pdf}
}

@article{neriEffectivenessComputerAssisted2008,
  title = {The Effectiveness of Computer Assisted Pronunciation Training for Foreign Language Learning by Children},
  author = {Neri, Ambra and Mich, Ornella and Gerosa, Matteo and Giuliani, Diego},
  date = {2008-12},
  journaltitle = {Computer Assisted Language Learning},
  shortjournal = {Computer Assisted Language Learning},
  volume = {21},
  number = {5},
  pages = {393--408},
  issn = {0958-8221, 1744-3210},
  doi = {10.1080/09588220802447651},
  url = {https://www.tandfonline.com/doi/full/10.1080/09588220802447651},
  urldate = {2025-04-04},
  langid = {english},
  file = {/home/pccady/Zotero/storage/6QN6VKGQ/Neri et al. - 2008 - The effectiveness of computer assisted pronunciation training for foreign language learning by child.pdf}
}

@inproceedings{nichasaideCanWeDefuse2019,
  title = {Can We Defuse the Digital Timebomb? Linguistics, Speech Technology and the Irish Language Community},
  booktitle = {Proceedings of the {{Language Technologies}} for {{All}} ({{LT4All}})},
  author = {Ní Chasaide, Ailbhe and Ní Chiaráin, Neasa and Berthelsen, Harald and Wendler, Christoph and Murphy, Andrew and Barnes, Emily and Gobl, Christer},
  date = {2019},
  pages = {177--181},
  publisher = {European Language Resources Association (ELRA)},
  doi = {10.21437/SpeechProsody.2016-73},
  url = {https://lt4all.elra.info/media/papers/O8/97.pdf},
  urldate = {2025-04-04},
  abstract = {Does speech/language technology represent a 'digital timebomb' - or an unprecedented opportunity - for minority and indigenous languages? For successful outcomes, technology development must address linguistic challenges, answer to the needs of the local language communities, enlisting them as a central partner in development. The Irish language ABAIR initiative is building (i) linguistic resources, (ii) core technologies, and (iii) applications for public, educational and access/disability use. The Government’s Digital Plan for Irish Speech and Language Technology provides a model of the support needed by minority languages in the digital age, if the language is to feature in everyday community activities.},
  eventtitle = {Proceedings of the {{Language Technologies}} for {{All}} ({{LT4All}})},
  langid = {english},
  file = {/home/pccady/Zotero/storage/HEZYJ5BN/O'Reilly and Ní Chasaide - 2016 - Modelling the timing and scaling of nuclear pitch accents of Connaught and Ulster Irish with the Fuj.pdf}
}

@article{nichasaideSPEECHTECHNOLOGYDOCUMENTATION2015,
  title = {{{SPEECH TECHNOLOGY AS DOCUMENTATION FOR ENDANGERED LANGUAGE PRESERVATION}}: {{THE CASE OF IRISH}}},
  author = {Ní Chasaide, Ailbhe and Ní Chiaráin, Neasa and Berthelsen, Harald and Wendler, Christoph and Murphy, Andrew},
  date = {2015},
  abstract = {Developing speech technology such as text-tospeech (TTS), requiring as it does a raft of phonetic and linguistic resources, can provide a powerful way to document endangered languages. Drawing on the experience of the ABAIR initiative, developing such resources for Irish [1], we illustrate how both the technology and the underpinning resources can be exploited in a variety of ways that can contribute to the preservation and revitalisation of these languages. By enabling new avenues of application, they can further help address the particular challenges that face the language users and learners. To maximise the immediate and downstream impact, resource development should ideally involve linguistically transparent, rule-based approaches, rather than the machine learning approaches typical of the commercially driven TTS systems for major world languages.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/FUC4REGR/Chasaide et al. - SPEECH TECHNOLOGY AS DOCUMENTATION FOR ENDANGERED LANGUAGE PRESERVATION THE CASE OF IRISH.pdf}
}

@incollection{nichiarainCorpasClisteCreating2022,
  title = {An {{Corpas Cliste}}: Creating a Learner Corpus for {{Irish}} from a New, Purpose-Built {{iCALL}} Platform},
  shorttitle = {An {{Corpas Cliste}}},
  booktitle = {Intelligent {{CALL}}, Granular Systems and Learner Data: Short Papers from                         {{EUROCALL}} 2022},
  author = {Ní Chiaráin, Neasa},
  editor = {Arnbjörnsdóttir, Birna and Bédi, Branislav and Bradley, Linda and Friðriksdóttir, Kolbrún and Garðarsdóttir, Hólmfríður and Thouësny, Sylvie and Whelpton, Matthew James},
  date = {2022-12-12},
  edition = {1},
  pages = {297--301},
  publisher = {Research-publishing.net},
  doi = {10.14705/rpnet.2022.61.1474},
  url = {https://research-publishing.net/manuscript?10.14705/rpnet.2022.61.1474},
  urldate = {2025-04-04},
  abstract = {An Corpas Cliste (‘Clever Corpus’) is an Irish language learner corpus. The corpus data comes from a purpose-built intelligent Computer Assisted Language Learning (iCALL) platform called An Scéalaí (‘the Storyteller’) and comprises both audio and text, produced by second and third level learners of Irish. Metadata (e.g. L1, level of Irish, dialect preference, age) is saved with every learner account, along with data on platform engagement (e.g. speech/language technologies employed, time spent on task). This paper illustrates how An Corpas Cliste is structured and is being prepared for analysis and the methodologies and resources that are being used to exploit it with a view to enhancing the learning experience.},
  isbn = {978-2-38372-015-7},
  langid = {english},
  file = {/home/pccady/Zotero/storage/A9BP3SY7/Ní Chiaráin - 2022 - An Corpas Cliste creating a learner corpus for Irish from a new, purpose-built iCALL platform.pdf}
}

@incollection{nichiarainDigichaintInteractiveGame2016,
  title = {The {{Digichaint}} Interactive Game as a Virtual Learning Environment for {{Irish}}},
  booktitle = {{{CALL}} Communities and Culture – Short Papers from {{EUROCALL}} 2016},
  author = {Ní Chiaráin, Neasa and Ní Chasaide, Ailbhe},
  editor = {Papadima-Sophocleous, Salomi and Bradley, Linda and Thouësny, Sylvie},
  date = {2016-12-18},
  pages = {330--336},
  publisher = {Research-publishing.net},
  doi = {10.14705/rpnet.2016.eurocall2016.584},
  url = {https://research-publishing.net/manuscript?10.14705/rpnet.2016.eurocall2016.584},
  urldate = {2025-04-04},
  abstract = {Although Text-To-Speech (TTS) synthesis has been little used in Computer-Assisted Language Learning (CALL), it is ripe for deployment, particularly for minority and endangered languages, where learners have little access to native speaker models and where few genuinely interactive and engaging teaching/learning materials are available. These considerations lie behind the development of Digichaint, an interactive language learning game which uses ABAIR Irish TTS voices. It provides a language-rich learning environment for Irish language pedagogy and is also used as a testbed to evaluate the intelligibility, quality and attractiveness of the ABAIR synthetic voices.},
  isbn = {978-1-908416-44-5},
  langid = {english},
  file = {/home/pccady/Zotero/storage/9A7YSCBP/Ní Chiaráin and Ní Chasaide - 2016 - The Digichaint interactive game as a virtual learning environment for Irish.pdf}
}

@inbook{nichiarainScealaiSyntheticVoices2018,
  title = {An {{Scéalaí}}: Synthetic Voices for Autonomous Learning},
  shorttitle = {An {{Scéalaí}}},
  booktitle = {Future-Proof {{CALL}}: Language Learning as Exploration and Encounters – Short Papers from {{EUROCALL}} 2018},
  author = {Ní Chiaráin, Neasa and Ní Chasaide, Ailbhe},
  date = {2018-12-08},
  pages = {230--235},
  publisher = {Research-publishing.net},
  doi = {10.14705/rpnet.2018.26.842},
  url = {https://research-publishing.net/manuscript?10.14705/rpnet.2018.26.842},
  urldate = {2025-04-04},
  abstract = {This paper details the motivation for and the main characteristics of An Scéalaí (‘The Storyteller’), an intelligent Computer Assisted Language Learning (iCALL) platform for autonomous learning that integrates the four skills; writing, listening, speaking, and reading. A key feature is the incorporation of speech technology. Speech synthesis provides aural feedback which draws the learners’ attention to errors in the text. Natural language prompts focus on common spelling and grammatical errors, further guiding the learners’ ability to revise and self-correct written materials. While An Scéalaí is still in early stages of development, results of a feasibility study are positive and point towards directions for further development.},
  bookauthor = {Taalas, Peppi and Jalkanen, Juha and Bradley, Linda and Thouësny, Sylvie},
  isbn = {978-2-490057-22-1},
  langid = {english},
  file = {/home/pccady/Zotero/storage/EZ85DTLC/Ní Chiaráin and Ní Chasaide - 2018 - An Scéalaí synthetic voices for autonomous learning.pdf}
}

@inproceedings{nichiarainUsingSpeechNLP2022,
  title = {Using {{Speech}} and {{NLP Resources}} to Build an {{iCALL}} Platform for a Minority Language, the Story of {{An Scéalaí}}, the {{Irish}} Experience to Date},
  booktitle = {Proceedings of the {{Fifth Workshop}} on the {{Use}} of {{Computational Methods}} in the {{Study}} of {{Endangered Languages}}},
  author = {Ní Chiaráin, Neasa and Nolan, Oisín and Comtois, Madeleine and Robinson Gunning, Neimhin and Berthelsen, Harald and Ni Chasaide, Ailbhe},
  date = {2022},
  pages = {109--118},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.computel-1.14},
  url = {https://aclanthology.org/2022.computel-1.14},
  urldate = {2025-04-04},
  abstract = {This paper describes how emerging linguistic resources and technologies can be used to build a language learning platform for Irish, an endangered language. This platform, An Scéalaí, harvests learner corpora – a vital resource both to study the stages of learners’ language acquisition and to guide future platform development. A technical description of the platform is provided, including details of how different speech technologies and linguistic resources are fused to provide a holistic learner experience. The active continuous participation of the community, and platform evaluations by learners and teachers, are discussed.},
  eventtitle = {Proceedings of the {{Fifth Workshop}} on the {{Use}} of {{Computational Methods}} in the {{Study}} of {{Endangered Languages}}},
  langid = {english},
  file = {/home/pccady/Zotero/storage/5J9PL7H6/Ní Chiaráin et al. - 2022 - Using Speech and NLP Resources to build an iCALL platform for a minority language, the story of An S.pdf}
}

@online{niehuesModelingConfidenceSequencetoSequence2019,
  title = {Modeling {{Confidence}} in {{Sequence-to-Sequence Models}}},
  author = {Niehues, Jan and Pham, Ngoc-Quan},
  date = {2019-10-04},
  eprint = {1910.01859},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.01859},
  url = {http://arxiv.org/abs/1910.01859},
  urldate = {2025-04-04},
  abstract = {Recently, significant improvements have been achieved in various natural language processing tasks using neural sequence-to-sequence models. While aiming for the best generation quality is important, ultimately it is also necessary to develop models that can assess the quality of their output.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/G7NFKM7V/Niehues and Pham - 2019 - Modeling Confidence in Sequence-to-Sequence Models.pdf}
}

@article{obczovskySystematizingDecisionsDesignBased2025,
  title = {Systematizing {{Decisions}} in {{Design}}‐{{Based Research}}: {{From Theory}} to {{Design}}},
  shorttitle = {Systematizing {{Decisions}} in {{Design}}‐{{Based Research}}},
  author = {Obczovsky, Markus and Bernsteiner, Angelika and Haagen‐Schützenhöfer, Claudia and Schubatzky, Thomas},
  date = {2025-03},
  journaltitle = {Science Education},
  shortjournal = {Science Education},
  volume = {109},
  number = {2},
  pages = {523--536},
  issn = {0036-8326, 1098-237X},
  doi = {10.1002/sce.21915},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sce.21915},
  urldate = {2025-04-04},
  abstract = {There is a general consensus that design‐based research (DBR) is a genre of approaches in education research to design interventions for specific problems with the aim to gain an understanding of how they work in the problem context. While there is a considerable body of literature discussing the epistemic and methodological aspects of DBR, we found little guidance on how to actually design an intervention. The majority of research articles about DBR projects focus on the output of the projects rather than the process. In this article, we reflect on our approach to design processes in two DBR projects, with a particular focus on making and grounding design decisions. Our aim is to provide support to other researchers engaged in DBR projects and to contribute to a discussion about practices in DBR to contribute to design methodologies. We conclude with three key contributions. First, we emphasize the significance of awareness of design decisions as well as their groundings, advocating for thorough consideration of the applicability of theories, and transferability of empirical findings, as well as pragmatic and personal factors. Second, we recommend a flexibility in revising design decisions, allowing for meaningful refinement of interventions in response to evolving contexts and insights. Third, we propose a systematic approach to arrive at and document design decisions, facilitating communication within DBR projects and across research communities.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/7VFP64GT/Obczovsky et al. - 2025 - Systematizing Decisions in Design‐Based Research From Theory to Design.pdf}
}

@article{papadopoulosConfidenceEstimationMethods2001,
  title = {Confidence Estimation Methods for Neural Networks: A Practical Comparison},
  shorttitle = {Confidence Estimation Methods for Neural Networks},
  author = {Papadopoulos, G. and Edwards, P.J. and Murray, A.F.},
  date = {2001-11},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  volume = {12},
  number = {6},
  pages = {1278--1287},
  issn = {10459227},
  doi = {10.1109/72.963764},
  url = {http://ieeexplore.ieee.org/document/963764/},
  urldate = {2025-04-04},
  abstract = {Feedforward neural networks, particularly multilayer perceptrons, are widely used in regression and classification tasks. A reliable and practical measure of prediction confidence is essential. In this work three alternative approaches to prediction confidence estimation are presented and compared. The three methods are the maximum likelihood, approximate Bayesian, and the bootstrap technique. We consider prediction uncertainty owing to both data noise and model parameter misspecification. The methods are tested on a number of controlled artificial problems and a real, industrial regression application, the prediction of paper “curl.” Confidence estimation performance is assessed by calculating the mean and standard deviation of the prediction interval coverage probability. We show that treating data noise variance as a function of the inputs is appropriate for the curl prediction task. Moreover, we show that the mean coverage probability can only gauge confidence estimation performance as an average over the input space, i.e., global performance and that the standard deviation of the coverage is unreliable as a measure of local performance. The approximate Bayesian approach is found to perform better in terms of global performance.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/DJHQC45P/Papadopoulos et al. - 2001 - Confidence estimation methods for neural networks a practical comparison.pdf}
}

@online{parikhEvaluatingLogitBasedGOP2025,
  title = {Evaluating {{Logit-Based GOP Scores}} for {{Mispronunciation Detection}}},
  author = {Parikh, Aditya Kamlesh and Tejedor-Garcia, Cristian and Cucchiarini, Catia and Strik, Helmer},
  date = {2025-07-08},
  eprint = {2506.12067},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2506.12067},
  url = {http://arxiv.org/abs/2506.12067},
  urldate = {2025-08-31},
  abstract = {Pronunciation assessment relies on goodness of pronunciation (GOP) scores, traditionally derived from softmax-based posterior probabilities. However, posterior probabilities may suffer from overconfidence and poor phoneme separation, limiting their effectiveness. This study compares logit-based GOP scores with probability-based GOP scores for mispronunciation detection. We conducted our experiment on two L2 English speech datasets spoken by Dutch and Mandarin speakers, assessing classification performance and correlation with human ratings. Logit-based methods outperform probability-based GOP in classification, but their effectiveness depends on dataset characteristics. The maximum logit GOP shows the strongest alignment with human perception, while a combination of different GOP scores balances probability and logit features. The findings suggest that hybrid GOP methods incorporating uncertainty modeling and phoneme-specific weighting improve pronunciation assessment.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/ETG9BVAH/Parikh et al. - 2025 - Evaluating Logit-Based GOP Scores for Mispronunciation Detection.pdf;/home/pccady/Zotero/storage/AYCNULJN/2506.html}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2025-09-08},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  file = {/home/pccady/Zotero/storage/IVX7WQNM/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf}
}

@article{pengEndtoEndMispronunciationDetection2023,
  title = {End-to-{{End Mispronunciation Detection}} and {{Diagnosis Using Transfer Learning}}},
  author = {Peng, Linkai and Gao, Yingming and Bao, Rian and Li, Ya and Zhang, Jinsong},
  date = {2023-06-02},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {13},
  number = {11},
  pages = {6793},
  issn = {2076-3417},
  doi = {10.3390/app13116793},
  url = {https://www.mdpi.com/2076-3417/13/11/6793},
  urldate = {2025-06-16},
  abstract = {As an indispensable module of computer-aided pronunciation training (CAPT) systems, mispronunciation detection and diagnosis (MDD) techniques have attracted a lot of attention from academia and industry over the past decade. To train robust MDD models, this technique requires massive human-annotated speech recordings which are usually expensive and even hard to acquire. In this study, we propose to use transfer learning to tackle the problem of data scarcity from two aspects. First, from audio modality, we explore the use of the pretrained model wav2vec2.0 for MDD tasks by learning robust general acoustic representation. Second, from text modality, we explore transferring prior texts into MDD by learning associations between acoustic and textual modalities. We propose textual modulation gates that assign more importance to the relevant text information while suppressing irrelevant text information. Moreover, given the transcriptions, we propose an extra contrastive loss to reduce the difference of learning objectives between the phoneme recognition and MDD tasks. Conducting experiments on the L2-Arctic dataset showed that our wav2vec2.0 based models outperformed the conventional methods. The proposed textual modulation gate and contrastive loss further improved the F1-score by more than 2.88\% and our best model achieved an F1-score of 61.75\%.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/D2YW35F7/Peng et al. - 2023 - End-to-End Mispronunciation Detection and Diagnosis Using Transfer Learning.pdf}
}

@online{pengOWSMCTCOpenEncoderOnly2024,
  title = {{{OWSM-CTC}}: {{An Open Encoder-Only Speech Foundation Model}} for {{Speech Recognition}}, {{Translation}}, and {{Language Identification}}},
  shorttitle = {{{OWSM-CTC}}},
  author = {Peng, Yifan and Sudo, Yui and Shakeel, Muhammad and Watanabe, Shinji},
  date = {2024-08-27},
  eprint = {2402.12654},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.12654},
  url = {http://arxiv.org/abs/2402.12654},
  urldate = {2025-04-04},
  abstract = {There has been an increasing interest in large speech models that can perform multiple tasks in a single model. Such models usually adopt an encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 24\% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up. We will publicly release our code, pre-trained model, and training logs to promote open science in speech foundation models.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/25JCJJAX/Peng et al. - 2024 - OWSM-CTC An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Lang.pdf}
}

@inproceedings{pengStudyFineTuningWav2vec202021,
  title = {A {{Study}} on {{Fine-Tuning}} Wav2vec2.0 {{Model}} for the {{Task}} of {{Mispronunciation Detection}} and {{Diagnosis}}},
  booktitle = {Interspeech 2021},
  author = {Peng, Linkai and Fu, Kaiqi and Lin, Binghuai and Ke, Dengfeng and Zhan, Jinsong},
  date = {2021-08-30},
  pages = {4448--4452},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2021-1344},
  url = {https://www.isca-archive.org/interspeech_2021/peng21e_interspeech.html},
  urldate = {2025-04-04},
  abstract = {Mispronunciation detection and diagnosis (MDD) technology is a key component of computer-assisted pronunciation training system (CAPT). The mainstream method is based on deep neural network automatic speech recognition. Unfortunately, the technique requires massive human-annotated speech recordings for training. Due to the huge variations in mother tongue, age, and proficiency level among second language learners, it is difficult to gather a large amount of matching data for acoustic model training, which greatly limits the model performance. In this paper, we explore the use of Self-Supervised Pretraining (SSP) model wav2vec2.0 for MDD tasks. SSP utilizes a large unlabelled dataset to learn general representation and can be applied in downstream tasks. We conduct experiments using two publicly available datasets (TIMIT, L2-arctic) and our best system achieves 60.44\% f1-score. Moreover, our method is able to achieve 55.52\% f1-score with 3 times less data, which demonstrates the effectiveness of SSP on MDD1.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/home/pccady/Zotero/storage/4VWGN4LM/Peng et al. - 2021 - A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis.pdf}
}

@online{pengTextAwareEndtoendMispronunciation2022,
  title = {Text-{{Aware End-to-end Mispronunciation Detection}} and {{Diagnosis}}},
  author = {Peng, Linkai and Gao, Yingming and Lin, Binghuai and Ke, Dengfeng and Xie, Yanlu and Zhang, Jinsong},
  date = {2022-06-15},
  eprint = {2206.07289},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07289},
  url = {http://arxiv.org/abs/2206.07289},
  urldate = {2025-06-16},
  abstract = {Mispronunciation detection and diagnosis (MDD) technology is a key component of computer-assisted pronunciation training system (CAPT). In the field of assessing the pronunciation quality of constrained speech, the given transcriptions can play the role of a teacher. Conventional methods have fully utilized the prior texts for the model construction or improving the system performance, e.g. forced-alignment and extended recognition networks. Recently, some end-to-end based methods attempt to incorporate the prior texts into model training and preliminarily show the effectiveness. However, previous studies mostly consider applying raw attention mechanism to fuse audio representations with text representations, without taking possible text-pronunciation mismatch into account. In this paper, we present a gating strategy that assigns more importance to the relevant audio features while suppressing irrelevant text information. Moreover, given the transcriptions, we design an extra contrastive loss to reduce the gap between the learning objective of phoneme recognition and MDD. We conducted experiments using two publicly available datasets (TIMIT and L2Arctic) and our best model improved the F1 score from 57.51\% to 61.75\% compared to the baselines. Besides, we provide a detailed analysis to shed light on the effectiveness of gating mechanism and contrastive learning on MDD1.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/BPCSWBTP/Peng et al. - 2022 - Text-Aware End-to-end Mispronunciation Detection and Diagnosis.pdf}
}

@article{pisoniHandbookSpeechPerception,
  title = {The {{Handbook}} of {{Speech Perception}}},
  author = {Pisoni, David B and Remez, Robert E},
  langid = {english},
  file = {/home/pccady/Zotero/storage/7HRR2MGX/Pisoni and Remez - The Handbook of Speech Perception.pdf}
}

@article{poveyKaldiSpeechRecognition,
  title = {The {{Kaldi Speech Recognition Toolkit}}},
  author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukaˇs and Glembek, Ondˇrej and Goel, Nagendra and Hannemann, Mirko and Motlıˇcek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel},
  abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/MAC6UCHS/Povey et al. - The Kaldi Speech Recognition Toolkit.pdf}
}

@online{prabhavalkarMinimumWordError2017,
  title = {Minimum {{Word Error Rate Training}} for {{Attention-based Sequence-to-Sequence Models}}},
  author = {Prabhavalkar, Rohit and Sainath, Tara N. and Wu, Yonghui and Nguyen, Patrick and Chen, Zhifeng and Chiu, Chung-Cheng and Kannan, Anjuli},
  date = {2017-12-05},
  eprint = {1712.01818},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1712.01818},
  url = {http://arxiv.org/abs/1712.01818},
  urldate = {2025-04-04},
  abstract = {Sequence-to-sequence models, such as attention-based models in automatic speech recognition (ASR), are typically trained to optimize the cross-entropy criterion which corresponds to improving the loglikelihood of the data. However, system performance is usually measured in terms of word error rate (WER), not log-likelihood. Traditional ASR systems benefit from discriminative sequence training which optimizes criteria such as the state-level minimum Bayes risk (sMBR) which are more closely related to WER.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/home/pccady/Zotero/storage/56J9L4X6/Prabhavalkar et al. - 2017 - Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models.pdf}
}

@article{pratapScalingSpeechTechnology,
  title = {Scaling {{Speech Technology}} to 1,000+ {{Languages}}},
  author = {Pratap, Vineel and Tjandra, Andros and Shi, Bowen and Babu, Paden Tomasello Arun and Kundu, Sayani and Elkahky, Ali and Ni, Zhaoheng and Vyas, Apoorv and Fazel-Zarandi, Maryam and Baevski, Alexei and Adi, Yossi and Zhang, Xiaohui and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
  abstract = {Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data. The MMS models are available at https://github.com/pytorch/fairseq/tree/master/examples/mms.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/5Z9XSBY6/Pratap et al. - Scaling Speech Technology to 1,000+ Languages.pdf}
}

@inproceedings{punjabiLanguageModelBootstrapping2019,
  title = {Language {{Model Bootstrapping Using Neural Machine Translation}} for {{Conversational Speech Recognition}}},
  booktitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Punjabi, Surabhi and Arsikere, Harish and Garimella, Sri},
  date = {2019-12},
  pages = {487--493},
  publisher = {IEEE},
  location = {SG, Singapore},
  doi = {10.1109/ASRU46091.2019.9003982},
  url = {https://ieeexplore.ieee.org/document/9003982/},
  urldate = {2025-06-16},
  abstract = {Building conversational speech recognition systems for new languages is constrained by the availability of utterances capturing user-device interactions. Data collection is expensive and limited by speed of manual transcription. In order to address this, we advocate the use of neural machine translation as a data augmentation technique for bootstrapping language models. Machine translation (MT) offers a systematic way of incorporating collections from mature, resource-rich conversational systems that may be available for a different language. However, ingesting raw translations from a general purpose MT system may not be effective owing to the presence of named entities, intra sentential code-switching and the domain mismatch between the conversational data being translated and the parallel text used for MT training. To circumvent this, we explore following domain adaptation techniques: (a) sentence embedding based data selection for MT training, (b) model finetuning, and (c) rescoring and filtering translated hypotheses. Using Hindi language as the experimental testbed, we supplement transcribed collections with translated US English utterances. We observe a relative word error rate reduction of 7.8-15.6\%, depending on the bootstrapping phase. Fine grained analysis reveals that translation particularly aids the interaction scenarios underrepresented in the transcribed data.},
  eventtitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-7281-0306-8},
  langid = {english},
  file = {/home/pccady/Zotero/storage/CPJQZQLD/Punjabi et al. - 2019 - Language Model Bootstrapping Using Neural Machine Translation for Conversational Speech Recognition.pdf}
}

@inproceedings{qianAutomaticSpeechRecognition2022,
  title = {Automatic {{Speech Recognition}} for {{Irish}}: Testing Lexicons and Language Models},
  shorttitle = {Automatic {{Speech Recognition}} for {{Irish}}},
  booktitle = {2022 33rd {{Irish Signals}} and {{Systems Conference}} ({{ISSC}})},
  author = {Qian, Mengjie and Berthelsen, Harald and Lonergan, Liam and Murphy, Andy and O'Neill, Claire and Ni Chiarain, Neasa and Gobl, Christer and Ni Chasaide, Ailbhe},
  date = {2022-06-09},
  pages = {1--6},
  publisher = {IEEE},
  location = {Cork, Ireland},
  doi = {10.1109/ISSC55427.2022.9826201},
  url = {https://ieeexplore.ieee.org/document/9826201/},
  urldate = {2025-04-04},
  abstract = {A range of lexicons and language models were tested in the development of ASR for Irish. One problem, common among minority languages, is the multiplicity of dialects, with no one spoken standard. To address this challenge, in a hybrid ASR system two alternative cross-dialect lexicons are tested, which draw on research in dialect phonology. First, individual lexicons were built for the three main dialects of Ulster (Ul), Connaught (Co) and Munster (Mu). With these, a Multi-dialect lexicon incorporated all dialect-varying word forms. An alternative Global lexicon, essentially a trans-dialect lexicon, used abstract representations of dialect-varying forms (phoneme or morpheme sized units). These two cross-dialect lexicons were tested along with the three dialect-specific lexicons. Several different language models were also tested. Results for the Global and Multi-dialect lexicons were found to yield the highest performance, with the lowest overall WER for the latter. There were considerable differences in results for the individual dialect lexicons: this may reflect a bias in the datasets used or could be indicators of the linguistic distance between the dialects — competing hypotheses that will need more rigorous testing. Results showed a strong effect of the language model used. Error patterns show frequent substitutions involving inflected forms.},
  eventtitle = {2022 33rd {{Irish Signals}} and {{Systems Conference}} ({{ISSC}})},
  isbn = {978-1-6654-5227-4},
  langid = {english},
  file = {/home/pccady/Zotero/storage/ETGTXC54/Qian et al. - 2022 - Automatic Speech Recognition for Irish testing lexicons and language models.pdf}
}

@online{ranathungaNeuralMachineTranslation2021,
  title = {Neural {{Machine Translation}} for {{Low-Resource Languages}}: {{A Survey}}},
  shorttitle = {Neural {{Machine Translation}} for {{Low-Resource Languages}}},
  author = {Ranathunga, Surangika and Lee, En-Shiun Annie and Skenduli, Marjana Prifti and Shekhar, Ravi and Alam, Mehreen and Kaur, Rishemjit},
  date = {2021-06-29},
  eprint = {2106.15115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.15115},
  url = {http://arxiv.org/abs/2106.15115},
  urldate = {2025-04-04},
  abstract = {Neural Machine Translation (NMT) has seen a tremendous spurt of growth in less than ten years, and has already entered a mature phase. While considered as the most widely used solution for Machine Translation, its performance on low-resource language pairs still remains sub-optimal compared to the high-resource counterparts, due to the unavailability of large parallel corpora. Therefore, the implementation of NMT techniques for low-resource language pairs has been receiving the spotlight in the recent NMT research arena, thus leading to a substantial amount of research reported on this topic. This paper presents a detailed survey of research advancements in low-resource language NMT (LRL-NMT), along with a quantitative analysis aimed at identifying the most popular solutions. Based on our findings from reviewing previous work, this survey paper provides a set of guidelines to select the possible NMT technique for a given LRL data setting. It also presents a holistic view of the LRL-NMT research landscape and provides a list of recommendations to further enhance the research efforts on LRL-NMT. CCS Concepts: • Computing methodologies → Natural language processing; Neural networks; Machine translation; Language resources; Machine learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/SC639869/Ranathunga et al. - 2021 - Neural Machine Translation for Low-Resource Languages A Survey.pdf}
}

@inproceedings{richterOrthographybasedPronunciationScoring2023,
  title = {Orthography-Based {{Pronunciation Scoring}} for {{Better CAPT Feedback}}},
  booktitle = {{{INTERSPEECH}} 2023},
  author = {Richter, Caitlin and Pálsson, Ragnar and O'Brien, Luke and Friðriksdóttir, Kolbrún and Bédi, Branislav and Magnúsdóttir, Eydís Huld and Guðnason, Jón},
  date = {2023-08-20},
  pages = {1004--1008},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2023-2577},
  url = {https://www.isca-archive.org/interspeech_2023/richter23b_interspeech.html},
  urldate = {2025-04-04},
  eventtitle = {{{INTERSPEECH}} 2023},
  langid = {english},
  file = {/home/pccady/Zotero/storage/9PX62BU9/Richter et al. - 2023 - Orthography-based Pronunciation Scoring for Better CAPT Feedback.pdf}
}

@article{rogerson-revellComputerAssistedPronunciationTraining2021,
  title = {Computer-{{Assisted Pronunciation Training}} ({{CAPT}}): {{Current Issues}} and {{Future Directions}}},
  shorttitle = {Computer-{{Assisted Pronunciation Training}} ({{CAPT}})},
  author = {Rogerson-Revell, Pamela M},
  date = {2021-04},
  journaltitle = {RELC Journal},
  shortjournal = {RELC Journal},
  volume = {52},
  number = {1},
  pages = {189--205},
  issn = {0033-6882, 1745-526X},
  doi = {10.1177/0033688220977406},
  url = {https://journals.sagepub.com/doi/10.1177/0033688220977406},
  urldate = {2025-04-04},
  abstract = {This viewpoint essay considers the current status of computer-assisted pronunciation training (CAPT) before examining some of the current issues and future directions in the field. The underlying premise is the pedagogic potential of CAPT systems and resources for teaching and learning, and the need for greater synergy between technological design and functionality on the one hand, and pedagogic purpose on the other. Some of the key issues examined include providing accurate and individualised automated feedback for pronunciation, for both learning and assessment, and evaluating the effectiveness of CAPT tools and systems. When considering future directions, the discussion focuses on what aspects of pedagogy are likely to be at the forefront of developments, including ubiquitous learning; intelligent tutoring and authentic interaction; and goal-oriented, task-based learning.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/EJLED84K/Rogerson-Revell - 2021 - Computer-Assisted Pronunciation Training (CAPT) Current Issues and Future Directions.pdf}
}

@article{rosenblumSpeechPerceptionMultimodal2008,
  title = {Speech {{Perception}} as a {{Multimodal Phenomenon}}},
  author = {Rosenblum, Lawrence D.},
  date = {2008-12},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {17},
  number = {6},
  pages = {405--409},
  issn = {0963-7214, 1467-8721},
  doi = {10.1111/j.1467-8721.2008.00615.x},
  url = {https://journals.sagepub.com/doi/10.1111/j.1467-8721.2008.00615.x},
  urldate = {2025-04-04},
  abstract = {Speech perception is inherently multimodal. Visual speech (lip-reading) information is used by all perceivers and readily integrates with auditory speech. Imaging research suggests that the brain treats auditory and visual speech similarly. These findings have led some researchers to consider that speech perception works by extracting amodal information that takes the same form across modalities. From this perspective, speech integration is a property of the input information itself. Amodal speech information could explain the reported automaticity, immediacy, and completeness of audiovisual speech integration. However, recent findings suggest that speech integration can be influenced by higher cognitive properties such as lexical status and semantic context. Proponents of amodal accounts will need to explain these results.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/HA5WE24Z/Rosenblum - 2008 - Speech Perception as a Multimodal Phenomenon.pdf}
}

@inproceedings{rouditchenkoComparisonMultilingualSelfSupervised2023,
  title = {Comparison of {{Multilingual Self-Supervised}} and {{Weakly-Supervised Speech Pre-Training}} for {{Adaptation}} to {{Unseen Languages}}},
  booktitle = {{{INTERSPEECH}} 2023},
  author = {Rouditchenko, Andrew and Khurana, Sameer and Thomas, Samuel and Feris, Rogerio and Karlinsky, Leonid and Kuehne, Hilde and Harwath, David and Kingsbury, Brian and Glass, James},
  date = {2023-08-20},
  pages = {2268--2272},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2023-1061},
  url = {https://www.isca-archive.org/interspeech_2023/rouditchenko23_interspeech.html},
  urldate = {2025-04-04},
  abstract = {Recent models such as XLS-R and Whisper have made multilingual speech technologies more accessible by pre-training on audio from around 100 spoken languages each. However, there are thousands of spoken languages worldwide, and adapting to new languages is an important problem. In this work, we aim to understand which model adapts better to languages unseen during pre-training. We fine-tune both models on 13 unseen languages and 18 seen languages. Our results show that the number of hours seen per language and language family during pre-training is predictive of how the models compare, despite the significant differences in the pre-training methods.},
  eventtitle = {{{INTERSPEECH}} 2023},
  langid = {english},
  file = {/home/pccady/Zotero/storage/SYX7C589/Rouditchenko et al. - 2023 - Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation.pdf}
}

@article{saitoEffectsSecondLanguage2019,
  title = {Effects of {{Second Language Pronunciation Teaching Revisited}}: {{A Proposed Measurement Framework}} and {{Meta}}‐{{Analysis}}},
  shorttitle = {Effects of {{Second Language Pronunciation Teaching Revisited}}},
  author = {Saito, Kazuya and Plonsky, Luke},
  date = {2019-09},
  journaltitle = {Language Learning},
  shortjournal = {Language Learning},
  volume = {69},
  number = {3},
  pages = {652--708},
  issn = {0023-8333, 1467-9922},
  doi = {10.1111/lang.12345},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/lang.12345},
  urldate = {2025-04-04},
  abstract = {We propose a new framework for conceptualizing measures of instructed L2 pronunciation proficiency according to three sets of parameters: (a) the constructs being focused on (global vs. specific), (b) the scoring method (human raters vs. acoustic analyses), and (c) the type of knowledge being elicited (controlled vs. spontaneous). Adopting the model (i.e., Framework for L2 Pronunciation Measurement) as a synthetic tool, we code the instruments found in 77 studies of L2 pronunciation teaching published between 1982 and 2017. We calculate the frequency of each measurement type and re-examine the interaction of instructional effectiveness and measurement within the sample. According to the results, instruction is most effective when it targets learners’ monitored production of specific segmental/suprasegmental features. The efficacy of instruction remains relatively unclear when gains are measured globally via subjective/human judgements especially at a spontaneous level. The findings are discussed to improve the designs in L2 pronunciation research and, more generally, strengthen the interface between pronunciation teaching, measurement and SLA.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/WZS99UEJ/Saito and Plonsky - 2019 - Effects of Second Language Pronunciation Teaching Revisited A Proposed Measurement Framework and Me.pdf}
}

@incollection{schmidtChapter2Attention2012,
  title = {Chapter 2. {{Attention}}, Awareness, and Individual Differences in Language Learning},
  booktitle = {Perspectives on {{Individual Characteristics}} and {{Foreign Language Education}}},
  author = {Schmidt, Richard},
  editor = {Chan, Wai Meng and Chin, Kwee Nyet and Bhatt, Sunil and Walker, Izumi},
  date = {2012-09-13},
  pages = {27--50},
  publisher = {DE GRUYTER},
  doi = {10.1515/9781614510932.27},
  url = {https://www.degruyter.com/document/doi/10.1515/9781614510932.27/html},
  urldate = {2025-04-04},
  isbn = {978-1-61451-095-6 978-1-61451-093-2},
  langid = {english},
  file = {/home/pccady/Zotero/storage/YN6L4LEJ/Schmidt - 2012 - Chapter 2. Attention, awareness, and individual differences in language learning.pdf}
}

@inproceedings{shahinPhonologicalLevelMispronunciationDetection2024,
  title = {Phonological-{{Level Mispronunciation Detection}} and {{Diagnosis}}},
  booktitle = {Interspeech 2024},
  author = {Shahin, Mostafa and Ahmed, Beena},
  date = {2024-09-01},
  pages = {307--311},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2024-2217},
  url = {https://www.isca-archive.org/interspeech_2024/shahin24_interspeech.html},
  urldate = {2025-04-04},
  abstract = {The automatic identification and analysis of pronunciation errors, known as mispronunciation detection and diagnosis (MDD), is vital in computer-aided pronunciation learning (CAPL) tools for second-language (L2) learning. Existing MDD methods focus on analyzing phonemes, but they can only detect categorical errors for phonemes with sufficient training data. Due to the unpredictable nature of non-native speakers’ pronunciation errors and limited training datasets, modelling all mispronunciations becomes impractical. Additionally, phoneme-level MDD approaches provide limited diagnostic information. In our proposed approach, we detect phonological features, breaking down phoneme production into elementary components related to the articulatory system, offering more informative feedback to learners. Applied to L2 English speech data, it outperformed traditional phoneme-level methods, reducing false acceptance rate (FAR), false rejection rate (FRR), and diagnostic error rate (DER).},
  eventtitle = {Interspeech 2024},
  langid = {english},
  file = {/home/pccady/Zotero/storage/F4SEIXYG/Shahin and Ahmed - 2024 - Phonological-Level Mispronunciation Detection and Diagnosis.pdf}
}

@article{sheenCorrectiveFeedbackLearner2004,
  title = {Corrective Feedback and Learner Uptake in Communicative Classrooms across Instructional Settings},
  author = {Sheen, YoungHee},
  date = {2004-07},
  journaltitle = {Language Teaching Research},
  shortjournal = {Language Teaching Research},
  volume = {8},
  number = {3},
  pages = {263--300},
  issn = {1362-1688, 1477-0954},
  doi = {10.1191/1362168804lr146oa},
  url = {https://journals.sagepub.com/doi/10.1191/1362168804lr146oa},
  urldate = {2025-04-04},
  abstract = {This paper reports similarities and differences in teachers’ corrective feedback and learners’ uptake across instructional settings. Four communicative classroom settings - French Immersion, Canada ESL, New Zealand ESL and Korean EFL - were examined using Lyster and Ranta’s taxonomy of teachers’ corrective feedback moves and learner uptake. The results indicate that recasts were the most frequent feedback type in all four contexts but were much more frequent in the Korean EFL and New Zealand ESL classrooms (83\% and 68\%, respectively) than in the Canadian Immersion and ESL classrooms (55\% for both). Also, the rates for both uptake and repair following recasts were greater in the New Zealand and Korean settings than in the Canadian contexts. The findings of this study suggest that the extent to which recasts lead to learner uptake and repair may be greater in contexts where the focus of the recasts is more salient, as with reduced/partial recasts, and where students are oriented to attending to linguistic form rather than meaning. The study underscores the importance of considering the influence of context on corrective feedback and learner uptake.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/SW5MRK2C/Sheen - 2004 - Corrective feedback and learner uptake in communicative classrooms across instructional settings.pdf}
}

@online{shenNaturalTTSSynthesis2018,
  title = {Natural {{TTS Synthesis}} by {{Conditioning WaveNet}} on {{Mel Spectrogram Predictions}}},
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerry-Ryan, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
  date = {2018-02-16},
  eprint = {1712.05884},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1712.05884},
  url = {http://arxiv.org/abs/1712.05884},
  urldate = {2025-04-04},
  abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/LFGHGH2P/Shen et al. - 2018 - Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.pdf}
}

@thesis{sjons2022articulation,
  type = {phdthesis},
  title = {Articulation Rate and Surprisal in Swedish Child-Directed Speech},
  author = {Sjons, Johan},
  date = {2022},
  institution = {Stockholm University},
  file = {/home/pccady/Zotero/storage/ANDVW38Q/Sjons - 2022 - Articulation rate and surprisal in swedish child-directed speech.pdf}
}

@article{snesarevaPalatalizationDublinIrish2016,
  title = {Palatalization in {{Dublin Irish}}: {{The Extent}} of {{Phonetic Interference}}},
  shorttitle = {Palatalization in {{Dublin Irish}}},
  author = {Snesareva, Marina},
  date = {2016-12},
  journaltitle = {Procedia - Social and Behavioral Sciences},
  shortjournal = {Procedia - Social and Behavioral Sciences},
  volume = {236},
  pages = {213--218},
  issn = {18770428},
  doi = {10.1016/j.sbspro.2016.12.009},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1877042816316421},
  urldate = {2025-04-04},
  abstract = {This paper focuses on palatalization in Irish spoken by Dublin-based bilinguals with English as their first language. It has already been pointed out that English phonetics affects Irish speakers even when Irish is their first language, especially in case of palatalization. The extent of English influence on palatalization in Dublin Irish and the possible reasons behind its inconsistent use acquire special prominence not only in terms of phonetics, but also because in Irish palatalization performs phonological functions.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/JZPBE6KT/Snesareva - 2016 - Palatalization in Dublin Irish The Extent of Phonetic Interference.pdf}
}

@online{somekiESPnetEZPythononlyESPnet2024,
  title = {{{ESPnet-EZ}}: {{Python-only ESPnet}} for {{Easy Fine-tuning}} and {{Integration}}},
  shorttitle = {{{ESPnet-EZ}}},
  author = {Someki, Masao and Choi, Kwanghee and Arora, Siddhant and Chen, William and Cornell, Samuele and Han, Jionghao and Peng, Yifan and Shi, Jiatong and Srivastav, Vaibhav and Watanabe, Shinji},
  date = {2024-09-14},
  eprint = {2409.09506},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2409.09506},
  url = {http://arxiv.org/abs/2409.09506},
  urldate = {2025-04-04},
  abstract = {We introduce ESPnet-EZ, an extension of the open-source speech processing toolkit ESPnet, aimed at quick and easy development of speech models. ESPnet-EZ focuses on two major aspects: (i) easy fine-tuning and inference of existing ESPnet models on various tasks and (ii) easy integration with popular deep neural network frameworks such as PyTorch-Lightning, Hugging Face transformers and datasets, and Lhotse. By replacing ESPnet design choices inherited from Kaldi with a Python-only, Bash-free interface, we dramatically reduce the effort required to build, debug, and use a new model. For example, to fine-tune a speech foundation model, ESPnet-EZ, compared to ESPnet, reduces the number of newly written code by 2.7x and the amount of dependent code by 6.7x while dramatically reducing the Bash script dependencies. The codebase of ESPnet-EZ is publicly available.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/UAN4YHZP/Someki et al. - 2024 - ESPnet-EZ Python-only ESPnet for Easy Fine-tuning and Integration.pdf}
}

@article{spolskyHandbookEducationalLinguistics2008,
  title = {The {{Handbook}} of {{Educational Linguistics}}},
  author = {Spolsky, Bernard and Hult, Francis M},
  date = {2008},
  journaltitle = {Wiley Online Library},
  langid = {english},
  file = {/home/pccady/Zotero/storage/U6S5NSSW/Spolsky and Hult - The Handbook of Educational Linguistics.pdf}
}

@inproceedings{stanleyImprovingL1specificPhonological2012,
  title = {Improving {{L1-specific}} Phonological Error Diagnosis in Computer Assisted Pronunciation Training},
  booktitle = {Interspeech 2012},
  author = {Stanley, Theban and Hacioglu, Kadri},
  date = {2012-09-09},
  pages = {827--830},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2012-251},
  url = {https://www.isca-archive.org/interspeech_2012/stanley12_interspeech.html},
  urldate = {2025-04-04},
  abstract = {With the increasing use of technology in classrooms, computer assisted pronunciation training (CAPT) is becoming a vital tool in language learning. In this paper, we present a system that takes advantage of data from learners of a specific L1 to better model phonological errors at various levels in the system. At the lexical level, a statistical machine translation approach is used to model common phonological errors produced by a specific L1 population. At the acoustic level, L1-dependent maximum likelihood (ML) nonnative models and discriminative training are explored. In our experiments, use of a Korean language dependent nonnative lexicon gives us diagnostic abilities that did not exist in our baseline configuration. Replacing the native ML acoustic model with the L1-dependent nonnative model produces relative improvements of 27–37\% in precision for phone detection/identification tasks. We also propose a constrained variant of minimum phone error (MPE) training which is better adapted to phone detection/diagnosis. This technique produces 5–6\% relative improvement in precision in comparison to ML nonnative acoustic models.},
  eventtitle = {Interspeech 2012},
  langid = {english},
  file = {/home/pccady/Zotero/storage/2S32QJ7Y/Stanley and Hacioglu - 2012 - Improving L1-specific phonological error diagnosis in computer assisted pronunciation training.pdf}
}

@book{stensonModernIrishComprehensive2020,
  title = {Modern {{Irish}}: A Comprehensive Grammar},
  shorttitle = {Modern {{Irish}}},
  author = {Stenson, Nancy},
  date = {2020},
  series = {Routledge Comprehensive Grammars},
  publisher = {Routledge, Taylor \& Francis},
  location = {London ; New York},
  abstract = {"Modern Irish: A Comprehensive Grammar is a complete reference guide to modern Irish grammar, providing a thorough overview of the language. Key features include: - highly systematic coverage of all levels of structure: sound system, word formation, sentence construction and connection of sentences - authentic examples and English translations which provide an accessible insight into the mechanics of the language - an extensive index, numbered sections, cross-references and summary charts which provide readers with easy access to the information. Modern Irish: A Comprehensive Grammar is an essential reference source for the learner and user of Irish. It is ideal for use in schools, colleges, universities, and adult classes of all types"--},
  isbn = {978-1-138-23652-3 978-1-138-23651-6},
  pagetotal = {304},
  keywords = {Grammar,Irish language}
}

@article{strikComparingDifferentApproaches2009,
  title = {Comparing Different Approaches for Automatic Pronunciation Error Detection},
  author = {Strik, Helmer and Truong, Khiet and De Wet, Febe and Cucchiarini, Catia},
  date = {2009-10},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {51},
  number = {10},
  pages = {845--852},
  issn = {01676393},
  doi = {10.1016/j.specom.2009.05.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639309000715},
  urldate = {2025-04-04},
  abstract = {One of the biggest challenges in designing computer assisted language learning (CALL) applications that provide automatic feedback on pronunciation errors consists in reliably detecting the pronunciation errors at such a detailed level that the information provided can be useful to learners. In our research we investigate pronunciation errors frequently made by foreigners learning Dutch as a second language. In the present paper we focus on the velar fricative /x/ and the velar plosive /k/. We compare four types of classifiers that can be used to detect erroneous pronunciations of these phones: two acoustic–phonetic classifiers (one of which employs Linear Discriminant Analysis (LDA)), a classifier based on cepstral coefficients in combination with LDA, and one based on confidence measures (the socalled Goodness Of Pronunciation score). The best results were obtained for the two LDA classifiers which produced accuracy levels of about 85–93\%.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/A4ZQD7E6/Strik et al. - 2009 - Comparing different approaches for automatic pronunciation error detection.pdf}
}

@inproceedings{sudhakaraImprovedGoodnessPronunciation2019,
  title = {An {{Improved Goodness}} of {{Pronunciation}} ({{GoP}}) {{Measure}} for {{Pronunciation Evaluation}} with {{DNN-HMM System Considering HMM Transition Probabilities}}},
  booktitle = {Interspeech 2019},
  author = {Sudhakara, Sweekar and Ramanathi, Manoj Kumar and Yarra, Chiranjeevi and Ghosh, Prasanta Kumar},
  date = {2019-09-15},
  pages = {954--958},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2019-2363},
  url = {https://www.isca-archive.org/interspeech_2019/sudhakara19_interspeech.html},
  urldate = {2025-04-04},
  abstract = {Goodness of pronunciation (GoP) is typically formulated with Gaussian mixture model-hidden Markov model (GMM-HMM) based acoustic models considering HMM state transition probabilities (STPs) and GMM likelihoods of context dependent phonemes. On the other hand, deep neural network (DNN)HMM based acoustic models employed sub-phonemic (senone) posteriors instead of GMM likelihoods along with STPs. However, each senone is shared across many states; thus, there is no one-to-one correspondence between them. In order to circumvent this, most of the existing works have proposed modifications to the GoP formulation considering only posteriors neglecting the STPs. In this work, we derive a formulation for the GoP and it results in the formulation involving both senone posteriors and STPs. Further, we illustrate the steps to implement the proposed GoP formulation in Kaldi, a state-of-the-art automatic speech recognition toolkit. Experiments are conducted on English data collected from Indian speakers using acoustic models trained with native English data from LibriSpeech and Fisher-English corpora. The highest improvement in the correlation coefficient between the scores from the formulations and the expert ratings is found to be 14.89\% (relative) better with the proposed approach compared to the best of the existing formulations that don’t include STPs.},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/home/pccady/Zotero/storage/3K5FF3IE/Sudhakara et al. - 2019 - An Improved Goodness of Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System.pdf}
}

@inproceedings{thaiSyntheticDataAugmentation2019,
  title = {Synthetic {{Data Augmentation}} for {{Improving Low-Resource ASR}}},
  booktitle = {2019 {{IEEE Western New York Image}} and {{Signal Processing Workshop}} ({{WNYISPW}})},
  author = {Thai, Bao and Jimerson, Robert and Arcoraci, Dominic and Prud'hommeaux, Emily and Ptucha, Raymond},
  date = {2019-10},
  pages = {1--9},
  publisher = {IEEE},
  location = {Rochester, NY, USA},
  doi = {10.1109/WNYIPW.2019.8923082},
  url = {https://ieeexplore.ieee.org/document/8923082/},
  urldate = {2025-08-28},
  abstract = {Although the application of deep learning to automatic speech recognition (ASR) has resulted in dramatic reductions in word error rate for languages with abundant training data, ASR for languages with few resources has yet to benefit from deep learning to the same extent. In this paper, we investigate various methods of acoustic modeling and data augmentation with the goal of improving the accuracy of a deep learning ASR framework for a low-resource language with a high baseline word error rate. We compare several methods of generating synthetic acoustic training data via voice transformation and signal distortion, and we explore several strategies for integrating this data into the acoustic training pipeline. We evaluate our methods on an indigenous language of North America with minimal training resources. We show that training initially via transfer learning from an existing high-resource language acoustic model, refining weights using a heavily concentrated synthetic dataset, and finally fine-tuning to the target language using limited synthetic data reduces WER by 15\% over just transfer learning using deep recurrent methods. Further, we show improvements over traditional frameworks by 19\% using a similar multistage training with deep convolutional approaches.},
  eventtitle = {2019 {{IEEE Western New York Image}} and {{Signal Processing Workshop}} ({{WNYISPW}})},
  isbn = {978-1-7281-4352-1},
  langid = {english},
  file = {/home/pccady/Zotero/storage/SC7Q7HGB/Thai et al. - 2019 - Synthetic Data Augmentation for Improving Low-Resource ASR.pdf}
}

@article{thomsonEffectivenessL2Pronunciation2014,
  title = {The {{Effectiveness}} of {{L2 Pronunciation Instruction}}: {{A Narrative Review}}},
  shorttitle = {The {{Effectiveness}} of {{L2 Pronunciation Instruction}}},
  author = {Thomson, Ron and Derwing, Tracey},
  date = {2014-12-08},
  journaltitle = {Applied Linguistics},
  shortjournal = {Applied Linguistics},
  volume = {2014},
  pages = {1--20},
  doi = {10.1093/applin/amu076},
  abstract = {Research on the efficacy of second language (L2) pronunciation instruction has produced mixed results, despite reports of significant improvement in many studies. Possible explanations for divergent outcomes include learner individual differences, goals and foci of instruction, type and duration of instructional input, and assessment procedures. After identifying key concepts, we survey 75 L2 pronunciation studies, particularly their methods and results. Despite a move towards emphasizing speech intelligibility and comprehensibility, most research surveyed promoted native-like pronunciation as the target. Although most studies entailed classroom instruction, many featured Computer Assisted Pronunciation Teaching (CAPT). Segmentals were studied more often than suprasegmentals. The amount of instruction required to effect change was related to researchers’ goals; interventions focusing on a single feature were generally shorter than those addressing more issues. Reading-aloud tasks were the most common form of assessment; very few studies measured spontaneous speech. The attribution of improvement as a result of instruction was compromised in some instances by lack of a control group. We summarize our findings, highlight limitations of current research, and offer suggestions for future directions.},
  file = {/home/pccady/Zotero/storage/WY9M4BSP/Thomson and Derwing - 2014 - The Effectiveness of L2 Pronunciation Instruction A Narrative Review.pdf}
}

@software{tkachenkoLabelStudio2020,
  title = {Label {{Studio}}},
  author = {Tkachenko, Maxim and Malyuk, Mikhail and Holmanyuk, Andrey and Liubimov, Nikolai},
  date = {2020/2025},
  url = {https://github.com/HumanSignal/label-studio},
  abstract = {Open source software available from https://github.com/HumanSignal/label-studio}
}

@book{vanpatten2007theories,
  title = {Theories in Second Language Acquisition},
  author = {VanPatten, Bill},
  file = {/home/pccady/Zotero/storage/UF4M8DVL/VanPatten - Theories in second language acquisition.pdf}
}

@inproceedings{volodina2016proceedings,
  title = {Proceedings of the Joint Workshop on {{NLP}} for Computer Assisted Language Learning and {{NLP}} for Language Acquisition},
  booktitle = {Proceedings of the Joint Workshop on {{NLP}} for Computer Assisted Language Learning and {{NLP}} for Language Acquisition},
  author = {Volodina, Elena and Grigonytė, Gintarė and Pilán, Ildikó and Björkenstam, Kristina Nilsson and Borin, Lars},
  date = {2016},
  file = {/home/pccady/Zotero/storage/W76J2LTZ/Volodina et al. - 2016 - Proceedings of the joint workshop on NLP for computer assisted language learning and NLP for languag.pdf}
}

@inproceedings{wangExploringNonAutoregressiveEndtoEnd2022,
  title = {Exploring {{Non-Autoregressive End-to-End Neural Modeling}} for {{English Mispronunciation Detection}} and {{Diagnosis}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Hsin-Wei and Yan, Bi-Cheng and Chiu, Hsuan-Sheng and Hsu, Yung-Chang and Chen, Berlin},
  date = {2022-05-23},
  pages = {6817--6821},
  publisher = {IEEE},
  location = {Singapore, Singapore},
  doi = {10.1109/ICASSP43922.2022.9747569},
  url = {https://ieeexplore.ieee.org/document/9747569/},
  urldate = {2025-04-04},
  abstract = {End-to-end (E2E) neural modeling has emerged as one predominant school of thought to develop computer-assisted pronunciation training (CAPT) systems, showing competitive performance to conventional pronunciation-scoring based methods. However, current E2E neural methods for CAPT are faced with at least two pivotal challenges. On one hand, most of the E2E methods operate in an autoregressive manner with left-to-right beam search to dictate the pronunciations of an L2 learners. This however leads to very slow inference speed, which inevitably hinders their practical use. On the other hand, E2E neural methods are normally data-hungry and meanwhile an insufficient amount of nonnative training data would often reduce their efficacy on mispronunciation detection and diagnosis (MD\&D). In response, we put forward a novel MD\&D method that leverages non-autoregressive (NAR) E2E neural modeling to dramatically speed up the inference time while maintaining performance in line with the conventional E2E neural methods. In addition, we design and develop a pronunciation modeling network stacked on top of the NAR E2E models of our method to further boost the effectiveness of MD\&D. Empirical experiments conducted on the L2-ARCTIC English dataset seems to validate the feasibility of our method, in comparison to some top-ofthe-line E2E models and an iconic pronunciation-scoring based method built on a DNN-HMM acoustic model.},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-6654-0540-9},
  langid = {english},
  file = {/home/pccady/Zotero/storage/AUJHWQR8/Wang et al. - 2022 - Exploring Non-Autoregressive End-to-End Neural Modeling for English Mispronunciation Detection and D.pdf}
}

@online{watanabeESPnetEndtoEndSpeech2018,
  title = {{{ESPnet}}: {{End-to-End Speech Processing Toolkit}}},
  shorttitle = {{{ESPnet}}},
  author = {Watanabe, Shinji and Hori, Takaaki and Karita, Shigeki and Hayashi, Tomoki and Nishitoba, Jiro and Unno, Yuya and Soplin, Nelson Enrique Yalta and Heymann, Jahn and Wiesner, Matthew and Chen, Nanxin and Renduchintala, Adithya and Ochiai, Tsubasa},
  date = {2018-03-30},
  eprint = {1804.00015},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1804.00015},
  url = {http://arxiv.org/abs/1804.00015},
  urldate = {2025-09-03},
  abstract = {This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/pccady/Zotero/storage/RHIAGLHI/Watanabe et al. - 2018 - ESPnet End-to-End Speech Processing Toolkit.pdf;/home/pccady/Zotero/storage/7YHGCAJ2/1804.html}
}

@online{weiMitigatingNeuralNetwork2022,
  title = {Mitigating {{Neural Network Overconfidence}} with {{Logit Normalization}}},
  author = {Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
  date = {2022-06-24},
  eprint = {2205.09310},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.09310},
  url = {http://arxiv.org/abs/2205.09310},
  urldate = {2025-06-16},
  abstract = {Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)—a simple fix to the cross-entropy loss—by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output’s norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30\% on common benchmarks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/pccady/Zotero/storage/DJ8YF4ZJ/Wei et al. - 2022 - Mitigating Neural Network Overconfidence with Logit Normalization.pdf}
}

@thesis{witt2000use,
  type = {phdthesis},
  title = {Use of Speech Recognition in Computer-Assisted Language Learning.},
  author = {Witt, Silke Maren},
  date = {2000},
  file = {/home/pccady/Zotero/storage/ZR5EB4JA/Witt - 2000 - Use of speech recognition in computer-assisted language learning..pdf}
}

@incollection{witt2014computer,
  title = {Computer-Assisted Pronunciation Teaching Based on Automatic Speech Recognition},
  booktitle = {Language Teaching and Language Technology},
  author = {Witt, Silke and Young, Steve},
  date = {2014},
  pages = {25--35},
  publisher = {Routledge},
  file = {/home/pccady/Zotero/storage/T7GISXBL/Witt and Young - 2014 - Computer-assisted pronunciation teaching based on automatic speech recognition.pdf}
}

@article{wittAutomaticErrorDetection,
  title = {Automatic {{Error Detection}} in {{Pronunciation Training}}: {{Where}} We Are and Where We Need to Go},
  author = {Witt, Silke M},
  abstract = {This paper discusses the state of the art of research in computer assisted pronunciation teaching as of early 2012. A discussion of all major components contributing to pronunciation assessment is presented. This is followed by a summary of existing research to date. Additionally, an overview is given on the use of this research in commercial language learning software. This is followed by a discussion of remaining challenges and possible directions of future research.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/LI6LMXJY/Witt - Automatic Error Detection in Pronunciation Training Where we are and where we need to go.pdf}
}

@article{wittPhonelevelPronunciationScoring2000,
  title = {Phone-Level Pronunciation Scoring and Assessment for Interactive Language Learning},
  author = {Witt, S.M and Young, S.J},
  date = {2000-02},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {30},
  number = {2--3},
  pages = {95--108},
  issn = {01676393},
  doi = {10.1016/S0167-6393(99)00044-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639399000448},
  urldate = {2025-04-04},
  abstract = {This paper investigates a method of automatic pronunciation scoring for use in computer-assisted language learning (CALL) systems. The method utilises a likelihood-based `Goodness of Pronunciation' (GOP) measure which is extended to include individual thresholds for each phone based on both averaged native conÆdence scores and on rejection statistics provided by human judges. Further improvements are obtained by incorporating models of the subject’s native language and by augmenting the recognition networks to include expected pronunciation errors. The various GOP measures are assessed using a specially recorded database of non-native speakers which has been annotated to mark phone-level pronunciation errors. Since pronunciation assessment is highly subjective, a set of four performance measures has been designed, each of them measuring di erent aspects of how well computer-derived phone-level scores agree with human scores. These performance measures are used to cross-validate the reference annotations and to assess the basic GOP algorithm and its reÆnements. The experimental results suggest that a likelihood-based pronunciation scoring metric can achieve usable performance, especially after applying the various enhancements. ” 2000 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/pccady/Zotero/storage/KF2BIY5D/Witt and Young - 2000 - Phone-level pronunciation scoring and assessment for interactive language learning.pdf}
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor = {Liu, Qun and Schlangen, David},
  date = {2020-10},
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://aclanthology.org/2020.emnlp-demos.6/},
  urldate = {2025-09-08},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  file = {/home/pccady/Zotero/storage/2Q3Y87JC/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Processing.pdf}
}

@inproceedings{wuTransformerBasedEndtoEnd2021,
  title = {Transformer {{Based End-to-End Mispronunciation Detection}} and {{Diagnosis}}},
  booktitle = {Interspeech 2021},
  author = {Wu, Minglin and Li, Kun and Leung, Wai-Kim and Meng, Helen},
  date = {2021-08-30},
  pages = {3954--3958},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2021-1467},
  url = {https://www.isca-archive.org/interspeech_2021/wu21h_interspeech.html},
  urldate = {2025-04-04},
  abstract = {This paper introduces two Transformer-based architectures for Mispronunciation Detection and Diagnosis (MDD). The first Transformer architecture (T-1) is a standard setup with an encoder, a decoder, a projection part and the Cross Entropy (CE) loss. T-1 takes in Mel-Frequency Cepstral Coefficients (MFCC) as input. The second architecture (T-2) is based on wav2vec 2.0, a pretraining framework. T-2 is composed of a CNN feature encoder, several Transformer blocks capturing contextual speech representations, a projection part and the Connectionist Temporal Classification (CTC) loss. Unlike T-1, T-2 takes in raw audio data as input. Both models are trained in an end-to-end manner. Experiments are conducted on the CU-CHLOE corpus, where T-1 achieves a Phone Error Rate (PER) of 8.69\% and F-measure of 77.23\%; and T-2 achieves a PER of 5.97\% and F-measure of 80.98\%. Both models significantly outperform the previously proposed AGPM and CNN-RNN-CTC models, with PERs at 11.1\% and 12.1\% respectively, and F-measures at 72.61\% and 74.65\% respectively.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/home/pccady/Zotero/storage/NDHLED7D/Wu et al. - 2021 - Transformer Based End-to-End Mispronunciation Detection and Diagnosis.pdf}
}

@inproceedings{xuExploreWav2vec202021,
  title = {Explore Wav2vec 2.0 for {{Mispronunciation Detection}}},
  booktitle = {Interspeech 2021},
  author = {Xu, Xiaoshuo and Kang, Yueteng and Cao, Songjun and Lin, Binghuai and Ma, Long},
  date = {2021-08-30},
  pages = {4428--4432},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2021-777},
  url = {https://www.isca-archive.org/interspeech_2021/xu21k_interspeech.html},
  urldate = {2025-04-04},
  abstract = {This paper presents an initial attempt to use self-supervised learning for Mispronunciaiton Detection. Unlike existing methods that use speech recognition corpus to train models, we exploit unlabeled data and utilize a self-supervised learning technique, Wav2vec 2.0, for pretraining. After the pretraining process, the training process only requires a little pronunciationlabeled data for finetuning. Formulating Mispronunciation Detection as a binary classification task, we add convolutional and pooling layers on the top of the pretrained model to detect mispronunciations of the given prompted texts within the alignment segmentations. The training process is simple and effective. Several experiments are conducted to validate the effectiveness of the pretrained method. Our approach outperforms existing methods on a public dataset L2-ARCTIC with a F1 value of 0.610.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/home/pccady/Zotero/storage/EMRXJ9JC/Xu et al. - 2021 - Explore wav2vec 2.0 for Mispronunciation Detection.pdf}
}

@inproceedings{xuIterativePseudoLabelingSpeech2020,
  title = {Iterative {{Pseudo-Labeling}} for {{Speech Recognition}}},
  booktitle = {Interspeech 2020},
  author = {Xu, Qiantong and Likhomanenko, Tatiana and Kahn, Jacob and Hannun, Awni and Synnaeve, Gabriel and Collobert, Ronan},
  date = {2020-10-25},
  pages = {1006--1010},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2020-1800},
  url = {https://www.isca-archive.org/interspeech_2020/xu20b_interspeech.html},
  urldate = {2025-04-04},
  eventtitle = {Interspeech 2020},
  langid = {english},
  file = {/home/pccady/Zotero/storage/9H4BDAHD/Xu et al. - 2020 - Iterative Pseudo-Labeling for Speech Recognition.pdf}
}

@online{xuSimpleEffectiveZeroshot2021,
  title = {Simple and {{Effective Zero-shot Cross-lingual Phoneme Recognition}}},
  author = {Xu, Qiantong and Baevski, Alexei and Auli, Michael},
  date = {2021-09-23},
  eprint = {2109.11680},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.11680},
  url = {http://arxiv.org/abs/2109.11680},
  urldate = {2025-04-04},
  abstract = {Recent progress in self-training, self-supervised pretraining and unsupervised learning enabled well performing speech recognition systems without any labeled data. However, in many cases there is labeled data available for related languages which is not utilized by these methods. This paper extends previous work on zero-shot cross-lingual transfer learning by fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen languages. This is done by mapping phonemes of the training languages to the target language using articulatory features. Experiments show that this simple method significantly outperforms prior work which introduced task-specific architectures and used only part of a monolingually pretrained model.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound},
  file = {/home/pccady/Zotero/storage/KYAVSW7J/Xu et al. - 2021 - Simple and Effective Zero-shot Cross-lingual Phoneme Recognition.pdf}
}

@inproceedings{yangImprovingMispronunciationDetection2022,
  title = {Improving {{Mispronunciation Detection}} with {{Wav2vec2-based Momentum Pseudo-Labeling}} for {{Accentedness}} and {{Intelligibility Assessment}}},
  booktitle = {Interspeech 2022},
  author = {Yang, Mu and Hirschi, Kevin and Looney, Stephen Daniel and Kang, Okim and Hansen, John H.L.},
  date = {2022-09-18},
  pages = {4481--4485},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2022-11039},
  url = {https://www.isca-archive.org/interspeech_2022/yang22v_interspeech.html},
  urldate = {2025-04-04},
  abstract = {Current leading mispronunciation detection and diagnosis (MDD) systems achieve promising performance via end-to-end phoneme recognition. One challenge of such end-to-end solutions is the scarcity of human-annotated phonemes on natural L2 speech. In this work, we leverage unlabeled L2 speech via a pseudo-labeling (PL) procedure and extend the fine-tuning approach based on pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec 2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples plus the created pseudo-labeled L2 speech samples. Our pseudo labels are dynamic and are produced by an ensemble of the online model on-the-fly, which ensures that our model is robust to pseudo label noise. We show that fine-tuning with pseudo labels achieves a 5.35\% phoneme error rate reduction and 2.48\% MDD F1 score improvement over a labeled-samples-only finetuning baseline. The proposed PL method is also shown to outperform conventional offline PL methods. Compared to the state-of-the-art MDD systems, our MDD solution produces a more accurate and consistent phonetic error diagnosis. In addition, we conduct an open test on a separate UTD-4Accents dataset, where our system recognition outputs show a strong correlation with human perception, based on accentedness and intelligibility.},
  eventtitle = {Interspeech 2022},
  langid = {english},
  file = {/home/pccady/Zotero/storage/YKI5Y5R7/Yang et al. - 2022 - Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness a.pdf}
}

@book{yuAutomaticSpeechRecognition2015,
  title = {Automatic {{Speech Recognition}}: {{A Deep Learning Approach}}},
  shorttitle = {Automatic {{Speech Recognition}}},
  author = {Yu, Dong and Deng, Li},
  date = {2015},
  series = {Signals and {{Communication Technology}}},
  publisher = {Springer London},
  location = {London},
  doi = {10.1007/978-1-4471-5779-3},
  url = {https://link.springer.com/10.1007/978-1-4471-5779-3},
  urldate = {2025-08-31},
  isbn = {978-1-4471-5778-6 978-1-4471-5779-3},
  langid = {english},
  file = {/home/pccady/Zotero/storage/JE6MPKNW/Yu and Deng - 2015 - Automatic Speech Recognition A Deep Learning Approach.pdf}
}

@online{zeyerWhyDoesCTC2021,
  title = {Why Does {{CTC}} Result in Peaky Behavior?},
  author = {Zeyer, Albert and Schlüter, Ralf and Ney, Hermann},
  date = {2021-06-03},
  eprint = {2105.14849},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.14849},
  url = {http://arxiv.org/abs/2105.14849},
  urldate = {2025-06-16},
  abstract = {The peaky behavior of CTC models is well known experimentally. However, an understanding about why peaky behavior occurs is missing, and whether this is a good property. We provide a formal analysis of the peaky behavior and gradient descent convergence properties of the CTC loss and related training criteria. Our analysis provides a deep understanding why peaky behavior occurs and when it is suboptimal. On a simple example which should be trivial to learn for any model, we prove that a feed-forward neural network trained with CTC from uniform initialization converges towards peaky behavior with a 100\% error rate. Our analysis further explains why CTC only works well together with the blank label. We further demonstrate that peaky behavior does not occur on other related losses including a label prior model, and that this improves convergence.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/home/pccady/Zotero/storage/4ZZED8DN/Zeyer et al. - 2021 - Why does CTC result in peaky behavior.pdf}
}

@inproceedings{zhangL2GENNeuralPhoneme2022,
  title = {L2-{{GEN}}: {{A Neural Phoneme Paraphrasing Approach}} to {{L2 Speech Synthesis}} for {{Mispronunciation Diagnosis}}},
  shorttitle = {L2-{{GEN}}},
  booktitle = {Interspeech 2022},
  author = {Zhang, Daniel and Ganesan, Ashwinkumar and Campbell, Sarah and Korzekwa, Daniel},
  date = {2022-09-18},
  pages = {4317--4321},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2022-209},
  url = {https://www.isca-archive.org/interspeech_2022/zhang22_interspeech.html},
  urldate = {2025-04-04},
  abstract = {In this paper, we study the problem of generating mispronounced speech mimicking non-native (L2) speakers learning English as a Second Language (ESL) for the mispronunciation detection and diagnosis (MDD) task. The paper is motivated by the widely observed yet not well addressed data sparsity issue in MDD research where both L2 speech audio and its finegrained phonetic annotations are difficult to obtain, leading to unsatisfactory mispronunciation feedback accuracy. We propose L2-GEN, a new data augmentation framework to generate L2 phoneme sequences that capture realistic mispronunciation patterns by devising an unique machine translation-based sequence paraphrasing model. A novel diversified and preferenceaware decoding algorithm is proposed to generalize L2-GEN to handle both unseen words and new learner population with very limited L2 training data. A contrastive augmentation technique is further designed to optimize MDD performance improvements with the generated synthetic L2 data. We evaluate L2-GEN on public L2-ARCTIC and SpeechOcean762 datasets. The results have shown that L2-GEN leads to up to 3.9\%, and 5.0\% MDD F1-score improvements in in-domain and out-ofdomain scenarios respectively.},
  eventtitle = {Interspeech 2022},
  langid = {english},
  file = {/home/pccady/Zotero/storage/WTAWKCMN/Zhang et al. - 2022 - L2-GEN A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis.pdf}
}

@online{zhangLearningSpeakFluently2019,
  title = {Learning to {{Speak Fluently}} in a {{Foreign Language}}: {{Multilingual Speech Synthesis}} and {{Cross-Language Voice Cloning}}},
  shorttitle = {Learning to {{Speak Fluently}} in a {{Foreign Language}}},
  author = {Zhang, Yu and Weiss, Ron J. and Zen, Heiga and Wu, Yonghui and Chen, Zhifeng and Skerry-Ryan, R. J. and Jia, Ye and Rosenberg, Andrew and Ramabhadran, Bhuvana},
  date = {2019-07-24},
  eprint = {1907.04448},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.04448},
  url = {http://arxiv.org/abs/1907.04448},
  urldate = {2025-04-04},
  abstract = {We present a multispeaker, multilingual text-to-speech (TTS) synthesis model based on Tacotron that is able to produce high quality speech in multiple languages. Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish speech using an English speaker’s voice, without training on any bilingual or parallel examples. Such transfer works across distantly related languages, e.g. English and Mandarin.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/PC7LRBXY/Zhang et al. - 2019 - Learning to Speak Fluently in a Foreign Language Multilingual Speech Synthesis and Cross-Language V.pdf}
}

@online{zhangNeMoInverseText2021,
  title = {{{NeMo Inverse Text Normalization}}: {{From Development To Production}}},
  shorttitle = {{{NeMo Inverse Text Normalization}}},
  author = {Zhang, Yang and Bakhturina, Evelina and Gorman, Kyle and Ginsburg, Boris},
  date = {2021-05-17},
  eprint = {2104.05055},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.05055},
  url = {http://arxiv.org/abs/2104.05055},
  urldate = {2025-04-04},
  abstract = {Inverse text normalization (ITN) converts spoken-domain automatic speech recognition (ASR) output into written-domain text to improve the readability of the ASR output. Many stateof-the-art ITN systems use hand-written weighted finite-state transducer (WFST) grammars since this task has extremely low tolerance to unrecoverable errors. We introduce an open-source Python WFST-based library for ITN which enables a seamless path from development to production. We describe the specification of ITN grammar rules for English, but the library can be adapted for other languages. It can also be used for writtento-spoken text normalization. We evaluate the NeMo ITN library using a modified version of the Google Text normalization dataset.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/C5YDN8W8/Zhang et al. - 2021 - NeMo Inverse Text Normalization From Development To Production.pdf}
}

@online{zhangSpeechocean762OpenSourceNonnative2021,
  title = {Speechocean762: {{An Open-Source Non-native English Speech Corpus For Pronunciation Assessment}}},
  shorttitle = {Speechocean762},
  author = {Zhang, Junbo and Zhang, Zhiwen and Wang, Yongqing and Yan, Zhiyong and Song, Qiong and Huang, Yukai and Li, Ke and Povey, Daniel and Wang, Yujun},
  date = {2021-06-02},
  eprint = {2104.01378},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.01378},
  url = {http://arxiv.org/abs/2104.01378},
  urldate = {2025-04-04},
  abstract = {This paper introduces a new open-source speech corpus named “speechocean762” designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children. Five experts annotated each of the utterances at sentence-level, wordlevel and phoneme-level. A baseline system is released in open source to illustrate the phoneme-level pronunciation assessment workflow on this corpus. This corpus is allowed to be used freely for commercial and non-commercial purposes. It is available for free download from OpenSLR, and the corresponding baseline system is published in the Kaldi speech recognition toolkit.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/pccady/Zotero/storage/8YRR74DH/Zhang et al. - 2021 - speechocean762 An Open-Source Non-native English Speech Corpus For Pronunciation Assessment.pdf}
}

@inproceedings{zhaoL2ARCTICNonnativeEnglish2018,
  title = {L2-{{ARCTIC}}: {{A Non-native English Speech Corpus}}},
  shorttitle = {L2-{{ARCTIC}}},
  booktitle = {Interspeech 2018},
  author = {Zhao, Guanlong and Sonsaat, Sinem and Silpachai, Alif and Lucic, Ivana and Chukharev-Hudilainen, Evgeny and Levis, John and Gutierrez-Osuna, Ricardo},
  date = {2018-09-02},
  pages = {2783--2787},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2018-1110},
  url = {https://www.isca-archive.org/interspeech_2018/zhao18b_interspeech.html},
  urldate = {2025-04-04},
  abstract = {In this paper, we introduce L2-ARCTIC, a speech corpus of non-native English that is intended for research in voice conversion, accent conversion, and mispronunciation detection. This initial release includes recordings from ten non-native speakers of English whose first languages (L1s) are Hindi, Korean, Mandarin, Spanish, and Arabic, each L1 containing recordings from one male and one female speaker. Each speaker recorded approximately one hour of read speech from the Carnegie Mellon University ARCTIC prompts, from which we generated orthographic and forced-aligned phonetic transcriptions. In addition, we manually annotated 150 utterances per speaker to identify three types of mispronunciation errors: substitutions, deletions, and additions, making it a valuable resource not only for research in voice conversion and accent conversion but also in computer-assisted pronunciation training. The corpus is publicly accessible at https://psi.engr.tamu.edu/l2-arctic-corpus/.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/home/pccady/Zotero/storage/IFZLMRKT/Zhao et al. - 2018 - L2-ARCTIC A Non-native English Speech Corpus.pdf}
}
