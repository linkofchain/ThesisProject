{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OWSM finetuning with custom dataset\n",
    "This Jupyter notebook provides a step-by-step guide on using the ESPnetEZ module to finetune owsm model. In this demonstration, we will leverage the custom dataset to finetune an OWSM model for ASR task.\n",
    "\n",
    "Author: Masao Someki [@Masao-Someki](https://github.com/Masao-Someki)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this tutorial, we assume that we have the custom dataset with 654 audio with the following directory structure:\n",
    "\n",
    "```\n",
    "audio\n",
    "├── 001 [420 files]\n",
    "└── 002 [234 files]\n",
    "transcription\n",
    "└── owsm_v3.1\n",
    "      ├── 001.csv\n",
    "      └── 002.csv\n",
    "```\n",
    "\n",
    "The csv files contains the audio path, text, and text_ctc data in Japanese. For example, the csv constains the following data:\n",
    "\n",
    "```\n",
    "audio/001/00014.wav,しゃべるたびに追いかけてくるんですけど,なんかしゃべるたびにおいかけてくるんですけど\n",
    "audio/001/00015.wav,え、どうしよう,えどうしよう\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/decoder/stft_decoder.py:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/encoder/stft_encoder.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/loss/criterions/time_domain.py:446: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/layers/tcn.py:458: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/layers/tcn.py:499: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/layers/uses.py:392: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n",
      "/home/peter/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/enh/layers/uses.py:421: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @torch.cuda.amp.autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "from espnet2.bin.s2t_inference import Speech2Text\n",
    "#from espnet2.layers.create_adapter_fn import create_lora_adapter\n",
    "import espnet2.layers.create_lora_adapter\n",
    "import espnetez as ez\n",
    "\n",
    "# Define hyper parameters\n",
    "DUMP_DIR = f\"./dump\"\n",
    "CSV_DIR = f\"./transcription\"\n",
    "EXP_DIR = f\"./exp/finetune\"\n",
    "STATS_DIR = f\"./exp/stats_finetune\"\n",
    "\n",
    "FINETUNE_MODEL = \"espnet/owsm_v3.1_ebf\"\n",
    "LORA_TARGET = [\n",
    "    \"w_1\", \"w_2\", \"merge_proj\"\n",
    "]\n",
    "LANGUAGE = \"jpn\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training configs and model\n",
    "\n",
    "Since we are going to finetune an OWSM model for ASR task, we will use the tokenizer and TokenIDConverter of the OWSM model. We will also use the training config as the default parameter sets, and update them with the finetuning configuration.\n",
    "\n",
    "In this demo, we will apply Lora adapter to the model for parameter efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 37 files: 100%|██████████| 37/37 [00:00<00:00, 123263.90it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'promptencoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m \u001b[43mSpeech2Text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFINETUNE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_sym\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mLANGUAGE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m \u001b[38;5;66;03m# Load model to extract configs.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pretrain_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m(pretrained_model\u001b[38;5;241m.\u001b[39ms2t_train_args)\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m pretrained_model\u001b[38;5;241m.\u001b[39mtokenizer\n",
      "File \u001b[0;32m~/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/bin/s2t_inference.py:525\u001b[0m, in \u001b[0;36mSpeech2Text.from_pretrained\u001b[0;34m(model_tag, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m     d \u001b[38;5;241m=\u001b[39m ModelDownloader()\n\u001b[1;32m    523\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md\u001b[38;5;241m.\u001b[39mdownload_and_unpack(model_tag))\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSpeech2Text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/bin/s2t_inference.py:95\u001b[0m, in \u001b[0;36mSpeech2Text.__init__\u001b[0;34m(self, s2t_train_config, s2t_model_file, lm_train_config, lm_file, ngram_scorer, ngram_file, token_type, bpemodel, device, maxlenratio, minlenratio, batch_size, dtype, beam_size, ctc_weight, lm_weight, ngram_weight, penalty, nbest, streaming, quantize_s2t_model, quantize_lm, quantize_modules, quantize_dtype, category_sym, task_sym, time_sym)\u001b[0m\n\u001b[1;32m     92\u001b[0m quantize_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, quantize_dtype)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# 1. Build S2T model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m s2t_model, s2t_train_args \u001b[38;5;241m=\u001b[39m \u001b[43mS2TTask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms2t_train_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2t_model_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m s2t_model\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(torch, dtype))\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quantize_s2t_model:\n",
      "File \u001b[0;32m~/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/tasks/abs_task.py:2045\u001b[0m, in \u001b[0;36mAbsTask.build_model_from_file\u001b[0;34m(cls, config_file, model_file, device)\u001b[0m\n\u001b[1;32m   2043\u001b[0m     args \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m   2044\u001b[0m args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m-> 2045\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, AbsESPnetModel):\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel must inherit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAbsESPnetModel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Studies/Thesis/ThesisProject/venv/lib/python3.10/site-packages/espnet2/tasks/s2t.py:454\u001b[0m, in \u001b[0;36mS2TTask.build_model\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    451\u001b[0m encoder \u001b[38;5;241m=\u001b[39m encoder_class(input_size\u001b[38;5;241m=\u001b[39minput_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39mencoder_conf)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# 5. Prompt Encoder\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m promptencoder_class \u001b[38;5;241m=\u001b[39m promptencoder_choices\u001b[38;5;241m.\u001b[39mget_class(\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromptencoder\u001b[49m)\n\u001b[1;32m    455\u001b[0m promptencoder \u001b[38;5;241m=\u001b[39m promptencoder_class(\n\u001b[1;32m    456\u001b[0m     input_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mpromptencoder_conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    457\u001b[0m     input_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39mpromptencoder_conf\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# 6. CTC\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'promptencoder'"
     ]
    }
   ],
   "source": [
    "pretrained_model = Speech2Text.from_pretrained(\n",
    "    FINETUNE_MODEL,\n",
    "    category_sym=f\"<{LANGUAGE}>\",\n",
    "    beam_size=10\n",
    ") # Load model to extract configs.\n",
    "pretrain_config = vars(pretrained_model.s2t_train_args)\n",
    "tokenizer = pretrained_model.tokenizer\n",
    "converter = pretrained_model.converter\n",
    "del pretrained_model\n",
    "\n",
    "# For the configuration, please refer to the last cell in this notebook.\n",
    "finetune_config = ez.config.update_finetune_config(\n",
    "\t's2t',\n",
    "\tpretrain_config,\n",
    "\tf\"finetune_with_lora.yaml\"\n",
    ")\n",
    "\n",
    "# When you don't use yaml file, you can load finetune_config in the following way.\n",
    "# task_class = ez.task.get_ez_task(\"s2t\")\n",
    "# default_config = task_class.get_default_config()\n",
    "# training_config = default_config.update(your_config_in_dict)\n",
    "\n",
    "# define model loading function\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def freeze_parameters(model):\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.requires_grad = False\n",
    "\n",
    "def build_model_fn(args):\n",
    "    pretrained_model = Speech2Text.from_pretrained(\n",
    "        FINETUNE_MODEL,\n",
    "        category_sym=f\"<{LANGUAGE}>\",\n",
    "        beam_size=10,\n",
    "    )\n",
    "    model = pretrained_model.s2t_model\n",
    "    model.train()\n",
    "    print(f'Trainable parameters: {count_parameters(model)}')\n",
    "    freeze_parameters(model)\n",
    "\n",
    "    # apply lora\n",
    "    create_lora_adapter(model, target_modules=LORA_TARGET)\n",
    "    print(f'Trainable parameters after LORA: {count_parameters(model)}')\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap with ESPnetEasyDataset\n",
    "\n",
    "Before initiating the training process, it is crucial to adapt the dataset to the ESPnet format. The dataset class should output tokenized text and audio files in `np.array` format.\n",
    "\n",
    "Then let's define the custom dataset class. The owsm finetuning requires `audio`, `text`, `text_prev` and `text_ctc` data. You can use your custom-defined dataset, huggingface `datasets` library, or `lhotse` library, or any other dataloader that you want to use.\n",
    "\n",
    "When you try to use custom-defined dataset, you should define the `data_info` dictionary. It defines the mapping between the output of your model and the input of ESPnet models.\n",
    "\n",
    "**Note**:\n",
    "- Currently we do not support the custom dataloader that feeds processed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        # data_list is a list of tuples (audio_path, text, text_ctc)\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._parse_single_data(self.data[idx])\n",
    "\n",
    "    def _parse_single_data(self, d):\n",
    "        text = f\"<{LANGUAGE}><asr><notimestamps> {d['transcript']}\"\n",
    "        return {\n",
    "            \"audio_path\": d[\"audio_path\"],\n",
    "            \"text\": text,\n",
    "            \"text_prev\": \"<na>\",\n",
    "            \"text_ctc\": d['text_ctc'],\n",
    "        }\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for csv_file in sorted(glob(os.path.join(CSV_DIR, \"*.csv\"))):\n",
    "    with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list += f.readlines()[1:] # skip header\n",
    "\n",
    "validation_examples = 20\n",
    "train_dataset = CustomDataset(data_list[:-validation_examples])\n",
    "valid_dataset = CustomDataset(data_list[-validation_examples:])\n",
    "\n",
    "def tokenize(text):\n",
    "    return np.array(converter.tokens2ids(tokenizer.text2tokens(text)))\n",
    "\n",
    "# The output of CustomDatasetInstance[idx] will converted to np.array\n",
    "# with the functions defined in the data_info dictionary.\n",
    "data_info = {\n",
    "    \"speech\": lambda d: librosa.load(d[\"audio_path\"], sr=16000)[0],\n",
    "    \"text\": lambda d: tokenize(d[\"text\"]),\n",
    "    \"text_prev\": lambda d: tokenize(d[\"text_prev\"]),\n",
    "    \"text_ctc\": lambda d: tokenize(d[\"text_ctc\"]),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to use `datasets` library from huggingface or `lhotse` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets library\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "train_dataset = load_dataset(\"audiofolder\", data_dir=f\"/path/to/huggingface_dataset\", split=f'train[:-{validation_examples}]')\n",
    "valid_dataset = load_dataset(\"audiofolder\", data_dir=f\"/path/to/huggingface_dataset\", split=f'train[-{validation_examples}:]')\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "valid_dataset = valid_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "data_info = {\n",
    "    \"speech\": lambda d: d['audio']['array'],\n",
    "    \"text\": lambda d: tokenize(f\"<{LANGUAGE}><asr><notimestamps> {d['transcript']}\"),\n",
    "    \"text_prev\": lambda d: tokenize(\"<na>\"),\n",
    "    \"text_ctc\": lambda d: tokenize(d[\"text_ctc\"]),\n",
    "}\n",
    "\n",
    "# Or lhotse library. The following code is from the official document.\n",
    "from pathlib import Path\n",
    "from lhotse import CutSet\n",
    "from lhotse.recipes import download_librispeech, prepare_librispeech\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    y, _ = librosa.load(audio_path, sr=16000)\n",
    "    return y\n",
    "\n",
    "root_dir = Path(\"data\")\n",
    "tmp_dir = Path(\"tmp\")\n",
    "tmp_dir.mkdir(exist_ok=True)\n",
    "num_jobs = os.cpu_count() - 1\n",
    "\n",
    "libri_variant = \"mini_librispeech\"\n",
    "libri_root = download_librispeech(root_dir, dataset_parts=libri_variant)\n",
    "libri = prepare_librispeech(\n",
    "    libri_root, dataset_parts=libri_variant, output_dir=root_dir, num_jobs=num_jobs\n",
    ")\n",
    "train_dataset = CutSet.from_manifests(**libri[\"train-clean-5\"])\n",
    "valid_dataset = CutSet.from_manifests(**libri[\"dev-clean-2\"])\n",
    "data_info = {\n",
    "    \"speech\": lambda d: load_audio(d.recording.sources[0].source),\n",
    "    \"text\": lambda d: tokenize(f\"<{LANGUAGE}><asr><notimestamps> {d.supervisions[0].text}\"),\n",
    "    \"text_prev\": lambda d: tokenize(\"<na>\"),\n",
    "    \"text_ctc\": lambda d: tokenize(d.supervisions[0].text),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally you need to wrap your custom dataset with ESPnetEZDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into ESPnet-EZ dataset format\n",
    "train_dataset = ez.dataset.ESPnetEZDataset(train_dataset, data_info=data_info)\n",
    "valid_dataset = ez.dataset.ESPnetEZDataset(valid_dataset, data_info=data_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "While the configuration remains consistent with other notebooks, the instantiation arguments for the Trainer class differ in this case. As we have not generated dump files, we can disregard arguments related to dump files and directly provide the train/valid dataset classes.\n",
    "\n",
    "```\n",
    "trainer = Trainer(\n",
    "    ...\n",
    "    train_dataset=your_train_dataset_instance,\n",
    "    train_dataset=your_valid_dataset_instance,\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ez.Trainer(\n",
    "    task='s2t',\n",
    "    train_config=finetune_config,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    build_model_fn=build_model_fn, # provide the pre-trained model\n",
    "    data_info=data_info,\n",
    "    output_dir=EXP_DIR,\n",
    "    stats_dir=STATS_DIR,\n",
    "    ngpu=1\n",
    ")\n",
    "trainer.collect_stats()\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "When training is done, we can use the inference API to generate the transcription, but don't forget to apply lora before loading the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = Speech2Text.from_pretrained(\n",
    "    \"espnet/owsm_v3.1_ebf\",\n",
    "    category_sym=\"<jpn>\",\n",
    "    beam_size=10,\n",
    "    device=DEVICE\n",
    ")\n",
    "create_lora_adapter(model.s2t_model, target_modules=LORA_TARGET)\n",
    "model.s2t_model.eval()\n",
    "d = torch.load(\"./exp/finetune/1epoch.pth\")\n",
    "model.s2t_model.load_state_dict(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "As a result, the finetuned owsm-v3.1 could successfully transcribe the audio files.\n",
    "\n",
    "**Example**\n",
    "- correct transcription: ダンスでこの世界に彩りを。\n",
    "- before finetune: 出してこの時間二のどりを。  \n",
    "- after finetune: ダンスでこの世界に彩りを。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA finetune related\n",
    "use_lora: true\n",
    "\n",
    "rir_scp: null\n",
    "rir_apply_prob: 1.0\n",
    "noise_scp: null\n",
    "noise_apply_prob: 1.0\n",
    "noise_db_range: '13_15'\n",
    "speech_volume_normalize: null\n",
    "non_linguistic_symbols: null\n",
    "\n",
    "preprocessor_conf:\n",
    "  speech_name: speech\n",
    "  text_name: text\n",
    "\n",
    "# training related\n",
    "seed: 2022\n",
    "num_workers: 4\n",
    "ngpu: 1\n",
    "batch_type: numel\n",
    "batch_bins: 1600000\n",
    "accum_grad: 4\n",
    "max_epoch: 70\n",
    "patience: null\n",
    "init: null\n",
    "best_model_criterion:\n",
    "-   - valid\n",
    "    - acc\n",
    "    - max\n",
    "keep_nbest_models: 10\n",
    "use_amp: true\n",
    "\n",
    "optim: adam\n",
    "optim_conf:\n",
    "    lr: 0.002\n",
    "    weight_decay: 0.000001\n",
    "scheduler: warmuplr\n",
    "scheduler_conf:\n",
    "    warmup_steps: 15000\n",
    "\n",
    "specaug: specaug\n",
    "specaug_conf:\n",
    "    apply_time_warp: true\n",
    "    time_warp_window: 5\n",
    "    time_warp_mode: bicubic\n",
    "    apply_freq_mask: true\n",
    "    freq_mask_width_range:\n",
    "    - 0\n",
    "    - 27\n",
    "    num_freq_mask: 2\n",
    "    apply_time_mask: true\n",
    "    time_mask_width_ratio_range:\n",
    "    - 0.\n",
    "    - 0.05\n",
    "    num_time_mask: 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
